{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Artificial Neural Networks and Deep Learning - 2025/2","text":"<p>Rodrigo Paoliello de Medeiros</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Data Exercise - 05/09</li> <li> Perceptron Exercise - 12/09</li> <li> MLP Exercise - 19/09</li> <li> Classification Project - 21/09</li> <li> Metrics Exercise - 17/10</li> <li> Regression Project - 19/10</li> <li> Generative Models Project - 16/11</li> </ul>"},{"location":"exercises/data/notebook/","title":"1. Data","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define parameters for each class\nclass_params = {\n    0: {'mean': [2, 3], 'std': [0.8, 2.5]},\n    1: {'mean': [5, 6], 'std': [1.2, 1.9]},\n    2: {'mean': [8, 1], 'std': [0.9, 0.9]},\n    3: {'mean': [15, 4], 'std': [0.5, 2.0]}\n}\n\n# Generate the data\ndata = []\nlabels = []\n\nfor label, params in class_params.items():\n    samples = np.random.randn(100, 2) * params['std'] + params['mean']\n    data.append(samples)\n    labels.append(np.full(100, label))\n\n# Combine into arrays\ndata = np.vstack(data)\nlabels = np.hstack(labels)\n\nprint(\"Dataset generated: \", data.shape, labels.shape)\n</pre> import numpy as np import matplotlib.pyplot as plt  # Set random seed for reproducibility np.random.seed(42)  # Define parameters for each class class_params = {     0: {'mean': [2, 3], 'std': [0.8, 2.5]},     1: {'mean': [5, 6], 'std': [1.2, 1.9]},     2: {'mean': [8, 1], 'std': [0.9, 0.9]},     3: {'mean': [15, 4], 'std': [0.5, 2.0]} }  # Generate the data data = [] labels = []  for label, params in class_params.items():     samples = np.random.randn(100, 2) * params['std'] + params['mean']     data.append(samples)     labels.append(np.full(100, label))  # Combine into arrays data = np.vstack(data) labels = np.hstack(labels)  print(\"Dataset generated: \", data.shape, labels.shape) <pre>Dataset generated:  (400, 2) (400,)\n</pre> In\u00a0[2]: Copied! <pre>plt.figure(figsize=(10, 7))\ncolors = ['red', 'blue', 'green', 'purple']\n\nfor i in range(4):\n    plt.scatter(\n        data[labels == i, 0],\n        data[labels == i, 1],\n        label=f'Class {i}',\n        color=colors[i],\n        alpha=0.7\n    )\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Synthetic Dataset - 4 Classes')\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> plt.figure(figsize=(10, 7)) colors = ['red', 'blue', 'green', 'purple']  for i in range(4):     plt.scatter(         data[labels == i, 0],         data[labels == i, 1],         label=f'Class {i}',         color=colors[i],         alpha=0.7     )  plt.xlabel('Feature 1') plt.ylabel('Feature 2') plt.title('Synthetic Dataset - 4 Classes') plt.legend() plt.grid(True) plt.show() <p>Class 0: Spread around (2,3) with a wider vertical spread due to the higher standard deviation in the y-direction (2.5).</p> <p>Class 1: Clustered near (5,6) but with moderate spread in both directions.</p> <p>Class 2: Compact and tight cluster around (8,1) with almost circular spread due to similar standard deviation in x and y.</p> <p>Class 3: Very concentrated horizontally near x=15 but elongated vertically due to a larger y standar deviation (2.0).</p> <p>There\u2019s clear separation between Class 3 and the others due to its large x mean.</p> <p>Classes 0, 1, and 2 show potential overlap:</p> <p>Class 0 and Class 1 overlap slightly in the higher y-region.</p> <p>Class 2 is more isolated near x=8 and y=1, but some overlap could occur along its boundary with Class 1 if noise increases.</p> <p>b. Based on your visual inspection, could a simple, linear boundary separate all classes?</p> <p>A single linear boundary would not perfectly separate all classes:</p> <p>Class 3 is linearly separable.</p> <p>Classes 0, 1, and 2 are intertwined in the central region, requiring non-linear decision boundaries for clean separation.</p> <p>c. On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</p> <p>A trained neural network would learn to separate these classes with non-linear boundaries that might look something like this:</p> <p></p> In\u00a0[3]: Copied! <pre># Parameters for Class A\nmean_A = np.array([0, 0, 0, 0, 0])\ncov_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\n# Parameters for Class B\nmean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\ncov_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n\n# Generate samples\nsamples_A = np.random.multivariate_normal(mean_A, cov_A, size=500)\nsamples_B = np.random.multivariate_normal(mean_B, cov_B, size=500)\n\n# Create labels\nlabels_A = np.zeros(500)  # Class A = 0\nlabels_B = np.ones(500)   # Class B = 1\n\n# Combine data\nX = np.vstack([samples_A, samples_B])\ny = np.hstack([labels_A, labels_B])\n\nprint(\"Dataset shape:\", X.shape)\nprint(\"Labels shape:\", y.shape)\n</pre> # Parameters for Class A mean_A = np.array([0, 0, 0, 0, 0]) cov_A = np.array([     [1.0, 0.8, 0.1, 0.0, 0.0],     [0.8, 1.0, 0.3, 0.0, 0.0],     [0.1, 0.3, 1.0, 0.5, 0.0],     [0.0, 0.0, 0.5, 1.0, 0.2],     [0.0, 0.0, 0.0, 0.2, 1.0] ])  # Parameters for Class B mean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5]) cov_B = np.array([     [1.5, -0.7, 0.2, 0.0, 0.0],     [-0.7, 1.5, 0.4, 0.0, 0.0],     [0.2, 0.4, 1.5, 0.6, 0.0],     [0.0, 0.0, 0.6, 1.5, 0.3],     [0.0, 0.0, 0.0, 0.3, 1.5] ])  # Generate samples samples_A = np.random.multivariate_normal(mean_A, cov_A, size=500) samples_B = np.random.multivariate_normal(mean_B, cov_B, size=500)  # Create labels labels_A = np.zeros(500)  # Class A = 0 labels_B = np.ones(500)   # Class B = 1  # Combine data X = np.vstack([samples_A, samples_B]) y = np.hstack([labels_A, labels_B])  print(\"Dataset shape:\", X.shape) print(\"Labels shape:\", y.shape) <pre>Dataset shape: (1000, 5)\nLabels shape: (1000,)\n</pre> In\u00a0[4]: Copied! <pre>from sklearn.decomposition import PCA\n\n# Apply PCA to reduce dimensions to 2D\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(10, 7))\ncolors = ['red', 'blue']\nclasses = ['A', 'B']\nfor i, color in enumerate(colors):\n    plt.scatter(\n        X_pca[y == i, 0],\n        X_pca[y == i, 1],\n        label=f'Class {classes[i]}',\n        color=color,\n        alpha=0.7\n    )\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.title('PCA Visualization of 5D Data')\nplt.legend()\nplt.grid()\nplt.show()\n</pre> from sklearn.decomposition import PCA  # Apply PCA to reduce dimensions to 2D pca = PCA(n_components=2) X_pca = pca.fit_transform(X)  plt.figure(figsize=(10, 7)) colors = ['red', 'blue'] classes = ['A', 'B'] for i, color in enumerate(colors):     plt.scatter(         X_pca[y == i, 0],         X_pca[y == i, 1],         label=f'Class {classes[i]}',         color=color,         alpha=0.7     ) plt.xlabel('First Principal Component') plt.ylabel('Second Principal Component') plt.title('PCA Visualization of 5D Data') plt.legend() plt.grid() plt.show() <p>a. Based on your 2D projection, describe the relationship between the two classes.</p> <p>The 2D projection shows significant overlap between Classes A and B. The class centers are slightly offset, but their distributions largely intersect. Both classes show similar spread patterns, making it difficult to distinguish clear boundaries between them in this reduced dimensional space.</p> <p>b. Discuss the **linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.**</p> <p>The 5D dataset presents significant challenges for linear classification due to its complex structure. The PCA visualization reveals substantial overlap between classes, and the different covariance patterns in each class create intricate relationships that a simple linear boundary cannot capture. A multi-layer neural network with non-linear activation functions would be more suitable as it can transform the input space and learn complex decision boundaries that better separate the classes.</p> <p>The Spaceship Titanic dataset is designed for a classification task where the goal is to predict the value of the target variable Transported. This column is boolean:</p> <p>True \u2192 the passenger was transported to another dimension following the spaceship incident.</p> <p>False \u2192 the passenger was not transported and stayed in the original dimension.</p> <p>This problem is analogous to survival prediction on the Titanic dataset but framed in a sci-fi setting.</p> In\u00a0[5]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"spaceship-titanic/train.csv\")\n\ndf.head(5)\n</pre> import pandas as pd  df = pd.read_csv(\"spaceship-titanic/train.csv\")  df.head(5) <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[5], line 3\n      1 import pandas as pd\n----&gt; 3 df = pd.read_csv(\"spaceship-titanic/train.csv\")\n      5 df.head(5)\n\nFile /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/io/common.py:873, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    868 elif isinstance(handle, str):\n    869     # Check whether the filename is to be opened in binary mode.\n    870     # Binary mode does not support 'encoding' and 'newline'.\n    871     if ioargs.encoding and \"b\" not in ioargs.mode:\n    872         # Encoding\n--&gt; 873         handle = open(\n    874             handle,\n    875             ioargs.mode,\n    876             encoding=ioargs.encoding,\n    877             errors=errors,\n    878             newline=\"\",\n    879         )\n    880     else:\n    881         # Binary mode\n    882         handle = open(handle, ioargs.mode)\n\nFileNotFoundError: [Errno 2] No such file or directory: 'spaceship-titanic/train.csv'</pre> <p>List the features and identify which are **numerical (e.g., <code>Age</code>, <code>RoomService</code>) and which are categorical (e.g., <code>HomePlanet</code>, <code>Destination</code>).**</p> In\u00a0[6]: Copied! <pre>target_column = 'Transported'\ny = df[target_column].astype(int)\nX = df.drop(columns=[target_column])\n\nnumerical_features = X.select_dtypes(include=['number']).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n\nprint(\"\\nNumerical Features:\")\nfor feature in numerical_features:\n    print(f\"- {feature}\")\n\nprint(\"\\nCategorical Features:\")\nfor feature in categorical_features:\n    print(f\"- {feature}\")\n</pre> target_column = 'Transported' y = df[target_column].astype(int) X = df.drop(columns=[target_column])  numerical_features = X.select_dtypes(include=['number']).columns.tolist() categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()  print(\"\\nNumerical Features:\") for feature in numerical_features:     print(f\"- {feature}\")  print(\"\\nCategorical Features:\") for feature in categorical_features:     print(f\"- {feature}\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 2\n      1 target_column = 'Transported'\n----&gt; 2 y = df[target_column].astype(int)\n      3 X = df.drop(columns=[target_column])\n      5 numerical_features = X.select_dtypes(include=['number']).columns.tolist()\n\nNameError: name 'df' is not defined</pre> <p>Investigate the dataset for **missing values. Which columns have them, and how many**</p> In\u00a0[7]: Copied! <pre>missing_counts = df.isna().sum()\nn_rows = len(df)\n\nmissing_table = (\n    pd.DataFrame({\n        \"missing_count\": missing_counts,\n        \"missing_pct\": (missing_counts / n_rows * 100).round(2)\n    })\n    .query(\"missing_count &gt; 0\")\n    .sort_values(\"missing_count\", ascending=False)\n)\n\nprint(\"\\nMissing Values\")\nprint(missing_table.to_string(float_format=lambda x: f\"{x:,.2f}%\"))\n</pre> missing_counts = df.isna().sum() n_rows = len(df)  missing_table = (     pd.DataFrame({         \"missing_count\": missing_counts,         \"missing_pct\": (missing_counts / n_rows * 100).round(2)     })     .query(\"missing_count &gt; 0\")     .sort_values(\"missing_count\", ascending=False) )  print(\"\\nMissing Values\") print(missing_table.to_string(float_format=lambda x: f\"{x:,.2f}%\")) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 missing_counts = df.isna().sum()\n      2 n_rows = len(df)\n      4 missing_table = (\n      5     pd.DataFrame({\n      6         \"missing_count\": missing_counts,\n   (...)     10     .sort_values(\"missing_count\", ascending=False)\n     11 )\n\nNameError: name 'df' is not defined</pre> <p>Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns. Justify your choices.</p> <ul> <li><p>Numerical Features (Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck) Use median imputation. Median is robust against outliers (e.g., some passengers spend huge amounts in VRDeck or FoodCourt).</p> </li> <li><p>Categorical Features (HomePlanet, Destination, Cabin) Use most frequent (mode) imputation. This fills missing entries with the most common value, which preserves the categorical distribution. Drop PassengerId and Name as they are identifiers.</p> </li> <li><p>Boolean Features (CryoSleep, VIP) Treat them as categorical and impute with most frequent value. This avoids introducing bias since only ~2% are missing.</p> </li> </ul> In\u00a0[8]: Copied! <pre>from sklearn.impute import SimpleImputer\n\n# Define the columns we will actually use downstream\nnumerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\ncategorical_features = ['HomePlanet', 'Destination', 'Cabin', 'CryoSleep', 'VIP']\n\n# Drop identifier columns from features\nid_cols = ['PassengerId', 'Name']\nX = df.drop(columns=[target_column] + id_cols, errors='ignore').copy()\n\n# Create imputers\nnum_imputer = SimpleImputer(strategy=\"median\")          # robust to outliers in spend columns\ncat_imputer = SimpleImputer(strategy=\"most_frequent\")   # preserves mode for categoricals/booleans\n\n# APPLY the imputers (this was missing)\nX[numerical_features] = num_imputer.fit_transform(X[numerical_features])\nX[categorical_features] = cat_imputer.fit_transform(X[categorical_features])\n\n# Sanity check: no missing values should remain in these groups\nprint(\"Remaining NAs (numeric):\", int(X[numerical_features].isna().sum().sum()))\nprint(\"Remaining NAs (categorical):\", int(X[categorical_features].isna().sum().sum()))\n</pre> from sklearn.impute import SimpleImputer  # Define the columns we will actually use downstream numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] categorical_features = ['HomePlanet', 'Destination', 'Cabin', 'CryoSleep', 'VIP']  # Drop identifier columns from features id_cols = ['PassengerId', 'Name'] X = df.drop(columns=[target_column] + id_cols, errors='ignore').copy()  # Create imputers num_imputer = SimpleImputer(strategy=\"median\")          # robust to outliers in spend columns cat_imputer = SimpleImputer(strategy=\"most_frequent\")   # preserves mode for categoricals/booleans  # APPLY the imputers (this was missing) X[numerical_features] = num_imputer.fit_transform(X[numerical_features]) X[categorical_features] = cat_imputer.fit_transform(X[categorical_features])  # Sanity check: no missing values should remain in these groups print(\"Remaining NAs (numeric):\", int(X[numerical_features].isna().sum().sum())) print(\"Remaining NAs (categorical):\", int(X[categorical_features].isna().sum().sum())) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 9\n      7 # Drop identifier columns from features\n      8 id_cols = ['PassengerId', 'Name']\n----&gt; 9 X = df.drop(columns=[target_column] + id_cols, errors='ignore').copy()\n     11 # Create imputers\n     12 num_imputer = SimpleImputer(strategy=\"median\")          # robust to outliers in spend columns\n\nNameError: name 'df' is not defined</pre> <p>Encode Categorical Features: Convert categorical columns like <code>HomePlanet</code>, <code>CryoSleep</code>, and <code>Destination</code> into a numerical format. One-hot encoding is a good choice.</p> In\u00a0[9]: Copied! <pre>X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\nprint(\"Encoded shape:\", X_encoded.shape)\n</pre> X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True) print(\"Encoded shape:\", X_encoded.shape) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n      2 print(\"Encoded shape:\", X_encoded.shape)\n\nFile /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/core/reshape/encoding.py:227, in get_dummies(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\n    225     result = concat(with_dummies, axis=1)\n    226 else:\n--&gt; 227     result = _get_dummies_1d(\n    228         data,\n    229         prefix,\n    230         prefix_sep,\n    231         dummy_na,\n    232         sparse=sparse,\n    233         drop_first=drop_first,\n    234         dtype=dtype,\n    235     )\n    236 return result\n\nFile /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/core/reshape/encoding.py:251, in _get_dummies_1d(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\n    248 from pandas.core.reshape.concat import concat\n    250 # Series avoids inconsistent NaN handling\n--&gt; 251 codes, levels = factorize_from_iterable(Series(data, copy=False))\n    253 if dtype is None and hasattr(data, \"dtype\"):\n    254     input_dtype = data.dtype\n\nFile /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/core/series.py:584, in Series.__init__(self, data, index, dtype, name, copy, fastpath)\n    582         data = data.copy()\n    583 else:\n--&gt; 584     data = sanitize_array(data, index, dtype, copy)\n    586     manager = _get_option(\"mode.data_manager\", silent=True)\n    587     if manager == \"block\":\n\nFile /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/core/construction.py:656, in sanitize_array(data, index, dtype, copy, allow_2d)\n    653             subarr = cast(np.ndarray, subarr)\n    654             subarr = maybe_infer_to_datetimelike(subarr)\n--&gt; 656 subarr = _sanitize_ndim(subarr, data, dtype, index, allow_2d=allow_2d)\n    658 if isinstance(subarr, np.ndarray):\n    659     # at this point we should have dtype be None or subarr.dtype == dtype\n    660     dtype = cast(np.dtype, dtype)\n\nFile /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/core/construction.py:715, in _sanitize_ndim(result, data, dtype, index, allow_2d)\n    713     if allow_2d:\n    714         return result\n--&gt; 715     raise ValueError(\n    716         f\"Data must be 1-dimensional, got ndarray of shape {data.shape} instead\"\n    717     )\n    718 if is_object_dtype(dtype) and isinstance(dtype, ExtensionDtype):\n    719     # i.e. NumpyEADtype(\"O\")\n    721     result = com.asarray_tuplesafe(data, dtype=np.dtype(\"object\"))\n\nValueError: Data must be 1-dimensional, got ndarray of shape (1000, 5) instead</pre> <p>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., <code>Age</code>, <code>RoomService</code>, etc.). Since the <code>tanh</code> activation function is centered at zero and outputs values in <code>[-1, 1]</code>, **Standardization (to mean 0, std 1) or Normalization to a <code>[-1, 1]</code> range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.**</p> In\u00a0[10]: Copied! <pre>from sklearn.preprocessing import MinMaxScaler\n\n# Scale numerics in the encoded/imputed feature matrix\nscaler = MinMaxScaler(feature_range=(-1, 1))\nX_scaled = X_encoded.copy()\nX_scaled[numerical_features] = scaler.fit_transform(X_scaled[numerical_features])\n\n# Quick preview\nprint(X_scaled[numerical_features].head())\n</pre> from sklearn.preprocessing import MinMaxScaler  # Scale numerics in the encoded/imputed feature matrix scaler = MinMaxScaler(feature_range=(-1, 1)) X_scaled = X_encoded.copy() X_scaled[numerical_features] = scaler.fit_transform(X_scaled[numerical_features])  # Quick preview print(X_scaled[numerical_features].head()) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 5\n      3 # Scale numerics in the encoded/imputed feature matrix\n      4 scaler = MinMaxScaler(feature_range=(-1, 1))\n----&gt; 5 X_scaled = X_encoded.copy()\n      6 X_scaled[numerical_features] = scaler.fit_transform(X_scaled[numerical_features])\n      8 # Quick preview\n\nNameError: name 'X_encoded' is not defined</pre> <p>Why this helps with tanh?</p> <ul> <li><p>tanh outputs in [-1,1] and is most sensitive near 0.</p> </li> <li><p>Scaling inputs into [-1,1] keeps activations in the high-gradient region, improving stability and speed of training.</p> </li> </ul> In\u00a0[11]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.figure()\ndf['FoodCourt'].dropna().hist(bins=30)\nplt.title('FoodCourt \u2014 Before Scaling')\n\nplt.figure()\nX_scaled['FoodCourt'].dropna().hist(bins=30)\nplt.title('FoodCourt \u2014 After Scaling ([-1, 1])')\n\nplt.figure()\ndf['Age'].dropna().hist(bins=30)\nplt.title('Age \u2014 Before Scaling')\n\nplt.figure()\nX_scaled['Age'].dropna().hist(bins=30)\nplt.title('Age \u2014 After Scaling ([-1, 1])')\n</pre> import matplotlib.pyplot as plt  plt.figure() df['FoodCourt'].dropna().hist(bins=30) plt.title('FoodCourt \u2014 Before Scaling')  plt.figure() X_scaled['FoodCourt'].dropna().hist(bins=30) plt.title('FoodCourt \u2014 After Scaling ([-1, 1])')  plt.figure() df['Age'].dropna().hist(bins=30) plt.title('Age \u2014 Before Scaling')  plt.figure() X_scaled['Age'].dropna().hist(bins=30) plt.title('Age \u2014 After Scaling ([-1, 1])')  <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 4\n      1 import matplotlib.pyplot as plt\n      3 plt.figure()\n----&gt; 4 df['FoodCourt'].dropna().hist(bins=30)\n      5 plt.title('FoodCourt \u2014 Before Scaling')\n      7 plt.figure()\n\nNameError: name 'df' is not defined</pre> <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre>"},{"location":"exercises/data/notebook/#data","title":"Data\u00b6","text":""},{"location":"exercises/data/notebook/#exercise-1-exploring-class-separability","title":"Exercise 1 - Exploring Class Separability\u00b6","text":"<p>Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"exercises/data/notebook/#generate-the-data","title":"Generate the Data\u00b6","text":"<p>Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each).</p>"},{"location":"exercises/data/notebook/#plot-the-data","title":"Plot the Data\u00b6","text":"<p>Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable.</p>"},{"location":"exercises/data/notebook/#analyze-and-draw-boundaries","title":"Analyze and Draw Boundaries\u00b6","text":"<p>a. Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.</p>"},{"location":"exercises/data/notebook/#exercise-2-non-linearity-in-higher-dimensions","title":"Exercise 2 - Non-Linearity in Higher Dimensions\u00b6","text":"<p>Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"exercises/data/notebook/#generate-the-data","title":"Generate the Data\u00b6","text":"<p>Create a dataset with 500 samples for Class A and 500 samples for Class B.</p>"},{"location":"exercises/data/notebook/#visualize-the-data","title":"Visualize the Data\u00b6","text":"<p>Since you cannot directly plot a 5D graph, you must reduce its dimensionality.</p> <p>Use a technique like **Principal Component Analysis (PCA) to project the 5D data down to 2 dimensions.** Create a scatter plot of this 2D representation, coloring the points by their class (A or B).</p>"},{"location":"exercises/data/notebook/#analyze-the-plot","title":"Analyze the Plot\u00b6","text":""},{"location":"exercises/data/notebook/#exercise-3-preparing-real-world-data-for-a-neural-network","title":"Exercise 3 - Preparing Real-World Data for a Neural Network\u00b6","text":"<p>This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (tanh) activation function in its hidden layers.</p>"},{"location":"exercises/data/notebook/#get-the-data","title":"Get the Data\u00b6","text":"<p>**Download the Spaceship Titanic dataset from Kaggle.**</p>"},{"location":"exercises/data/notebook/#describe-the-data","title":"Describe the Data\u00b6","text":"<p>Briefly describe the dataset's objective (i.e., what does the <code>Transported</code> column represent?).</p>"},{"location":"exercises/data/notebook/#preprocess-the-data","title":"Preprocess the Data\u00b6","text":"<p>Your goal is to clean and transform the data so it can be fed into a neural network. The <code>tanh</code> activation function produces outputs in the range <code>[-1, 1]</code>, so your input data should be scaled appropriately for stable training.</p>"},{"location":"exercises/data/notebook/#visualize-the-results","title":"Visualize the Results:\u00b6","text":"<p>Create histograms for one or two numerical features (like <code>FoodCourt</code> or <code>Age</code>) **before and after scaling to show the effect of your transformation.**</p>"}]}