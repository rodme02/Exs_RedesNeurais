{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Artificial Neural Networks and Deep Learning - 2025/2","text":"<p>Rodrigo Paoliello de Medeiros</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Data Exercise - 05/09</li> <li> Perceptron Exercise - 12/09</li> <li> MLP Exercise - 19/09</li> <li> Classification Project - 21/09</li> <li> Metrics Exercise - 17/10</li> <li> Regression Project - 19/10</li> <li> Generative Models Project - 16/11</li> </ul>"},{"location":"exercises/data/notebook/","title":"1. Data","text":"In\u00a0[7]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define parameters for each class\nclass_params = {\n    0: {'mean': [2, 3], 'std': [0.8, 2.5]},\n    1: {'mean': [5, 6], 'std': [1.2, 1.9]},\n    2: {'mean': [8, 1], 'std': [0.9, 0.9]},\n    3: {'mean': [15, 4], 'std': [0.5, 2.0]}\n}\n\n# Generate the data\ndata = []\nlabels = []\n\nfor label, params in class_params.items():\n    samples = np.random.randn(100, 2) * params['std'] + params['mean']\n    data.append(samples)\n    labels.append(np.full(100, label))\n\n# Combine into arrays\ndata = np.vstack(data)\nlabels = np.hstack(labels)\n\nprint(\"Dataset generated: \", data.shape, labels.shape)\n</pre> import numpy as np import matplotlib.pyplot as plt  # Set random seed for reproducibility np.random.seed(42)  # Define parameters for each class class_params = {     0: {'mean': [2, 3], 'std': [0.8, 2.5]},     1: {'mean': [5, 6], 'std': [1.2, 1.9]},     2: {'mean': [8, 1], 'std': [0.9, 0.9]},     3: {'mean': [15, 4], 'std': [0.5, 2.0]} }  # Generate the data data = [] labels = []  for label, params in class_params.items():     samples = np.random.randn(100, 2) * params['std'] + params['mean']     data.append(samples)     labels.append(np.full(100, label))  # Combine into arrays data = np.vstack(data) labels = np.hstack(labels)  print(\"Dataset generated: \", data.shape, labels.shape) <pre>Dataset generated:  (400, 2) (400,)\n</pre> In\u00a0[28]: Copied! <pre>plt.figure(figsize=(10, 7))\ncolors = ['red', 'blue', 'green', 'purple']\n\nfor i in range(4):\n    plt.scatter(\n        data[labels == i, 0],\n        data[labels == i, 1],\n        label=f'Class {i}',\n        color=colors[i],\n        alpha=0.7\n    )\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Synthetic Dataset - 4 Classes')\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> plt.figure(figsize=(10, 7)) colors = ['red', 'blue', 'green', 'purple']  for i in range(4):     plt.scatter(         data[labels == i, 0],         data[labels == i, 1],         label=f'Class {i}',         color=colors[i],         alpha=0.7     )  plt.xlabel('Feature 1') plt.ylabel('Feature 2') plt.title('Synthetic Dataset - 4 Classes') plt.legend() plt.grid(True) plt.show() <p>Class 0: Spread around (2,3) with a wider vertical spread due to the higher standard deviation in the y-direction (2.5).</p> <p>Class 1: Clustered near (5,6) but with moderate spread in both directions.</p> <p>Class 2: Compact and tight cluster around (8,1) with almost circular spread due to similar standard deviation in x and y.</p> <p>Class 3: Very concentrated horizontally near x=15 but elongated vertically due to a larger y standar deviation (2.0).</p> <p>There\u2019s clear separation between Class 3 and the others due to its large x mean.</p> <p>Classes 0, 1, and 2 show potential overlap:</p> <p>Class 0 and Class 1 overlap slightly in the higher y-region.</p> <p>Class 2 is more isolated near x=8 and y=1, but some overlap could occur along its boundary with Class 1 if noise increases.</p> <p>b. Based on your visual inspection, could a simple, linear boundary separate all classes?</p> <p>A single linear boundary would not perfectly separate all classes:</p> <p>Class 3 is linearly separable.</p> <p>Classes 0, 1, and 2 are intertwined in the central region, requiring non-linear decision boundaries for clean separation.</p> <p>c. On your plot, sketch the decision boundaries that you think a trained neural network might learn to separate these classes.</p> <p>A trained neural network would learn to separate these classes with non-linear boundaries that might look something like this:</p> <p></p> In\u00a0[11]: Copied! <pre># Parameters for Class A\nmean_A = np.array([0, 0, 0, 0, 0])\ncov_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\n# Parameters for Class B\nmean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\ncov_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n\n# Generate samples\nsamples_A = np.random.multivariate_normal(mean_A, cov_A, size=500)\nsamples_B = np.random.multivariate_normal(mean_B, cov_B, size=500)\n\n# Create labels\nlabels_A = np.zeros(500)  # Class A = 0\nlabels_B = np.ones(500)   # Class B = 1\n\n# Combine data\nX = np.vstack([samples_A, samples_B])\ny = np.hstack([labels_A, labels_B])\n\nprint(\"Dataset shape:\", X.shape)\nprint(\"Labels shape:\", y.shape)\n</pre> # Parameters for Class A mean_A = np.array([0, 0, 0, 0, 0]) cov_A = np.array([     [1.0, 0.8, 0.1, 0.0, 0.0],     [0.8, 1.0, 0.3, 0.0, 0.0],     [0.1, 0.3, 1.0, 0.5, 0.0],     [0.0, 0.0, 0.5, 1.0, 0.2],     [0.0, 0.0, 0.0, 0.2, 1.0] ])  # Parameters for Class B mean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5]) cov_B = np.array([     [1.5, -0.7, 0.2, 0.0, 0.0],     [-0.7, 1.5, 0.4, 0.0, 0.0],     [0.2, 0.4, 1.5, 0.6, 0.0],     [0.0, 0.0, 0.6, 1.5, 0.3],     [0.0, 0.0, 0.0, 0.3, 1.5] ])  # Generate samples samples_A = np.random.multivariate_normal(mean_A, cov_A, size=500) samples_B = np.random.multivariate_normal(mean_B, cov_B, size=500)  # Create labels labels_A = np.zeros(500)  # Class A = 0 labels_B = np.ones(500)   # Class B = 1  # Combine data X = np.vstack([samples_A, samples_B]) y = np.hstack([labels_A, labels_B])  print(\"Dataset shape:\", X.shape) print(\"Labels shape:\", y.shape) <pre>Dataset shape: (1000, 5)\nLabels shape: (1000,)\n</pre> In\u00a0[27]: Copied! <pre>from sklearn.decomposition import PCA\n\n# Apply PCA to reduce dimensions to 2D\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(10, 7))\ncolors = ['red', 'blue']\nclasses = ['A', 'B']\nfor i, color in enumerate(colors):\n    plt.scatter(\n        X_pca[y == i, 0],\n        X_pca[y == i, 1],\n        label=f'Class {classes[i]}',\n        color=color,\n        alpha=0.7\n    )\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.title('PCA Visualization of 5D Data')\nplt.legend()\nplt.grid()\nplt.show()\n</pre> from sklearn.decomposition import PCA  # Apply PCA to reduce dimensions to 2D pca = PCA(n_components=2) X_pca = pca.fit_transform(X)  plt.figure(figsize=(10, 7)) colors = ['red', 'blue'] classes = ['A', 'B'] for i, color in enumerate(colors):     plt.scatter(         X_pca[y == i, 0],         X_pca[y == i, 1],         label=f'Class {classes[i]}',         color=color,         alpha=0.7     ) plt.xlabel('First Principal Component') plt.ylabel('Second Principal Component') plt.title('PCA Visualization of 5D Data') plt.legend() plt.grid() plt.show() <p>a. Based on your 2D projection, describe the relationship between the two classes.</p> <p>The 2D projection shows significant overlap between Classes A and B. The class centers are slightly offset, but their distributions largely intersect. Both classes show similar spread patterns, making it difficult to distinguish clear boundaries between them in this reduced dimensional space.</p> <p>b. Discuss the **linear separability of the data. Explain why this type of data structure poses a challenge for simple linear models and would likely require a multi-layer neural network with non-linear activation functions to be classified accurately.**</p> <p>The 5D dataset presents significant challenges for linear classification due to its complex structure. The PCA visualization reveals substantial overlap between classes, and the different covariance patterns in each class create intricate relationships that a simple linear boundary cannot capture. A multi-layer neural network with non-linear activation functions would be more suitable as it can transform the input space and learn complex decision boundaries that better separate the classes.</p> <p>The Spaceship Titanic dataset is designed for a classification task where the goal is to predict the value of the target variable Transported. This column is boolean:</p> <p>True \u2192 the passenger was transported to another dimension following the spaceship incident.</p> <p>False \u2192 the passenger was not transported and stayed in the original dimension.</p> <p>This problem is analogous to survival prediction on the Titanic dataset but framed in a sci-fi setting.</p> In\u00a0[21]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"spaceship-titanic/train.csv\")\n\ndf.head(5)\n</pre> import pandas as pd  df = pd.read_csv(\"spaceship-titanic/train.csv\")  df.head(5) Out[21]: PassengerId HomePlanet CryoSleep Cabin Destination Age VIP RoomService FoodCourt ShoppingMall Spa VRDeck Name Transported 0 0001_01 Europa False B/0/P TRAPPIST-1e 39.0 False 0.0 0.0 0.0 0.0 0.0 Maham Ofracculy False 1 0002_01 Earth False F/0/S TRAPPIST-1e 24.0 False 109.0 9.0 25.0 549.0 44.0 Juanna Vines True 2 0003_01 Europa False A/0/S TRAPPIST-1e 58.0 True 43.0 3576.0 0.0 6715.0 49.0 Altark Susent False 3 0003_02 Europa False A/0/S TRAPPIST-1e 33.0 False 0.0 1283.0 371.0 3329.0 193.0 Solam Susent False 4 0004_01 Earth False F/1/S TRAPPIST-1e 16.0 False 303.0 70.0 151.0 565.0 2.0 Willy Santantines True <p>List the features and identify which are **numerical (e.g., <code>Age</code>, <code>RoomService</code>) and which are categorical (e.g., <code>HomePlanet</code>, <code>Destination</code>).**</p> In\u00a0[\u00a0]: Copied! <pre>target_column = 'Transported'\ny = df[target_column].astype(int)\nX = df.drop(columns=[target_column])\n\nnumerical_features = X.select_dtypes(include=['number']).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n\nprint(\"\\nNumerical Features:\")\nfor feature in numerical_features:\n    print(f\"- {feature}\")\n\nprint(\"\\nCategorical Features:\")\nfor feature in categorical_features:\n    print(f\"- {feature}\")\n</pre> target_column = 'Transported' y = df[target_column].astype(int) X = df.drop(columns=[target_column])  numerical_features = X.select_dtypes(include=['number']).columns.tolist() categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()  print(\"\\nNumerical Features:\") for feature in numerical_features:     print(f\"- {feature}\")  print(\"\\nCategorical Features:\") for feature in categorical_features:     print(f\"- {feature}\") <pre>\nNumerical Features:\n- Age\n- RoomService\n- FoodCourt\n- ShoppingMall\n- Spa\n- VRDeck\n\nCategorical Features:\n- PassengerId\n- HomePlanet\n- CryoSleep\n- Cabin\n- Destination\n- VIP\n- Name\n</pre> <p>Investigate the dataset for **missing values. Which columns have them, and how many**</p> In\u00a0[23]: Copied! <pre>missing_counts = df.isna().sum()\nn_rows = len(df)\n\nmissing_table = (\n    pd.DataFrame({\n        \"missing_count\": missing_counts,\n        \"missing_pct\": (missing_counts / n_rows * 100).round(2)\n    })\n    .query(\"missing_count &gt; 0\")\n    .sort_values(\"missing_count\", ascending=False)\n)\n\nprint(\"\\nMissing Values\")\nprint(missing_table.to_string(float_format=lambda x: f\"{x:,.2f}%\"))\n</pre> missing_counts = df.isna().sum() n_rows = len(df)  missing_table = (     pd.DataFrame({         \"missing_count\": missing_counts,         \"missing_pct\": (missing_counts / n_rows * 100).round(2)     })     .query(\"missing_count &gt; 0\")     .sort_values(\"missing_count\", ascending=False) )  print(\"\\nMissing Values\") print(missing_table.to_string(float_format=lambda x: f\"{x:,.2f}%\")) <pre>\nMissing Values\n              missing_count  missing_pct\nCryoSleep               217        2.50%\nShoppingMall            208        2.39%\nVIP                     203        2.34%\nHomePlanet              201        2.31%\nName                    200        2.30%\nCabin                   199        2.29%\nVRDeck                  188        2.16%\nFoodCourt               183        2.11%\nSpa                     183        2.11%\nDestination             182        2.09%\nRoomService             181        2.08%\nAge                     179        2.06%\n</pre> <p>Handle Missing Data: Devise and implement a strategy to handle the missing values in all the affected columns. Justify your choices.</p> <ul> <li><p>Numerical Features (Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck) Use median imputation. Median is robust against outliers (e.g., some passengers spend huge amounts in VRDeck or FoodCourt).</p> </li> <li><p>Categorical Features (HomePlanet, Destination, Cabin) Use most frequent (mode) imputation. This fills missing entries with the most common value, which preserves the categorical distribution. Drop PassengerId and Name as they are identifiers.</p> </li> <li><p>Boolean Features (CryoSleep, VIP) Treat them as categorical and impute with most frequent value. This avoids introducing bias since only ~2% are missing.</p> </li> </ul> In\u00a0[26]: Copied! <pre>from sklearn.impute import SimpleImputer\n\n# Define the columns we will actually use downstream\nnumerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\ncategorical_features = ['HomePlanet', 'Destination', 'Cabin', 'CryoSleep', 'VIP']\n\n# Drop identifier columns from features\nid_cols = ['PassengerId', 'Name']\nX = df.drop(columns=[target_column] + id_cols, errors='ignore').copy()\n\n# Create imputers\nnum_imputer = SimpleImputer(strategy=\"median\")          # robust to outliers in spend columns\ncat_imputer = SimpleImputer(strategy=\"most_frequent\")   # preserves mode for categoricals/booleans\n\n# APPLY the imputers (this was missing)\nX[numerical_features] = num_imputer.fit_transform(X[numerical_features])\nX[categorical_features] = cat_imputer.fit_transform(X[categorical_features])\n\n# Sanity check: no missing values should remain in these groups\nprint(\"Remaining NAs (numeric):\", int(X[numerical_features].isna().sum().sum()))\nprint(\"Remaining NAs (categorical):\", int(X[categorical_features].isna().sum().sum()))\n</pre> from sklearn.impute import SimpleImputer  # Define the columns we will actually use downstream numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] categorical_features = ['HomePlanet', 'Destination', 'Cabin', 'CryoSleep', 'VIP']  # Drop identifier columns from features id_cols = ['PassengerId', 'Name'] X = df.drop(columns=[target_column] + id_cols, errors='ignore').copy()  # Create imputers num_imputer = SimpleImputer(strategy=\"median\")          # robust to outliers in spend columns cat_imputer = SimpleImputer(strategy=\"most_frequent\")   # preserves mode for categoricals/booleans  # APPLY the imputers (this was missing) X[numerical_features] = num_imputer.fit_transform(X[numerical_features]) X[categorical_features] = cat_imputer.fit_transform(X[categorical_features])  # Sanity check: no missing values should remain in these groups print(\"Remaining NAs (numeric):\", int(X[numerical_features].isna().sum().sum())) print(\"Remaining NAs (categorical):\", int(X[categorical_features].isna().sum().sum())) <pre>Remaining NAs (numeric): 0\nRemaining NAs (categorical): 0\n</pre> <p>Encode Categorical Features: Convert categorical columns like <code>HomePlanet</code>, <code>CryoSleep</code>, and <code>Destination</code> into a numerical format. One-hot encoding is a good choice.</p> In\u00a0[28]: Copied! <pre>X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\nprint(\"Encoded shape:\", X_encoded.shape)\n</pre> X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True) print(\"Encoded shape:\", X_encoded.shape) <pre>Encoded shape: (8693, 6571)\n</pre> <p>Normalize/Standardize Numerical Features: Scale the numerical columns (e.g., <code>Age</code>, <code>RoomService</code>, etc.). Since the <code>tanh</code> activation function is centered at zero and outputs values in <code>[-1, 1]</code>, **Standardization (to mean 0, std 1) or Normalization to a <code>[-1, 1]</code> range are excellent choices. Implement one and explain why it is a good practice for training neural networks with this activation function.**</p> In\u00a0[29]: Copied! <pre>from sklearn.preprocessing import MinMaxScaler\n\n# Scale numerics in the encoded/imputed feature matrix\nscaler = MinMaxScaler(feature_range=(-1, 1))\nX_scaled = X_encoded.copy()\nX_scaled[numerical_features] = scaler.fit_transform(X_scaled[numerical_features])\n\n# Quick preview\nprint(X_scaled[numerical_features].head())\n</pre> from sklearn.preprocessing import MinMaxScaler  # Scale numerics in the encoded/imputed feature matrix scaler = MinMaxScaler(feature_range=(-1, 1)) X_scaled = X_encoded.copy() X_scaled[numerical_features] = scaler.fit_transform(X_scaled[numerical_features])  # Quick preview print(X_scaled[numerical_features].head()) <pre>        Age  RoomService  FoodCourt  ShoppingMall       Spa    VRDeck\n0 -0.012658    -1.000000  -1.000000     -1.000000 -1.000000 -1.000000\n1 -0.392405    -0.984784  -0.999396     -0.997872 -0.951000 -0.996354\n2  0.468354    -0.993997  -0.760105     -1.000000 -0.400660 -0.995939\n3 -0.164557    -1.000000  -0.913930     -0.968415 -0.702874 -0.984005\n4 -0.594937    -0.957702  -0.995304     -0.987145 -0.949572 -0.999834\n</pre> <p>Why this helps with tanh?</p> <ul> <li><p>tanh outputs in [-1,1] and is most sensitive near 0.</p> </li> <li><p>Scaling inputs into [-1,1] keeps activations in the high-gradient region, improving stability and speed of training.</p> </li> </ul> In\u00a0[34]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.figure()\ndf['FoodCourt'].dropna().hist(bins=30)\nplt.title('FoodCourt \u2014 Before Scaling')\n\nplt.figure()\nX_scaled['FoodCourt'].dropna().hist(bins=30)\nplt.title('FoodCourt \u2014 After Scaling ([-1, 1])')\n\nplt.figure()\ndf['Age'].dropna().hist(bins=30)\nplt.title('Age \u2014 Before Scaling')\n\nplt.figure()\nX_scaled['Age'].dropna().hist(bins=30)\nplt.title('Age \u2014 After Scaling ([-1, 1])')\n</pre> import matplotlib.pyplot as plt  plt.figure() df['FoodCourt'].dropna().hist(bins=30) plt.title('FoodCourt \u2014 Before Scaling')  plt.figure() X_scaled['FoodCourt'].dropna().hist(bins=30) plt.title('FoodCourt \u2014 After Scaling ([-1, 1])')  plt.figure() df['Age'].dropna().hist(bins=30) plt.title('Age \u2014 Before Scaling')  plt.figure() X_scaled['Age'].dropna().hist(bins=30) plt.title('Age \u2014 After Scaling ([-1, 1])')  Out[34]: <pre>Text(0.5, 1.0, 'Age \u2014 After Scaling ([-1, 1])')</pre>"},{"location":"exercises/data/notebook/#data","title":"Data\u00b6","text":""},{"location":"exercises/data/notebook/#exercise-1-exploring-class-separability","title":"Exercise 1 - Exploring Class Separability\u00b6","text":"<p>Understanding how data is distributed is the first step before designing a network architecture. In this exercise, you will generate and visualize a two-dimensional dataset to explore how data distribution affects the complexity of the decision boundaries a neural network would need to learn.</p>"},{"location":"exercises/data/notebook/#generate-the-data","title":"Generate the Data\u00b6","text":"<p>Create a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each).</p>"},{"location":"exercises/data/notebook/#plot-the-data","title":"Plot the Data\u00b6","text":"<p>Create a 2D scatter plot showing all the data points. Use a different color for each class to make them distinguishable.</p>"},{"location":"exercises/data/notebook/#analyze-and-draw-boundaries","title":"Analyze and Draw Boundaries\u00b6","text":"<p>a. Examine the scatter plot carefully. Describe the distribution and overlap of the four classes.</p>"},{"location":"exercises/data/notebook/#exercise-2-non-linearity-in-higher-dimensions","title":"Exercise 2 - Non-Linearity in Higher Dimensions\u00b6","text":"<p>Simple neural networks (like a Perceptron) can only learn linear boundaries. Deep networks excel when data is not linearly separable. This exercise challenges you to create and visualize such a dataset.</p>"},{"location":"exercises/data/notebook/#generate-the-data","title":"Generate the Data\u00b6","text":"<p>Create a dataset with 500 samples for Class A and 500 samples for Class B.</p>"},{"location":"exercises/data/notebook/#visualize-the-data","title":"Visualize the Data\u00b6","text":"<p>Since you cannot directly plot a 5D graph, you must reduce its dimensionality.</p> <p>Use a technique like **Principal Component Analysis (PCA) to project the 5D data down to 2 dimensions.** Create a scatter plot of this 2D representation, coloring the points by their class (A or B).</p>"},{"location":"exercises/data/notebook/#analyze-the-plot","title":"Analyze the Plot\u00b6","text":""},{"location":"exercises/data/notebook/#exercise-3-preparing-real-world-data-for-a-neural-network","title":"Exercise 3 - Preparing Real-World Data for a Neural Network\u00b6","text":"<p>This exercise uses a real dataset from Kaggle. Your task is to perform the necessary preprocessing to make it suitable for a neural network that uses the hyperbolic tangent (tanh) activation function in its hidden layers.</p>"},{"location":"exercises/data/notebook/#get-the-data","title":"Get the Data\u00b6","text":"<p>**Download the Spaceship Titanic dataset from Kaggle.**</p>"},{"location":"exercises/data/notebook/#describe-the-data","title":"Describe the Data\u00b6","text":"<p>Briefly describe the dataset's objective (i.e., what does the <code>Transported</code> column represent?).</p>"},{"location":"exercises/data/notebook/#preprocess-the-data","title":"Preprocess the Data\u00b6","text":"<p>Your goal is to clean and transform the data so it can be fed into a neural network. The <code>tanh</code> activation function produces outputs in the range <code>[-1, 1]</code>, so your input data should be scaled appropriately for stable training.</p>"},{"location":"exercises/data/notebook/#visualize-the-results","title":"Visualize the Results:\u00b6","text":"<p>Create histograms for one or two numerical features (like <code>FoodCourt</code> or <code>Age</code>) **before and after scaling to show the effect of your transformation.**</p>"}]}