{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Artificial Neural Networks and Deep Learning - 2025/2","text":"<p>Rodrigo Paoliello de Medeiros</p>"},{"location":"#deliverables","title":"Deliverables","text":"<ul> <li> Data Exercise</li> <li> Perceptron Exercise</li> <li> MLP Exercise</li> <li> Classification Project</li> <li> Metrics Exercise</li> <li> Regression Project</li> <li> Generative Models Project</li> </ul>"},{"location":"exercises/data/notebook/","title":"1. Data","text":"<p>First we will generate a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each), using a Gaussian distribution based on the given means and standard deviations:</p> In\u00a0[99]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define parameters for each class\nclass_params = {\n    0: {'mean': [2, 3], 'std': [0.8, 2.5]},\n    1: {'mean': [5, 6], 'std': [1.2, 1.9]},\n    2: {'mean': [8, 1], 'std': [0.9, 0.9]},\n    3: {'mean': [15, 4], 'std': [0.5, 2.0]}\n}\n\n# Generate the data\ndata = []\nlabels = []\n\n# Gaussian distributions for each class\nfor label, params in class_params.items():\n    points = np.random.normal(loc=params['mean'], scale=params['std'], size=(100, 2))\n    data.append(points)\n    labels.append(np.full(100, label))\n\n# Combine into arrays\ndata = np.vstack(data)\nlabels = np.hstack(labels)\n\nprint(\"Dataset generated: \", data.shape, labels.shape)\n</pre> import numpy as np import matplotlib.pyplot as plt  # Set random seed for reproducibility np.random.seed(42)  # Define parameters for each class class_params = {     0: {'mean': [2, 3], 'std': [0.8, 2.5]},     1: {'mean': [5, 6], 'std': [1.2, 1.9]},     2: {'mean': [8, 1], 'std': [0.9, 0.9]},     3: {'mean': [15, 4], 'std': [0.5, 2.0]} }  # Generate the data data = [] labels = []  # Gaussian distributions for each class for label, params in class_params.items():     points = np.random.normal(loc=params['mean'], scale=params['std'], size=(100, 2))     data.append(points)     labels.append(np.full(100, label))  # Combine into arrays data = np.vstack(data) labels = np.hstack(labels)  print(\"Dataset generated: \", data.shape, labels.shape) <pre>Dataset generated:  (400, 2) (400,)\n</pre> <p>We will now plot a 2D scatter plot showing all the data points, with a different color for each class:</p> In\u00a0[100]: Copied! <pre>plt.figure(figsize=(10, 7))\ncolors = ['red', 'blue', 'green', 'purple']\n\n# Plot each class with different colors\nfor i in range(4):\n    plt.scatter(\n        data[labels == i, 0],\n        data[labels == i, 1],\n        label=f'Class {i}',\n        color=colors[i],\n        alpha=0.7\n    )\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Synthetic Dataset - 4 Classes')\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> plt.figure(figsize=(10, 7)) colors = ['red', 'blue', 'green', 'purple']  # Plot each class with different colors for i in range(4):     plt.scatter(         data[labels == i, 0],         data[labels == i, 1],         label=f'Class {i}',         color=colors[i],         alpha=0.7     )  plt.xlabel('Feature 1') plt.ylabel('Feature 2') plt.title('Synthetic Dataset - 4 Classes') plt.legend() plt.grid(True) plt.show() <p>The synthetic dataset forms four Gaussian clusters with distinct shapes and spreads. Class 0 is centered near (2, 3) with a wide vertical spread due to its larger variance in the y-direction, while Class 1 lies near (5, 6) with moderate spread in both axes. Class 2 is a compact, almost circular cluster at (8, 1), and Class 3 is tightly concentrated around x = 15 but elongated vertically. Class 3 is clearly isolated because of its much larger x-mean, while Classes 0, 1, and 2 occupy overlapping regions in the central space. Class 0 and Class 1 overlap in the higher y-region, and although Class 2 is generally separate, its boundary edges could touch Class 1 if variance increases.</p> <p>This arrangement shows that purely linear separation is not feasible: Class 3 could be split off by a vertical line, but Classes 0, 1, and 2 require non-linear boundaries. A neural network, such as an MLP with <code>tanh</code> activations, would likely learn curved, flexible decision regions: bending around Class 2, carving out Classes 0 and 1 in the upper region, and isolating Class 3 on the far right. The sketch below illustrates what such non-linear boundaries might look like:</p> <p></p> <p>We'll create a synthetic dataset with 500 samples for Class A and 500 samples for Class B, using a multivariate normal distribution with the parameters provided:</p> In\u00a0[101]: Copied! <pre># Parameters for Class A\nmean_A = np.array([0, 0, 0, 0, 0])\ncov_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\n# Parameters for Class B\nmean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\ncov_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n\n# Generate samples using multivariate normal distribution\nsamples_A = np.random.multivariate_normal(mean_A, cov_A, size=500)\nsamples_B = np.random.multivariate_normal(mean_B, cov_B, size=500)\n\n# Create labels\nlabels_A = np.zeros(500)  # Class A = 0\nlabels_B = np.ones(500)   # Class B = 1\n\n# Combine data\nX = np.vstack([samples_A, samples_B])\ny = np.hstack([labels_A, labels_B])\n\nprint(\"Dataset shape:\", X.shape)\nprint(\"Labels shape:\", y.shape)\n</pre> # Parameters for Class A mean_A = np.array([0, 0, 0, 0, 0]) cov_A = np.array([     [1.0, 0.8, 0.1, 0.0, 0.0],     [0.8, 1.0, 0.3, 0.0, 0.0],     [0.1, 0.3, 1.0, 0.5, 0.0],     [0.0, 0.0, 0.5, 1.0, 0.2],     [0.0, 0.0, 0.0, 0.2, 1.0] ])  # Parameters for Class B mean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5]) cov_B = np.array([     [1.5, -0.7, 0.2, 0.0, 0.0],     [-0.7, 1.5, 0.4, 0.0, 0.0],     [0.2, 0.4, 1.5, 0.6, 0.0],     [0.0, 0.0, 0.6, 1.5, 0.3],     [0.0, 0.0, 0.0, 0.3, 1.5] ])  # Generate samples using multivariate normal distribution samples_A = np.random.multivariate_normal(mean_A, cov_A, size=500) samples_B = np.random.multivariate_normal(mean_B, cov_B, size=500)  # Create labels labels_A = np.zeros(500)  # Class A = 0 labels_B = np.ones(500)   # Class B = 1  # Combine data X = np.vstack([samples_A, samples_B]) y = np.hstack([labels_A, labels_B])  print(\"Dataset shape:\", X.shape) print(\"Labels shape:\", y.shape) <pre>Dataset shape: (1000, 5)\nLabels shape: (1000,)\n</pre> <p>Since we cannot plot a 5D graph, we will use Principal Component Analysis (PCA) to project the 5D data down to 2 dimensions. Then we'll create a scatter plot of this 2D representation, with Class A represented by red points and Class B being represented as blue points:</p> In\u00a0[102]: Copied! <pre>from sklearn.decomposition import PCA\n\n# Apply PCA to reduce dimensions to 2D\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Scatter plot\nplt.figure(figsize=(10, 7))\ncolors = ['red', 'blue']\nclasses = ['A', 'B']\nfor i, color in enumerate(colors):\n    plt.scatter(\n        X_pca[y == i, 0],\n        X_pca[y == i, 1],\n        label=f'Class {classes[i]}',\n        color=color,\n        alpha=0.7\n    )\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.title('PCA Visualization of 5D Data')\nplt.legend()\nplt.grid()\nplt.show()\n</pre> from sklearn.decomposition import PCA  # Apply PCA to reduce dimensions to 2D pca = PCA(n_components=2) X_pca = pca.fit_transform(X)  # Scatter plot plt.figure(figsize=(10, 7)) colors = ['red', 'blue'] classes = ['A', 'B'] for i, color in enumerate(colors):     plt.scatter(         X_pca[y == i, 0],         X_pca[y == i, 1],         label=f'Class {classes[i]}',         color=color,         alpha=0.7     ) plt.xlabel('First Principal Component') plt.ylabel('Second Principal Component') plt.title('PCA Visualization of 5D Data') plt.legend() plt.grid() plt.show() <p>The PCA projection of the 5D dataset reveals that Classes A and B overlap substantially in two dimensions. Their centers are slightly offset, but the distributions largely intersect, and both classes display similar spread patterns. This means that in the reduced 2D space there are no clean visual boundaries to separate them.</p> <p>This overlap reflects the underlying challenge: the full 5D structure is governed by different covariance patterns in each class, producing relationships that are not well captured by straight lines. A simple linear classifier cannot resolve such intertwined regions, as any single hyperplane would misclassify a significant portion of the data. To achieve better separation, a more expressive model is needed. A multi-layer neural network with non-linear activation functions can transform the input space into higher-order representations, bending decision boundaries around the overlapping regions. Such non-linear models are better suited to capture the complex geometry of the dataset, making accurate classification feasible where linear methods fall short.</p> <p>First, we'll download the Spaceship Titanic dataset from Kaggle, especifically the <code>train.csv</code>, since we're only using this for preparation.</p> <p>The Spaceship Titanic dataset is a sci-fi reimagining of the classic Titanic survival prediction task. It is framed as a binary classification problem, where the target column <code>Transported</code> indicates whether a passenger was transported to another dimension (<code>True</code>) or remained in the original dimension (<code>False</code>) following the spaceship incident.</p> <p>The training file, <code>train.csv</code>, contains records for roughly two-thirds of the ~8,700 passengers. Each passenger is identified by a unique <code>PassengerId</code> that encodes group membership (<code>gggg_pp</code>, where <code>gggg</code> is the group and <code>pp</code> is the index within that group). Groups often represent families or traveling companions.</p> <p>The dataset provides a mix of demographic, behavioral, and voyage-related features:</p> <ul> <li>HomePlanet \u2014 Planet of origin (permanent residence).</li> <li>CryoSleep \u2014 Whether the passenger elected suspended animation for the voyage.</li> <li>Cabin \u2014 Passenger cabin in the format <code>deck/num/side</code>, where <code>side</code> is <code>P</code> (Port) or <code>S</code> (Starboard).</li> <li>Destination \u2014 Planet of debarkation.</li> <li>Age \u2014 Passenger\u2019s age in years.</li> <li>VIP \u2014 Whether the passenger paid for special VIP service.</li> <li>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck \u2014 Expenditures at various luxury amenities onboard.</li> <li>Name \u2014 Passenger\u2019s full name (not directly predictive).</li> <li>Transported \u2014 Target variable: <code>True</code> if transported to another dimension, <code>False</code> otherwise.</li> </ul> <p>Together, these variables form a rich dataset combining categorical, numerical, and textual features. The challenge lies in preprocessing and modeling these attributes effectively to predict the outcome <code>Transported</code>. The task is analogous to Titanic survival prediction but recast in a futuristic setting.</p> In\u00a0[103]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"spaceship-titanic/train.csv\")\n\ndf.head(5)\n</pre> import pandas as pd  df = pd.read_csv(\"spaceship-titanic/train.csv\")  df.head(5) Out[103]: PassengerId HomePlanet CryoSleep Cabin Destination Age VIP RoomService FoodCourt ShoppingMall Spa VRDeck Name Transported 0 0001_01 Europa False B/0/P TRAPPIST-1e 39.0 False 0.0 0.0 0.0 0.0 0.0 Maham Ofracculy False 1 0002_01 Earth False F/0/S TRAPPIST-1e 24.0 False 109.0 9.0 25.0 549.0 44.0 Juanna Vines True 2 0003_01 Europa False A/0/S TRAPPIST-1e 58.0 True 43.0 3576.0 0.0 6715.0 49.0 Altark Susent False 3 0003_02 Europa False A/0/S TRAPPIST-1e 33.0 False 0.0 1283.0 371.0 3329.0 193.0 Solam Susent False 4 0004_01 Earth False F/1/S TRAPPIST-1e 16.0 False 303.0 70.0 151.0 565.0 2.0 Willy Santantines True <p>Let's list all the numerical and categorical features of this dataset:</p> In\u00a0[104]: Copied! <pre># Separate features and target\ntarget_column = 'Transported'\ny = df[target_column].astype(int)\nX = df.drop(columns=[target_column])\n\n# Identify numerical and categorical features\nnumerical_features = X.select_dtypes(include=['number']).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n\nprint(\"\\nNumerical Features:\")\nfor feature in numerical_features:\n    print(f\"- {feature}\")\n\nprint(\"\\nCategorical Features:\")\nfor feature in categorical_features:\n    print(f\"- {feature}\")\n</pre> # Separate features and target target_column = 'Transported' y = df[target_column].astype(int) X = df.drop(columns=[target_column])  # Identify numerical and categorical features numerical_features = X.select_dtypes(include=['number']).columns.tolist() categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()  print(\"\\nNumerical Features:\") for feature in numerical_features:     print(f\"- {feature}\")  print(\"\\nCategorical Features:\") for feature in categorical_features:     print(f\"- {feature}\") <pre>\nNumerical Features:\n- Age\n- RoomService\n- FoodCourt\n- ShoppingMall\n- Spa\n- VRDeck\n\nCategorical Features:\n- PassengerId\n- HomePlanet\n- CryoSleep\n- Cabin\n- Destination\n- VIP\n- Name\n</pre> <p>We'll now investigate the dataset for missing values:</p> In\u00a0[105]: Copied! <pre># Count missing values\nmissing_counts = df.isna().sum()\nn_rows = len(df)\n\n# Create a table for missing values\nmissing_table = (\n    pd.DataFrame({\n        \"missing_count\": missing_counts,\n        \"missing_pct\": (missing_counts / n_rows * 100).round(2)\n    })\n    .query(\"missing_count &gt; 0\")\n    .sort_values(\"missing_count\", ascending=False)\n)\n\nprint(\"\\nMissing Values\")\nprint(missing_table.to_string(float_format=lambda x: f\"{x:,.2f}%\"))\n</pre> # Count missing values missing_counts = df.isna().sum() n_rows = len(df)  # Create a table for missing values missing_table = (     pd.DataFrame({         \"missing_count\": missing_counts,         \"missing_pct\": (missing_counts / n_rows * 100).round(2)     })     .query(\"missing_count &gt; 0\")     .sort_values(\"missing_count\", ascending=False) )  print(\"\\nMissing Values\") print(missing_table.to_string(float_format=lambda x: f\"{x:,.2f}%\")) <pre>\nMissing Values\n              missing_count  missing_pct\nCryoSleep               217        2.50%\nShoppingMall            208        2.39%\nVIP                     203        2.34%\nHomePlanet              201        2.31%\nName                    200        2.30%\nCabin                   199        2.29%\nVRDeck                  188        2.16%\nFoodCourt               183        2.11%\nSpa                     183        2.11%\nDestination             182        2.09%\nRoomService             181        2.08%\nAge                     179        2.06%\n</pre> <p>We will now clean and transform the data so it can be fed into a neural network. The <code>tanh</code> activation function produces outputs in the range <code>[-1, 1]</code>, so the input data should be scaled appropriately for stable training.</p> <p>First we'll implement a strategy to handle the missing values in all the affected columns:</p> <ul> <li><p>Numerical Features (Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck): Use median imputation. Median is robust against outliers (e.g., some passengers spend huge amounts in VRDeck or FoodCourt).</p> </li> <li><p>Categorical Features (HomePlanet, Destination, Cabin): Use most frequent (mode) imputation. This fills missing entries with the most common value, which preserves the categorical distribution. Drop PassengerId and Name as they are identifiers.</p> </li> <li><p>Boolean Features (CryoSleep, VIP): Treat them as categorical and impute with most frequent value. This avoids introducing bias since only ~2% are missing.</p> </li> </ul> In\u00a0[106]: Copied! <pre>from sklearn.impute import SimpleImputer\n\n# Define the columns we will actually use downstream\nnumerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\ncategorical_features = ['HomePlanet', 'Destination', 'Cabin', 'CryoSleep', 'VIP']\n\n# Drop identifier columns from features\nid_cols = ['PassengerId', 'Name']\nX = df.drop(columns=[target_column] + id_cols, errors='ignore').copy()\n\n# Create imputers\nnum_imputer = SimpleImputer(strategy=\"median\")          # robust to outliers in numericals\ncat_imputer = SimpleImputer(strategy=\"most_frequent\")   # preserves mode for categoricals/booleans\n\n# Apply the imputers\nX[numerical_features] = num_imputer.fit_transform(X[numerical_features])\nX[categorical_features] = cat_imputer.fit_transform(X[categorical_features])\n\n# Sanity check: no missing values should remain in these groups\nprint(\"Remaining NAs (numeric):\", int(X[numerical_features].isna().sum().sum()))\nprint(\"Remaining NAs (categorical):\", int(X[categorical_features].isna().sum().sum()))\n</pre> from sklearn.impute import SimpleImputer  # Define the columns we will actually use downstream numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] categorical_features = ['HomePlanet', 'Destination', 'Cabin', 'CryoSleep', 'VIP']  # Drop identifier columns from features id_cols = ['PassengerId', 'Name'] X = df.drop(columns=[target_column] + id_cols, errors='ignore').copy()  # Create imputers num_imputer = SimpleImputer(strategy=\"median\")          # robust to outliers in numericals cat_imputer = SimpleImputer(strategy=\"most_frequent\")   # preserves mode for categoricals/booleans  # Apply the imputers X[numerical_features] = num_imputer.fit_transform(X[numerical_features]) X[categorical_features] = cat_imputer.fit_transform(X[categorical_features])  # Sanity check: no missing values should remain in these groups print(\"Remaining NAs (numeric):\", int(X[numerical_features].isna().sum().sum())) print(\"Remaining NAs (categorical):\", int(X[categorical_features].isna().sum().sum())) <pre>Remaining NAs (numeric): 0\nRemaining NAs (categorical): 0\n</pre> <p>Now, we'll encode categorical features into a numerical format using one-hot encoding with <code>pd.get_dummies()</code>, which creates binary columns for each category:</p> In\u00a0[107]: Copied! <pre># One-hot encode categorical features\nX_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\nprint(\"Encoded shape:\", X_encoded.shape)\n</pre> # One-hot encode categorical features X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True) print(\"Encoded shape:\", X_encoded.shape) <pre>Encoded shape: (8693, 6571)\n</pre> <p>We will now scale the numerical variables. Because the <code>tanh</code> activation function is centered at zero and outputs values in the range [-1, 1], bringing inputs onto a similar scale is essential. Scaling prevents features with large ranges from dominating learning, stabilizes gradient updates, and accelerates convergence.</p> <p>Here we normalize values to [-1, 1], aligning the inputs with the activation\u2019s range. This practice improves training efficiency and helps the network learn more reliable non-linear decision boundaries.</p> In\u00a0[108]: Copied! <pre>from sklearn.preprocessing import MinMaxScaler\n\n# Scale numerical features in the encoded/imputed feature matrix\nscaler = MinMaxScaler(feature_range=(-1, 1))\nX_scaled = X_encoded.copy()\nX_scaled[numerical_features] = scaler.fit_transform(X_scaled[numerical_features])\n\n# Quick preview\nprint(X_scaled[numerical_features].head())\n</pre> from sklearn.preprocessing import MinMaxScaler  # Scale numerical features in the encoded/imputed feature matrix scaler = MinMaxScaler(feature_range=(-1, 1)) X_scaled = X_encoded.copy() X_scaled[numerical_features] = scaler.fit_transform(X_scaled[numerical_features])  # Quick preview print(X_scaled[numerical_features].head()) <pre>        Age  RoomService  FoodCourt  ShoppingMall       Spa    VRDeck\n0 -0.012658    -1.000000  -1.000000     -1.000000 -1.000000 -1.000000\n1 -0.392405    -0.984784  -0.999396     -0.997872 -0.951000 -0.996354\n2  0.468354    -0.993997  -0.760105     -1.000000 -0.400660 -0.995939\n3 -0.164557    -1.000000  -0.913930     -0.968415 -0.702874 -0.984005\n4 -0.594937    -0.957702  -0.995304     -0.987145 -0.949572 -0.999834\n</pre> <p>We'll now create histograms for <code>FoodCourt</code> and <code>Age</code> before and after scaling to show the difference, the values should be between [-1, 1] instead of their original values, following the same distribution:</p> In\u00a0[109]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 1, figsize=(16, 10))\n\n# FoodCourt before\ndf['FoodCourt'].dropna().hist(bins=30, ax=axes[0])\naxes[0].set_title('FoodCourt \u2014 Before Scaling')\n\n# FoodCourt after\nX_scaled['FoodCourt'].dropna().hist(bins=30, ax=axes[1])\naxes[1].set_title('FoodCourt \u2014 After Scaling ([-1, 1])')\n\nplt.show()\n\nfig, axes = plt.subplots(2, 1, figsize=(16, 10))\n\n# Age before\ndf['Age'].dropna().hist(bins=30, ax=axes[0])\naxes[0].set_title('Age \u2014 Before Scaling')\n\n# Age after\nX_scaled['Age'].dropna().hist(bins=30, ax=axes[1])\naxes[1].set_title('Age \u2014 After Scaling ([-1, 1])')\n\nplt.show()\n</pre> import matplotlib.pyplot as plt  fig, axes = plt.subplots(2, 1, figsize=(16, 10))  # FoodCourt before df['FoodCourt'].dropna().hist(bins=30, ax=axes[0]) axes[0].set_title('FoodCourt \u2014 Before Scaling')  # FoodCourt after X_scaled['FoodCourt'].dropna().hist(bins=30, ax=axes[1]) axes[1].set_title('FoodCourt \u2014 After Scaling ([-1, 1])')  plt.show()  fig, axes = plt.subplots(2, 1, figsize=(16, 10))  # Age before df['Age'].dropna().hist(bins=30, ax=axes[0]) axes[0].set_title('Age \u2014 Before Scaling')  # Age after X_scaled['Age'].dropna().hist(bins=30, ax=axes[1]) axes[1].set_title('Age \u2014 After Scaling ([-1, 1])')  plt.show()"},{"location":"exercises/data/notebook/#data","title":"Data\u00b6","text":""},{"location":"exercises/data/notebook/#exercise-1-exploring-class-separability","title":"Exercise 1 - Exploring Class Separability\u00b6","text":""},{"location":"exercises/data/notebook/#generate-the-data","title":"Generate the Data\u00b6","text":""},{"location":"exercises/data/notebook/#plot-the-data","title":"Plot the Data\u00b6","text":""},{"location":"exercises/data/notebook/#exercise-2-non-linearity-in-higher-dimensions","title":"Exercise 2 - Non-Linearity in Higher Dimensions\u00b6","text":""},{"location":"exercises/data/notebook/#generate-the-data","title":"Generate the Data\u00b6","text":""},{"location":"exercises/data/notebook/#visualize-the-data","title":"Visualize the Data\u00b6","text":""},{"location":"exercises/data/notebook/#analyze-the-plot","title":"Analyze the Plot\u00b6","text":""},{"location":"exercises/data/notebook/#exercise-3-preparing-real-world-data-for-a-neural-network","title":"Exercise 3 - Preparing Real-World Data for a Neural Network\u00b6","text":""},{"location":"exercises/data/notebook/#get-the-data","title":"Get the Data\u00b6","text":""},{"location":"exercises/data/notebook/#describe-the-data","title":"Describe the Data\u00b6","text":""},{"location":"exercises/data/notebook/#preprocess-the-data","title":"Preprocess the Data\u00b6","text":""},{"location":"exercises/data/notebook/#visualize-the-results","title":"Visualize the Results\u00b6","text":""},{"location":"exercises/data/notebook/#ai-assistance","title":"AI Assistance\u00b6","text":"<p>I used AI (ChatGPT) to help with:</p> <ul> <li>Reviewing my code and outputs for clarity.</li> <li>Suggesting improvements to formatting and readability.</li> </ul> <p>All code was executed, tested, and validated locally by me. Nothing was done with AI that I didn't understand.</p>"},{"location":"exercises/mlp/notebook/","title":"3. MLP","text":"In\u00a0[256]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom typing import List, Tuple, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Configure matplotlib\nplt.style.use('seaborn-v0_8-darkgrid')\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, confusion_matrix import seaborn as sns from typing import List, Tuple, Optional import warnings warnings.filterwarnings('ignore')  # Set random seed for reproducibility np.random.seed(42)  # Configure matplotlib plt.style.use('seaborn-v0_8-darkgrid') In\u00a0[257]: Copied! <pre># Given values from the exercise\nx = np.array([0.5, -0.2])\ny = 1.0\nW1 = np.array([[0.3, -0.1], [0.2, 0.4]])\nb1 = np.array([0.1, -0.2])\nW2 = np.array([0.5, -0.3])\nb2 = 0.2\neta = 0.3  # Learning rate\n</pre> # Given values from the exercise x = np.array([0.5, -0.2]) y = 1.0 W1 = np.array([[0.3, -0.1], [0.2, 0.4]]) b1 = np.array([0.1, -0.2]) W2 = np.array([0.5, -0.3]) b2 = 0.2 eta = 0.3  # Learning rate In\u00a0[258]: Copied! <pre># Define activation functions from scratch\ndef tanh_manual(x):\n    \"\"\"Hyperbolic tangent activation function\"\"\"\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    \"\"\"Derivative of tanh: 1 - tanh^2(x)\"\"\"\n    return 1 - np.tanh(x)**2\n\n# Hidden layer pre-activations\nz1 = np.dot(W1, x) + b1\nprint(f\"\\nHidden layer pre-activations:\")\nprint(f\"z^(1) = {z1}\")\n\n# Hidden layer activations\nh1 = tanh_manual(z1)\nprint(f\"\\nHidden layer activations:\")\nprint(f\"h^(1) = {h1}\")\n\n# Output pre-activation\nu2 = np.dot(W2, h1) + b2\nprint(f\"\\nOutput pre-activation:\")\nprint(f\"u^(2) = {u2}\")\n\n# Final output\ny_hat = tanh_manual(u2)\nprint(f\"\\nFinal output:\")\nprint(f\"\u0177 = {y_hat}\")\n</pre> # Define activation functions from scratch def tanh_manual(x):     \"\"\"Hyperbolic tangent activation function\"\"\"     return np.tanh(x)  def tanh_derivative(x):     \"\"\"Derivative of tanh: 1 - tanh^2(x)\"\"\"     return 1 - np.tanh(x)**2  # Hidden layer pre-activations z1 = np.dot(W1, x) + b1 print(f\"\\nHidden layer pre-activations:\") print(f\"z^(1) = {z1}\")  # Hidden layer activations h1 = tanh_manual(z1) print(f\"\\nHidden layer activations:\") print(f\"h^(1) = {h1}\")  # Output pre-activation u2 = np.dot(W2, h1) + b2 print(f\"\\nOutput pre-activation:\") print(f\"u^(2) = {u2}\")  # Final output y_hat = tanh_manual(u2) print(f\"\\nFinal output:\") print(f\"\u0177 = {y_hat}\") <pre>\nHidden layer pre-activations:\nz^(1) = [ 0.27 -0.18]\n\nHidden layer activations:\nh^(1) = [ 0.26362484 -0.17808087]\n\nOutput pre-activation:\nu^(2) = 0.38523667817130075\n\nFinal output:\n\u0177 = 0.36724656264510797\n</pre> In\u00a0[259]: Copied! <pre># Calculate MSE loss\nN = 1  # Single sample\nloss = (1/N) * (y - y_hat)**2\nprint(f\"\\nMean Squared Error (MSE) Loss:\")\nprint(f\"L = {loss}\")\n</pre> # Calculate MSE loss N = 1  # Single sample loss = (1/N) * (y - y_hat)**2 print(f\"\\nMean Squared Error (MSE) Loss:\") print(f\"L = {loss}\") <pre>\nMean Squared Error (MSE) Loss:\nL = 0.4003769124844312\n</pre> In\u00a0[260]: Copied! <pre># Gradient of loss w.r.t output\ndL_dy_hat = -2 * (y - y_hat) / N\nprint(f\"\\nGradient of loss w.r.t. output:\")\nprint(f\"\u2202L/\u2202\u0177 = {dL_dy_hat}\")\n\n# Gradient w.r.t output pre-activation\ndL_du2 = dL_dy_hat * tanh_derivative(u2)\nprint(f\"\\nGradient w.r.t. output pre-activation:\")\nprint(f\"\u2202L/\u2202u^(2) = {dL_du2}\")\n\n# Gradients for output layer\ndL_dW2 = dL_du2 * h1\ndL_db2 = dL_du2\nprint(f\"\\nGradients for output layer:\")\nprint(f\"\u2202L/\u2202W^(2) = {dL_dW2}\")\nprint(f\"\u2202L/\u2202b^(2) = {dL_db2}\")\n\n# Propagate to hidden layer\ndL_dh1 = dL_du2 * W2\nprint(f\"\\nGradient w.r.t. hidden layer activations:\")\nprint(f\"\u2202L/\u2202h^(1) = {dL_dh1}\")\n\n# Gradient w.r.t hidden pre-activations\ndL_dz1 = dL_dh1 * tanh_derivative(z1)\nprint(f\"\\nGradient w.r.t. hidden pre-activations:\")\nprint(f\"\u2202L/\u2202z^(1) = {dL_dz1}\")\n\n# Gradients for hidden layer\ndL_dW1 = np.outer(dL_dz1, x)\ndL_db1 = dL_dz1\nprint(f\"\\nGradients for hidden layer:\")\nprint(f\"\u2202L/\u2202W^(1) =\\n{dL_dW1}\")\nprint(f\"\u2202L/\u2202b^(1) = {dL_db1}\")\n</pre> # Gradient of loss w.r.t output dL_dy_hat = -2 * (y - y_hat) / N print(f\"\\nGradient of loss w.r.t. output:\") print(f\"\u2202L/\u2202\u0177 = {dL_dy_hat}\")  # Gradient w.r.t output pre-activation dL_du2 = dL_dy_hat * tanh_derivative(u2) print(f\"\\nGradient w.r.t. output pre-activation:\") print(f\"\u2202L/\u2202u^(2) = {dL_du2}\")  # Gradients for output layer dL_dW2 = dL_du2 * h1 dL_db2 = dL_du2 print(f\"\\nGradients for output layer:\") print(f\"\u2202L/\u2202W^(2) = {dL_dW2}\") print(f\"\u2202L/\u2202b^(2) = {dL_db2}\")  # Propagate to hidden layer dL_dh1 = dL_du2 * W2 print(f\"\\nGradient w.r.t. hidden layer activations:\") print(f\"\u2202L/\u2202h^(1) = {dL_dh1}\")  # Gradient w.r.t hidden pre-activations dL_dz1 = dL_dh1 * tanh_derivative(z1) print(f\"\\nGradient w.r.t. hidden pre-activations:\") print(f\"\u2202L/\u2202z^(1) = {dL_dz1}\")  # Gradients for hidden layer dL_dW1 = np.outer(dL_dz1, x) dL_db1 = dL_dz1 print(f\"\\nGradients for hidden layer:\") print(f\"\u2202L/\u2202W^(1) =\\n{dL_dW1}\") print(f\"\u2202L/\u2202b^(1) = {dL_db1}\") <pre>\nGradient of loss w.r.t. output:\n\u2202L/\u2202\u0177 = -1.265506874709784\n\nGradient w.r.t. output pre-activation:\n\u2202L/\u2202u^(2) = -1.0948279147135995\n\nGradients for output layer:\n\u2202L/\u2202W^(2) = [-0.28862383  0.19496791]\n\u2202L/\u2202b^(2) = -1.0948279147135995\n\nGradient w.r.t. hidden layer activations:\n\u2202L/\u2202h^(1) = [-0.54741396  0.32844837]\n\nGradient w.r.t. hidden pre-activations:\n\u2202L/\u2202z^(1) = [-0.50936975  0.31803236]\n\nGradients for hidden layer:\n\u2202L/\u2202W^(1) =\n[[-0.25468488  0.10187395]\n [ 0.15901618 -0.06360647]]\n\u2202L/\u2202b^(1) = [-0.50936975  0.31803236]\n</pre> In\u00a0[261]: Copied! <pre># Set learning rate\neta = 0.1\n\n# Update weights and biases\nW2_new = W2 - eta * dL_dW2\nb2_new = b2 - eta * dL_db2\nW1_new = W1 - eta * dL_dW1\nb1_new = b1 - eta * dL_db1\n\nprint(f\"\\nUsing learning rate \u03b7 = {eta}\")\nprint(f\"\\nOutput layer updates:\")\nprint(f\"W^(2)_new = {W2_new}\")\nprint(f\"b^(2)_new = {b2_new:.6f}\")\nprint(f\"\\nHidden layer updates:\")\nprint(f\"W^(1)_new =\\n{W1_new}\")\nprint(f\"b^(1)_new = {b1_new}\")\n</pre> # Set learning rate eta = 0.1  # Update weights and biases W2_new = W2 - eta * dL_dW2 b2_new = b2 - eta * dL_db2 W1_new = W1 - eta * dL_dW1 b1_new = b1 - eta * dL_db1  print(f\"\\nUsing learning rate \u03b7 = {eta}\") print(f\"\\nOutput layer updates:\") print(f\"W^(2)_new = {W2_new}\") print(f\"b^(2)_new = {b2_new:.6f}\") print(f\"\\nHidden layer updates:\") print(f\"W^(1)_new =\\n{W1_new}\") print(f\"b^(1)_new = {b1_new}\") <pre>\nUsing learning rate \u03b7 = 0.1\n\nOutput layer updates:\nW^(2)_new = [ 0.52886238 -0.31949679]\nb^(2)_new = 0.309483\n\nHidden layer updates:\nW^(1)_new =\n[[ 0.32546849 -0.1101874 ]\n [ 0.18409838  0.40636065]]\nb^(1)_new = [ 0.15093698 -0.23180324]\n</pre> <p>The manual calculations show how backpropagation works step by step. The gradients flow backward from the output layer through the hidden layer, and each parameter is updated in the opposite direction of its gradient to minimize the loss.</p> In\u00a0[262]: Copied! <pre># Generate synthetic dataset with asymmetric clusters\ndef generate_binary_data_ex2():\n    \"\"\"Generate binary classification data with 1 cluster for class 0 and 2 clusters for class 1\"\"\"\n    # Generate class 0 with 1 cluster\n    X0, y0 = make_classification(\n        n_samples=1000,\n        n_features=2,\n        n_informative=2,\n        n_redundant=0,\n        n_clusters_per_class=1,\n        n_classes=2,\n        random_state=42,\n        class_sep=2,\n        flip_y=0.1\n    )\n    X0 = X0[y0 == 0][:500]  # Keep only class 0\n    y0 = np.zeros(len(X0))\n    \n    # Generate class 1 with 2 clusters\n    np.random.seed(43)\n    X1_cluster1 = np.random.randn(250, 2) * 0.8 + [3, 2]\n    X1_cluster2 = np.random.randn(250, 2) * 0.8 + [-2, 3]\n    X1 = np.vstack([X1_cluster1, X1_cluster2])\n    y1 = np.ones(len(X1))\n    \n    # Combine and shuffle\n    X = np.vstack([X0, X1])\n    y = np.hstack([y0, y1])\n    shuffle_idx = np.random.permutation(len(X))\n    X = X[shuffle_idx]\n    y = y[shuffle_idx]\n    \n    return X, y\n\n# Generate the dataset\nX_ex2, y_ex2 = generate_binary_data_ex2()\nX_train_ex2, X_test_ex2, y_train_ex2, y_test_ex2 = train_test_split(\n    X_ex2, y_ex2, test_size=0.2, random_state=42\n)\n\nprint(f\"\\nDataset generated:\")\nprint(f\"Training samples: {len(X_train_ex2)}\")\nprint(f\"Testing samples: {len(X_test_ex2)}\")\nprint(f\"Features: {X_ex2.shape[1]}\")\nprint(f\"Classes: {len(np.unique(y_ex2))}\")\n</pre> # Generate synthetic dataset with asymmetric clusters def generate_binary_data_ex2():     \"\"\"Generate binary classification data with 1 cluster for class 0 and 2 clusters for class 1\"\"\"     # Generate class 0 with 1 cluster     X0, y0 = make_classification(         n_samples=1000,         n_features=2,         n_informative=2,         n_redundant=0,         n_clusters_per_class=1,         n_classes=2,         random_state=42,         class_sep=2,         flip_y=0.1     )     X0 = X0[y0 == 0][:500]  # Keep only class 0     y0 = np.zeros(len(X0))          # Generate class 1 with 2 clusters     np.random.seed(43)     X1_cluster1 = np.random.randn(250, 2) * 0.8 + [3, 2]     X1_cluster2 = np.random.randn(250, 2) * 0.8 + [-2, 3]     X1 = np.vstack([X1_cluster1, X1_cluster2])     y1 = np.ones(len(X1))          # Combine and shuffle     X = np.vstack([X0, X1])     y = np.hstack([y0, y1])     shuffle_idx = np.random.permutation(len(X))     X = X[shuffle_idx]     y = y[shuffle_idx]          return X, y  # Generate the dataset X_ex2, y_ex2 = generate_binary_data_ex2() X_train_ex2, X_test_ex2, y_train_ex2, y_test_ex2 = train_test_split(     X_ex2, y_ex2, test_size=0.2, random_state=42 )  print(f\"\\nDataset generated:\") print(f\"Training samples: {len(X_train_ex2)}\") print(f\"Testing samples: {len(X_test_ex2)}\") print(f\"Features: {X_ex2.shape[1]}\") print(f\"Classes: {len(np.unique(y_ex2))}\") <pre>\nDataset generated:\nTraining samples: 800\nTesting samples: 200\nFeatures: 2\nClasses: 2\n</pre> In\u00a0[263]: Copied! <pre># Visualize the generated data\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_train_ex2[y_train_ex2 == 0, 0], X_train_ex2[y_train_ex2 == 0, 1], \n            c='blue', alpha=0.6, label='Class 0 (1 cluster)')\nplt.scatter(X_train_ex2[y_train_ex2 == 1, 0], X_train_ex2[y_train_ex2 == 1, 1], \n            c='red', alpha=0.6, label='Class 1 (2 clusters)')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Training Data Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test_ex2[y_test_ex2 == 0, 0], X_test_ex2[y_test_ex2 == 0, 1], \n            c='blue', alpha=0.6, label='Class 0')\nplt.scatter(X_test_ex2[y_test_ex2 == 1, 0], X_test_ex2[y_test_ex2 == 1, 1], \n            c='red', alpha=0.6, label='Class 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Testing Data Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</pre> # Visualize the generated data plt.figure(figsize=(12, 5))  plt.subplot(1, 2, 1) plt.scatter(X_train_ex2[y_train_ex2 == 0, 0], X_train_ex2[y_train_ex2 == 0, 1],              c='blue', alpha=0.6, label='Class 0 (1 cluster)') plt.scatter(X_train_ex2[y_train_ex2 == 1, 0], X_train_ex2[y_train_ex2 == 1, 1],              c='red', alpha=0.6, label='Class 1 (2 clusters)') plt.xlabel('Feature 1') plt.ylabel('Feature 2') plt.title('Training Data Distribution') plt.legend() plt.grid(True, alpha=0.3)  plt.subplot(1, 2, 2) plt.scatter(X_test_ex2[y_test_ex2 == 0, 0], X_test_ex2[y_test_ex2 == 0, 1],              c='blue', alpha=0.6, label='Class 0') plt.scatter(X_test_ex2[y_test_ex2 == 1, 0], X_test_ex2[y_test_ex2 == 1, 1],              c='red', alpha=0.6, label='Class 1') plt.xlabel('Feature 1') plt.ylabel('Feature 2') plt.title('Testing Data Distribution') plt.legend() plt.grid(True, alpha=0.3)  plt.tight_layout() plt.show() In\u00a0[264]: Copied! <pre>class MLPBinaryClassifier:\n    \"\"\"\n    Multi-Layer Perceptron for Binary Classification\n    Implemented from scratch without using ML libraries\n    \"\"\"\n    \n    def __init__(self, input_size: int, hidden_sizes: List[int], \n                 learning_rate: float = 0.01, activation: str = 'tanh'):\n        \"\"\"Initialize the MLP\"\"\"\n        self.input_size = input_size\n        self.hidden_sizes = hidden_sizes\n        self.learning_rate = learning_rate\n        self.activation = activation\n        \n        # Initialize weights and biases\n        self.weights = []\n        self.biases = []\n        \n        # Layer sizes including input and output\n        layer_sizes = [input_size] + hidden_sizes + [1]  # Binary output\n        \n        # Initialize parameters with Xavier/He initialization\n        for i in range(len(layer_sizes) - 1):\n            if activation == 'relu':\n                # He initialization for ReLU\n                w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2/layer_sizes[i])\n            else:\n                # Xavier initialization for tanh/sigmoid\n                w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(1/layer_sizes[i])\n            b = np.zeros(layer_sizes[i+1])\n            self.weights.append(w)\n            self.biases.append(b)\n        \n        # Storage for layer outputs (for backprop)\n        self.activations = []\n        self.z_values = []\n        \n    def _activate(self, z):\n        \"\"\"Apply activation function\"\"\"\n        if self.activation == 'tanh':\n            return np.tanh(z)\n        elif self.activation == 'relu':\n            return np.maximum(0, z)\n        elif self.activation == 'sigmoid':\n            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n        \n    def _activate_derivative(self, z):\n        \"\"\"Compute derivative of activation function\"\"\"\n        if self.activation == 'tanh':\n            return 1 - np.tanh(z)**2\n        elif self.activation == 'relu':\n            return (z &gt; 0).astype(float)\n        elif self.activation == 'sigmoid':\n            sig = self._activate(z)\n            return sig * (1 - sig)\n    \n    def _sigmoid(self, z):\n        \"\"\"Sigmoid for output layer\"\"\"\n        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n    \n    def _sigmoid_derivative(self, z):\n        \"\"\"Derivative of sigmoid\"\"\"\n        sig = self._sigmoid(z)\n        return sig * (1 - sig)\n    \n    def forward(self, X):\n        \"\"\"Forward pass through the network\"\"\"\n        self.activations = [X]\n        self.z_values = []\n        \n        current_input = X\n        \n        # Hidden layers\n        for i in range(len(self.weights) - 1):\n            z = np.dot(current_input, self.weights[i].T) + self.biases[i]\n            self.z_values.append(z)\n            a = self._activate(z)\n            self.activations.append(a)\n            current_input = a\n        \n        # Output layer (sigmoid for binary classification)\n        z = np.dot(current_input, self.weights[-1].T) + self.biases[-1]\n        self.z_values.append(z)\n        output = self._sigmoid(z)\n        self.activations.append(output)\n        \n        return output.reshape(-1)\n    \n    def binary_cross_entropy(self, y_true, y_pred):\n        \"\"\"Compute binary cross-entropy loss\"\"\"\n        eps = 1e-7  # Small value to avoid log(0)\n        y_pred = np.clip(y_pred, eps, 1 - eps)\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    \n    def backward(self, X, y):\n        \"\"\"Backward pass (backpropagation)\"\"\"\n        m = len(X)\n        \n        # Gradients storage\n        dW = []\n        db = []\n        \n        # Output layer gradient (keep delta as shape (m, 1))\n        y_pred = self.activations[-1]      # already shape (m, 1)\n        delta = (y_pred - y.reshape(-1, 1))   # (m, 1)\n        \n        # Backpropagate through layers\n        for i in range(len(self.weights) - 1, -1, -1):\n            # Weight and bias gradients\n            dW_i = np.dot(delta.T, self.activations[i]) / m\n            db_i = np.mean(delta, axis=0)\n\n            dW.insert(0, dW_i)\n            db.insert(0, db_i)\n\n            if i &gt; 0:\n                # Propagate delta to previous layer\n                delta = np.dot(delta, self.weights[i]) * self._activate_derivative(self.z_values[i-1])\n\n        \n        # Update weights and biases\n        for i in range(len(self.weights)):\n            self.weights[i] -= self.learning_rate * dW[i]\n            self.biases[i] -= self.learning_rate * db[i]\n    \n    def train(self, X_train, y_train, epochs=100, verbose=True):\n        \"\"\"Train the MLP\"\"\"\n        losses = []\n        \n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self.forward(X_train)\n            \n            # Compute loss\n            loss = self.binary_cross_entropy(y_train, y_pred)\n            losses.append(loss)\n            \n            # Backward pass\n            self.backward(X_train, y_train)\n            \n            # Print progress\n            if verbose and (epoch + 1) % 50 == 0:\n                accuracy = np.mean((y_pred &gt; 0.5) == y_train)\n                print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")\n        \n        return losses\n    \n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        probs = self.forward(X)\n        return (probs &gt; 0.5).astype(int)\n    \n    def predict_proba(self, X):\n        \"\"\"Get prediction probabilities\"\"\"\n        return self.forward(X)\n</pre> class MLPBinaryClassifier:     \"\"\"     Multi-Layer Perceptron for Binary Classification     Implemented from scratch without using ML libraries     \"\"\"          def __init__(self, input_size: int, hidden_sizes: List[int],                   learning_rate: float = 0.01, activation: str = 'tanh'):         \"\"\"Initialize the MLP\"\"\"         self.input_size = input_size         self.hidden_sizes = hidden_sizes         self.learning_rate = learning_rate         self.activation = activation                  # Initialize weights and biases         self.weights = []         self.biases = []                  # Layer sizes including input and output         layer_sizes = [input_size] + hidden_sizes + [1]  # Binary output                  # Initialize parameters with Xavier/He initialization         for i in range(len(layer_sizes) - 1):             if activation == 'relu':                 # He initialization for ReLU                 w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2/layer_sizes[i])             else:                 # Xavier initialization for tanh/sigmoid                 w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(1/layer_sizes[i])             b = np.zeros(layer_sizes[i+1])             self.weights.append(w)             self.biases.append(b)                  # Storage for layer outputs (for backprop)         self.activations = []         self.z_values = []              def _activate(self, z):         \"\"\"Apply activation function\"\"\"         if self.activation == 'tanh':             return np.tanh(z)         elif self.activation == 'relu':             return np.maximum(0, z)         elif self.activation == 'sigmoid':             return 1 / (1 + np.exp(-np.clip(z, -500, 500)))              def _activate_derivative(self, z):         \"\"\"Compute derivative of activation function\"\"\"         if self.activation == 'tanh':             return 1 - np.tanh(z)**2         elif self.activation == 'relu':             return (z &gt; 0).astype(float)         elif self.activation == 'sigmoid':             sig = self._activate(z)             return sig * (1 - sig)          def _sigmoid(self, z):         \"\"\"Sigmoid for output layer\"\"\"         return 1 / (1 + np.exp(-np.clip(z, -500, 500)))          def _sigmoid_derivative(self, z):         \"\"\"Derivative of sigmoid\"\"\"         sig = self._sigmoid(z)         return sig * (1 - sig)          def forward(self, X):         \"\"\"Forward pass through the network\"\"\"         self.activations = [X]         self.z_values = []                  current_input = X                  # Hidden layers         for i in range(len(self.weights) - 1):             z = np.dot(current_input, self.weights[i].T) + self.biases[i]             self.z_values.append(z)             a = self._activate(z)             self.activations.append(a)             current_input = a                  # Output layer (sigmoid for binary classification)         z = np.dot(current_input, self.weights[-1].T) + self.biases[-1]         self.z_values.append(z)         output = self._sigmoid(z)         self.activations.append(output)                  return output.reshape(-1)          def binary_cross_entropy(self, y_true, y_pred):         \"\"\"Compute binary cross-entropy loss\"\"\"         eps = 1e-7  # Small value to avoid log(0)         y_pred = np.clip(y_pred, eps, 1 - eps)         return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))          def backward(self, X, y):         \"\"\"Backward pass (backpropagation)\"\"\"         m = len(X)                  # Gradients storage         dW = []         db = []                  # Output layer gradient (keep delta as shape (m, 1))         y_pred = self.activations[-1]      # already shape (m, 1)         delta = (y_pred - y.reshape(-1, 1))   # (m, 1)                  # Backpropagate through layers         for i in range(len(self.weights) - 1, -1, -1):             # Weight and bias gradients             dW_i = np.dot(delta.T, self.activations[i]) / m             db_i = np.mean(delta, axis=0)              dW.insert(0, dW_i)             db.insert(0, db_i)              if i &gt; 0:                 # Propagate delta to previous layer                 delta = np.dot(delta, self.weights[i]) * self._activate_derivative(self.z_values[i-1])                   # Update weights and biases         for i in range(len(self.weights)):             self.weights[i] -= self.learning_rate * dW[i]             self.biases[i] -= self.learning_rate * db[i]          def train(self, X_train, y_train, epochs=100, verbose=True):         \"\"\"Train the MLP\"\"\"         losses = []                  for epoch in range(epochs):             # Forward pass             y_pred = self.forward(X_train)                          # Compute loss             loss = self.binary_cross_entropy(y_train, y_pred)             losses.append(loss)                          # Backward pass             self.backward(X_train, y_train)                          # Print progress             if verbose and (epoch + 1) % 50 == 0:                 accuracy = np.mean((y_pred &gt; 0.5) == y_train)                 print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")                  return losses          def predict(self, X):         \"\"\"Make predictions\"\"\"         probs = self.forward(X)         return (probs &gt; 0.5).astype(int)          def predict_proba(self, X):         \"\"\"Get prediction probabilities\"\"\"         return self.forward(X) In\u00a0[265]: Copied! <pre># Create and train the MLP for Exercise 2\nprint(\"\\nTraining MLP for Binary Classification\")\nprint(\"-\" * 40)\nmlp_ex2 = MLPBinaryClassifier(\n    input_size=2,\n    hidden_sizes=[8, 4],  # 2 hidden layers\n    learning_rate=0.1,\n    activation='tanh'\n)\n\n# Train the model\nlosses_ex2 = mlp_ex2.train(X_train_ex2, y_train_ex2, epochs=300, verbose=True)\n</pre> # Create and train the MLP for Exercise 2 print(\"\\nTraining MLP for Binary Classification\") print(\"-\" * 40) mlp_ex2 = MLPBinaryClassifier(     input_size=2,     hidden_sizes=[8, 4],  # 2 hidden layers     learning_rate=0.1,     activation='tanh' )  # Train the model losses_ex2 = mlp_ex2.train(X_train_ex2, y_train_ex2, epochs=300, verbose=True) <pre>\nTraining MLP for Binary Classification\n----------------------------------------\nEpoch 50/300 - Loss: 0.1524 - Accuracy: 0.9663\nEpoch 100/300 - Loss: 0.1251 - Accuracy: 0.9675\nEpoch 150/300 - Loss: 0.1163 - Accuracy: 0.9675\nEpoch 200/300 - Loss: 0.1112 - Accuracy: 0.9675\nEpoch 250/300 - Loss: 0.1076 - Accuracy: 0.9688\nEpoch 300/300 - Loss: 0.1048 - Accuracy: 0.9675\n</pre> In\u00a0[266]: Copied! <pre># Evaluate the model\nprint(\"\\nModel Evaluation\")\nprint(\"-\" * 40)\n\n# Training accuracy\ntrain_pred_ex2 = mlp_ex2.predict(X_train_ex2)\ntrain_acc_ex2 = accuracy_score(y_train_ex2, train_pred_ex2)\nprint(f\"Training Accuracy: {train_acc_ex2:.4f}\")\n\n# Testing accuracy\ntest_pred_ex2 = mlp_ex2.predict(X_test_ex2)\ntest_acc_ex2 = accuracy_score(y_test_ex2, test_pred_ex2)\nprint(f\"Testing Accuracy: {test_acc_ex2:.4f}\")\n\n# Confusion matrix\ncm_ex2 = confusion_matrix(y_test_ex2, test_pred_ex2)\nprint(f\"\\nConfusion Matrix:\")\nprint(cm_ex2)\n</pre> # Evaluate the model print(\"\\nModel Evaluation\") print(\"-\" * 40)  # Training accuracy train_pred_ex2 = mlp_ex2.predict(X_train_ex2) train_acc_ex2 = accuracy_score(y_train_ex2, train_pred_ex2) print(f\"Training Accuracy: {train_acc_ex2:.4f}\")  # Testing accuracy test_pred_ex2 = mlp_ex2.predict(X_test_ex2) test_acc_ex2 = accuracy_score(y_test_ex2, test_pred_ex2) print(f\"Testing Accuracy: {test_acc_ex2:.4f}\")  # Confusion matrix cm_ex2 = confusion_matrix(y_test_ex2, test_pred_ex2) print(f\"\\nConfusion Matrix:\") print(cm_ex2) <pre>\nModel Evaluation\n----------------------------------------\nTraining Accuracy: 0.9675\nTesting Accuracy: 0.9900\n\nConfusion Matrix:\n[[ 94   1]\n [  1 104]]\n</pre> In\u00a0[267]: Copied! <pre># Visualization of results\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Plot 1: Training Loss\naxes[0, 0].plot(losses_ex2)\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Binary Cross-Entropy Loss')\naxes[0, 0].set_title('Training Loss Over Time')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Confusion Matrix\nsns.heatmap(cm_ex2, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\naxes[0, 1].set_xlabel('Predicted')\naxes[0, 1].set_ylabel('Actual')\naxes[0, 1].set_title('Confusion Matrix - Test Set')\n\n# Plot 3: Decision Boundary\nh = 0.02  # step size in mesh\nx_min, x_max = X_ex2[:, 0].min() - 1, X_ex2[:, 0].max() + 1\ny_min, y_max = X_ex2[:, 1].min() - 1, X_ex2[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = mlp_ex2.predict_proba(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\naxes[1, 0].contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.8)\naxes[1, 0].scatter(X_train_ex2[y_train_ex2 == 0, 0], \n                   X_train_ex2[y_train_ex2 == 0, 1], \n                   c='blue', edgecolor='black', s=50, label='Class 0')\naxes[1, 0].scatter(X_train_ex2[y_train_ex2 == 1, 0], \n                   X_train_ex2[y_train_ex2 == 1, 1], \n                   c='red', edgecolor='black', s=50, label='Class 1')\naxes[1, 0].set_xlabel('Feature 1')\naxes[1, 0].set_ylabel('Feature 2')\naxes[1, 0].set_title('Decision Boundary - Training Set')\naxes[1, 0].legend()\n\n# Plot 4: Test set predictions\naxes[1, 1].contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.8)\ncolors = ['blue' if p == 0 else 'red' for p in test_pred_ex2]\naxes[1, 1].scatter(X_test_ex2[:, 0], X_test_ex2[:, 1], \n                   c=colors, edgecolor='black', s=50, alpha=0.7)\naxes[1, 1].set_xlabel('Feature 1')\naxes[1, 1].set_ylabel('Feature 2')\naxes[1, 1].set_title(f'Test Set Predictions (Accuracy: {test_acc_ex2:.2%})')\n\nplt.tight_layout()\nplt.show()\n</pre> # Visualization of results fig, axes = plt.subplots(2, 2, figsize=(14, 12))  # Plot 1: Training Loss axes[0, 0].plot(losses_ex2) axes[0, 0].set_xlabel('Epoch') axes[0, 0].set_ylabel('Binary Cross-Entropy Loss') axes[0, 0].set_title('Training Loss Over Time') axes[0, 0].grid(True, alpha=0.3)  # Plot 2: Confusion Matrix sns.heatmap(cm_ex2, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1]) axes[0, 1].set_xlabel('Predicted') axes[0, 1].set_ylabel('Actual') axes[0, 1].set_title('Confusion Matrix - Test Set')  # Plot 3: Decision Boundary h = 0.02  # step size in mesh x_min, x_max = X_ex2[:, 0].min() - 1, X_ex2[:, 0].max() + 1 y_min, y_max = X_ex2[:, 1].min() - 1, X_ex2[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h),                      np.arange(y_min, y_max, h))  Z = mlp_ex2.predict_proba(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape)  axes[1, 0].contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.8) axes[1, 0].scatter(X_train_ex2[y_train_ex2 == 0, 0],                     X_train_ex2[y_train_ex2 == 0, 1],                     c='blue', edgecolor='black', s=50, label='Class 0') axes[1, 0].scatter(X_train_ex2[y_train_ex2 == 1, 0],                     X_train_ex2[y_train_ex2 == 1, 1],                     c='red', edgecolor='black', s=50, label='Class 1') axes[1, 0].set_xlabel('Feature 1') axes[1, 0].set_ylabel('Feature 2') axes[1, 0].set_title('Decision Boundary - Training Set') axes[1, 0].legend()  # Plot 4: Test set predictions axes[1, 1].contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.8) colors = ['blue' if p == 0 else 'red' for p in test_pred_ex2] axes[1, 1].scatter(X_test_ex2[:, 0], X_test_ex2[:, 1],                     c=colors, edgecolor='black', s=50, alpha=0.7) axes[1, 1].set_xlabel('Feature 1') axes[1, 1].set_ylabel('Feature 2') axes[1, 1].set_title(f'Test Set Predictions (Accuracy: {test_acc_ex2:.2%})')  plt.tight_layout() plt.show() <p>The MLP successfully learns a non-linear decision boundary to separate the two classes. The model achieves good accuracy despite Class 1 having two distinct clusters, demonstrating the power of hidden layers with non-linear activation functions.</p> In\u00a0[268]: Copied! <pre># Generate multi-class dataset with different clusters per class\ndef generate_multiclass_data_ex3():\n    \"\"\"Generate 3-class dataset with 2, 3, and 4 clusters per class\"\"\"\n    np.random.seed(42)\n    \n    # Class 0 - 2 clusters\n    cluster0_1 = np.random.randn(250, 4) * 0.5 + [2, 2, 1, 1]\n    cluster0_2 = np.random.randn(250, 4) * 0.5 + [-2, -2, -1, -1]\n    X0 = np.vstack([cluster0_1, cluster0_2])\n    y0 = np.zeros(500)\n    \n    # Class 1 - 3 clusters\n    cluster1_1 = np.random.randn(167, 4) * 0.5 + [4, 0, 2, -2]\n    cluster1_2 = np.random.randn(167, 4) * 0.5 + [-4, 0, -2, 2]\n    cluster1_3 = np.random.randn(166, 4) * 0.5 + [0, 4, 0, 0]\n    X1 = np.vstack([cluster1_1, cluster1_2, cluster1_3])\n    y1 = np.ones(500)\n    \n    # Class 2 - 4 clusters\n    cluster2_1 = np.random.randn(125, 4) * 0.5 + [3, 3, 3, 3]\n    cluster2_2 = np.random.randn(125, 4) * 0.5 + [-3, 3, -3, 3]\n    cluster2_3 = np.random.randn(125, 4) * 0.5 + [3, -3, 3, -3]\n    cluster2_4 = np.random.randn(125, 4) * 0.5 + [-3, -3, -3, -3]\n    X2 = np.vstack([cluster2_1, cluster2_2, cluster2_3, cluster2_4])\n    y2 = np.ones(500) * 2\n    \n    # Combine and shuffle\n    X = np.vstack([X0, X1, X2])\n    y = np.hstack([y0, y1, y2])\n    shuffle_idx = np.random.permutation(len(X))\n    X = X[shuffle_idx]\n    y = y[shuffle_idx]\n    \n    return X, y.astype(int)\n\n# Generate the dataset\nX_ex3, y_ex3 = generate_multiclass_data_ex3()\nX_train_ex3, X_test_ex3, y_train_ex3, y_test_ex3 = train_test_split(\n    X_ex3, y_ex3, test_size=0.2, random_state=42\n)\n\nprint(f\"\\nDataset generated:\")\nprint(f\"Training samples: {len(X_train_ex3)}\")\nprint(f\"Testing samples: {len(X_test_ex3)}\")\nprint(f\"Features: {X_ex3.shape[1]}\")\nprint(f\"Classes: {len(np.unique(y_ex3))} - {np.unique(y_ex3)}\")\nprint(f\"Class distribution: {np.bincount(y_ex3)}\")\n</pre> # Generate multi-class dataset with different clusters per class def generate_multiclass_data_ex3():     \"\"\"Generate 3-class dataset with 2, 3, and 4 clusters per class\"\"\"     np.random.seed(42)          # Class 0 - 2 clusters     cluster0_1 = np.random.randn(250, 4) * 0.5 + [2, 2, 1, 1]     cluster0_2 = np.random.randn(250, 4) * 0.5 + [-2, -2, -1, -1]     X0 = np.vstack([cluster0_1, cluster0_2])     y0 = np.zeros(500)          # Class 1 - 3 clusters     cluster1_1 = np.random.randn(167, 4) * 0.5 + [4, 0, 2, -2]     cluster1_2 = np.random.randn(167, 4) * 0.5 + [-4, 0, -2, 2]     cluster1_3 = np.random.randn(166, 4) * 0.5 + [0, 4, 0, 0]     X1 = np.vstack([cluster1_1, cluster1_2, cluster1_3])     y1 = np.ones(500)          # Class 2 - 4 clusters     cluster2_1 = np.random.randn(125, 4) * 0.5 + [3, 3, 3, 3]     cluster2_2 = np.random.randn(125, 4) * 0.5 + [-3, 3, -3, 3]     cluster2_3 = np.random.randn(125, 4) * 0.5 + [3, -3, 3, -3]     cluster2_4 = np.random.randn(125, 4) * 0.5 + [-3, -3, -3, -3]     X2 = np.vstack([cluster2_1, cluster2_2, cluster2_3, cluster2_4])     y2 = np.ones(500) * 2          # Combine and shuffle     X = np.vstack([X0, X1, X2])     y = np.hstack([y0, y1, y2])     shuffle_idx = np.random.permutation(len(X))     X = X[shuffle_idx]     y = y[shuffle_idx]          return X, y.astype(int)  # Generate the dataset X_ex3, y_ex3 = generate_multiclass_data_ex3() X_train_ex3, X_test_ex3, y_train_ex3, y_test_ex3 = train_test_split(     X_ex3, y_ex3, test_size=0.2, random_state=42 )  print(f\"\\nDataset generated:\") print(f\"Training samples: {len(X_train_ex3)}\") print(f\"Testing samples: {len(X_test_ex3)}\") print(f\"Features: {X_ex3.shape[1]}\") print(f\"Classes: {len(np.unique(y_ex3))} - {np.unique(y_ex3)}\") print(f\"Class distribution: {np.bincount(y_ex3)}\") <pre>\nDataset generated:\nTraining samples: 1200\nTesting samples: 300\nFeatures: 4\nClasses: 3 - [0 1 2]\nClass distribution: [500 500 500]\n</pre> In\u00a0[269]: Copied! <pre># Reusable MLP class for multi-class classification\nclass MLPMultiClassifier:\n    \"\"\"\n    Multi-Layer Perceptron for Multi-Class Classification\n    This is a reusable version that can handle both binary and multi-class problems\n    \"\"\"\n    \n    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int,\n                 learning_rate: float = 0.01, activation: str = 'tanh'):\n        \"\"\"Initialize the MLP\"\"\"\n        self.input_size = input_size\n        self.hidden_sizes = hidden_sizes\n        self.output_size = output_size\n        self.learning_rate = learning_rate\n        self.activation = activation\n        \n        # Initialize weights and biases\n        self.weights = []\n        self.biases = []\n        \n        # Layer sizes\n        layer_sizes = [input_size] + hidden_sizes + [output_size]\n        \n        # Initialize parameters\n        for i in range(len(layer_sizes) - 1):\n            if activation == 'relu':\n                w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2/layer_sizes[i])\n            else:\n                w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(1/layer_sizes[i])\n            b = np.zeros(layer_sizes[i+1])\n            self.weights.append(w)\n            self.biases.append(b)\n        \n        self.activations = []\n        self.z_values = []\n        \n    def _activate(self, z):\n        \"\"\"Apply activation function\"\"\"\n        if self.activation == 'tanh':\n            return np.tanh(z)\n        elif self.activation == 'relu':\n            return np.maximum(0, z)\n        elif self.activation == 'sigmoid':\n            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n        \n    def _activate_derivative(self, z):\n        \"\"\"Compute derivative of activation function\"\"\"\n        if self.activation == 'tanh':\n            return 1 - np.tanh(z)**2\n        elif self.activation == 'relu':\n            return (z &gt; 0).astype(float)\n        elif self.activation == 'sigmoid':\n            sig = self._activate(z)\n            return sig * (1 - sig)\n    \n    def _softmax(self, z):\n        \"\"\"Softmax for output layer (multi-class)\"\"\"\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n    \n    def forward(self, X):\n        \"\"\"Forward pass through the network\"\"\"\n        self.activations = [X]\n        self.z_values = []\n        \n        current_input = X\n        \n        # Hidden layers\n        for i in range(len(self.weights) - 1):\n            z = np.dot(current_input, self.weights[i].T) + self.biases[i]\n            self.z_values.append(z)\n            a = self._activate(z)\n            self.activations.append(a)\n            current_input = a\n        \n        # Output layer\n        z = np.dot(current_input, self.weights[-1].T) + self.biases[-1]\n        self.z_values.append(z)\n        \n        if self.output_size == 1:\n            # Binary classification\n            output = 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n        else:\n            # Multi-class classification\n            output = self._softmax(z)\n        \n        self.activations.append(output)\n        return output\n    \n    def categorical_cross_entropy(self, y_true_oh, y_pred):\n        \"\"\"Compute categorical cross-entropy loss\"\"\"\n        eps = 1e-7\n        y_pred = np.clip(y_pred, eps, 1 - eps)\n        return -np.mean(np.sum(y_true_oh * np.log(y_pred), axis=1))\n    \n    def binary_cross_entropy(self, y_true, y_pred):\n        \"\"\"Binary cross-entropy (for compatibility)\"\"\"\n        eps = 1e-7\n        y_pred = np.clip(y_pred, eps, 1 - eps)\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    \n    def backward(self, X, y_true_oh):\n        \"\"\"Backward pass (backpropagation) for multi-class\"\"\"\n        m = len(X)\n        \n        # Gradients storage\n        dW = []\n        db = []\n        \n        # Output layer gradient (softmax with cross-entropy)\n        y_pred = self.activations[-1]\n        delta = (y_pred - y_true_oh) / m\n        \n        # Backpropagate through layers\n        for i in range(len(self.weights) - 1, -1, -1):\n            # Gradient for weights and biases\n            dW_i = np.dot(delta.T, self.activations[i])\n            db_i = np.sum(delta, axis=0)\n            \n            dW.insert(0, dW_i)\n            db.insert(0, db_i)\n            \n            if i &gt; 0:\n                # Propagate error to previous layer\n                delta = np.dot(delta, self.weights[i])\n                # Apply derivative of activation function\n                delta = delta * self._activate_derivative(self.z_values[i-1])\n        \n        # Update weights and biases\n        for i in range(len(self.weights)):\n            self.weights[i] -= self.learning_rate * dW[i]\n            self.biases[i] -= self.learning_rate * db[i]\n    \n    def train(self, X_train, y_train, epochs=100, verbose=True):\n        \"\"\"Train the MLP\"\"\"\n        losses = []\n        \n        # Convert labels to one-hot encoding\n        if self.output_size &gt; 1:\n            y_train_oh = np.zeros((len(y_train), self.output_size))\n            y_train_oh[np.arange(len(y_train)), y_train] = 1\n        else:\n            y_train_oh = y_train.reshape(-1, 1)\n        \n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self.forward(X_train)\n            \n            # Compute loss\n            if self.output_size == 1:\n                loss = self.binary_cross_entropy(y_train, y_pred.flatten())\n            else:\n                loss = self.categorical_cross_entropy(y_train_oh, y_pred)\n            losses.append(loss)\n            \n            # Backward pass\n            self.backward(X_train, y_train_oh)\n            \n            # Print progress\n            if verbose and (epoch + 1) % 50 == 0:\n                if self.output_size == 1:\n                    accuracy = np.mean((y_pred.flatten() &gt; 0.5) == y_train)\n                else:\n                    accuracy = np.mean(np.argmax(y_pred, axis=1) == y_train)\n                print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")\n        \n        return losses\n    \n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        probs = self.forward(X)\n        if self.output_size == 1:\n            return (probs.flatten() &gt; 0.5).astype(int)\n        else:\n            return np.argmax(probs, axis=1)\n    \n    def predict_proba(self, X):\n        \"\"\"Get prediction probabilities\"\"\"\n        return self.forward(X)\n</pre> # Reusable MLP class for multi-class classification class MLPMultiClassifier:     \"\"\"     Multi-Layer Perceptron for Multi-Class Classification     This is a reusable version that can handle both binary and multi-class problems     \"\"\"          def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int,                  learning_rate: float = 0.01, activation: str = 'tanh'):         \"\"\"Initialize the MLP\"\"\"         self.input_size = input_size         self.hidden_sizes = hidden_sizes         self.output_size = output_size         self.learning_rate = learning_rate         self.activation = activation                  # Initialize weights and biases         self.weights = []         self.biases = []                  # Layer sizes         layer_sizes = [input_size] + hidden_sizes + [output_size]                  # Initialize parameters         for i in range(len(layer_sizes) - 1):             if activation == 'relu':                 w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2/layer_sizes[i])             else:                 w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(1/layer_sizes[i])             b = np.zeros(layer_sizes[i+1])             self.weights.append(w)             self.biases.append(b)                  self.activations = []         self.z_values = []              def _activate(self, z):         \"\"\"Apply activation function\"\"\"         if self.activation == 'tanh':             return np.tanh(z)         elif self.activation == 'relu':             return np.maximum(0, z)         elif self.activation == 'sigmoid':             return 1 / (1 + np.exp(-np.clip(z, -500, 500)))              def _activate_derivative(self, z):         \"\"\"Compute derivative of activation function\"\"\"         if self.activation == 'tanh':             return 1 - np.tanh(z)**2         elif self.activation == 'relu':             return (z &gt; 0).astype(float)         elif self.activation == 'sigmoid':             sig = self._activate(z)             return sig * (1 - sig)          def _softmax(self, z):         \"\"\"Softmax for output layer (multi-class)\"\"\"         exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability         return exp_z / np.sum(exp_z, axis=1, keepdims=True)          def forward(self, X):         \"\"\"Forward pass through the network\"\"\"         self.activations = [X]         self.z_values = []                  current_input = X                  # Hidden layers         for i in range(len(self.weights) - 1):             z = np.dot(current_input, self.weights[i].T) + self.biases[i]             self.z_values.append(z)             a = self._activate(z)             self.activations.append(a)             current_input = a                  # Output layer         z = np.dot(current_input, self.weights[-1].T) + self.biases[-1]         self.z_values.append(z)                  if self.output_size == 1:             # Binary classification             output = 1 / (1 + np.exp(-np.clip(z, -500, 500)))         else:             # Multi-class classification             output = self._softmax(z)                  self.activations.append(output)         return output          def categorical_cross_entropy(self, y_true_oh, y_pred):         \"\"\"Compute categorical cross-entropy loss\"\"\"         eps = 1e-7         y_pred = np.clip(y_pred, eps, 1 - eps)         return -np.mean(np.sum(y_true_oh * np.log(y_pred), axis=1))          def binary_cross_entropy(self, y_true, y_pred):         \"\"\"Binary cross-entropy (for compatibility)\"\"\"         eps = 1e-7         y_pred = np.clip(y_pred, eps, 1 - eps)         return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))          def backward(self, X, y_true_oh):         \"\"\"Backward pass (backpropagation) for multi-class\"\"\"         m = len(X)                  # Gradients storage         dW = []         db = []                  # Output layer gradient (softmax with cross-entropy)         y_pred = self.activations[-1]         delta = (y_pred - y_true_oh) / m                  # Backpropagate through layers         for i in range(len(self.weights) - 1, -1, -1):             # Gradient for weights and biases             dW_i = np.dot(delta.T, self.activations[i])             db_i = np.sum(delta, axis=0)                          dW.insert(0, dW_i)             db.insert(0, db_i)                          if i &gt; 0:                 # Propagate error to previous layer                 delta = np.dot(delta, self.weights[i])                 # Apply derivative of activation function                 delta = delta * self._activate_derivative(self.z_values[i-1])                  # Update weights and biases         for i in range(len(self.weights)):             self.weights[i] -= self.learning_rate * dW[i]             self.biases[i] -= self.learning_rate * db[i]          def train(self, X_train, y_train, epochs=100, verbose=True):         \"\"\"Train the MLP\"\"\"         losses = []                  # Convert labels to one-hot encoding         if self.output_size &gt; 1:             y_train_oh = np.zeros((len(y_train), self.output_size))             y_train_oh[np.arange(len(y_train)), y_train] = 1         else:             y_train_oh = y_train.reshape(-1, 1)                  for epoch in range(epochs):             # Forward pass             y_pred = self.forward(X_train)                          # Compute loss             if self.output_size == 1:                 loss = self.binary_cross_entropy(y_train, y_pred.flatten())             else:                 loss = self.categorical_cross_entropy(y_train_oh, y_pred)             losses.append(loss)                          # Backward pass             self.backward(X_train, y_train_oh)                          # Print progress             if verbose and (epoch + 1) % 50 == 0:                 if self.output_size == 1:                     accuracy = np.mean((y_pred.flatten() &gt; 0.5) == y_train)                 else:                     accuracy = np.mean(np.argmax(y_pred, axis=1) == y_train)                 print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")                  return losses          def predict(self, X):         \"\"\"Make predictions\"\"\"         probs = self.forward(X)         if self.output_size == 1:             return (probs.flatten() &gt; 0.5).astype(int)         else:             return np.argmax(probs, axis=1)          def predict_proba(self, X):         \"\"\"Get prediction probabilities\"\"\"         return self.forward(X) In\u00a0[270]: Copied! <pre># Create and train the MLP for Exercise 3\nprint(\"\\nTraining MLP for Multi-Class Classification\")\nprint(\"-\" * 40)\nmlp_ex3 = MLPMultiClassifier(\n    input_size=4,\n    hidden_sizes=[16],  # Single hidden layer\n    output_size=3,\n    learning_rate=0.1,\n    activation='tanh'\n)\n\n# Train the model\nlosses_ex3 = mlp_ex3.train(X_train_ex3, y_train_ex3, epochs=500, verbose=True)\n</pre> # Create and train the MLP for Exercise 3 print(\"\\nTraining MLP for Multi-Class Classification\") print(\"-\" * 40) mlp_ex3 = MLPMultiClassifier(     input_size=4,     hidden_sizes=[16],  # Single hidden layer     output_size=3,     learning_rate=0.1,     activation='tanh' )  # Train the model losses_ex3 = mlp_ex3.train(X_train_ex3, y_train_ex3, epochs=500, verbose=True) <pre>\nTraining MLP for Multi-Class Classification\n----------------------------------------\nEpoch 50/500 - Loss: 0.9518 - Accuracy: 0.5550\nEpoch 100/500 - Loss: 0.8063 - Accuracy: 0.7450\nEpoch 150/500 - Loss: 0.6013 - Accuracy: 0.8900\nEpoch 200/500 - Loss: 0.4148 - Accuracy: 0.9733\nEpoch 250/500 - Loss: 0.2915 - Accuracy: 0.9850\nEpoch 300/500 - Loss: 0.2173 - Accuracy: 0.9883\nEpoch 350/500 - Loss: 0.1710 - Accuracy: 0.9908\nEpoch 400/500 - Loss: 0.1401 - Accuracy: 0.9917\nEpoch 450/500 - Loss: 0.1185 - Accuracy: 0.9925\nEpoch 500/500 - Loss: 0.1027 - Accuracy: 0.9925\n</pre> In\u00a0[271]: Copied! <pre># Training accuracy\ntrain_pred_ex3 = mlp_ex3.predict(X_train_ex3)\ntrain_acc_ex3 = accuracy_score(y_train_ex3, train_pred_ex3)\nprint(f\"Training Accuracy: {train_acc_ex3:.4f}\")\n\n# Testing accuracy\ntest_pred_ex3 = mlp_ex3.predict(X_test_ex3)\ntest_acc_ex3 = accuracy_score(y_test_ex3, test_pred_ex3)\nprint(f\"Testing Accuracy: {test_acc_ex3:.4f}\")\n\n# Confusion matrix\ncm_ex3 = confusion_matrix(y_test_ex3, test_pred_ex3)\nprint(f\"\\nConfusion Matrix:\")\nprint(cm_ex3)\n\n# Per-class accuracy\nfor i in range(3):\n    class_mask = y_test_ex3 == i\n    if np.any(class_mask):\n        class_acc = accuracy_score(y_test_ex3[class_mask], test_pred_ex3[class_mask])\n        print(f\"Class {i} Accuracy: {class_acc:.4f}\")\n</pre> # Training accuracy train_pred_ex3 = mlp_ex3.predict(X_train_ex3) train_acc_ex3 = accuracy_score(y_train_ex3, train_pred_ex3) print(f\"Training Accuracy: {train_acc_ex3:.4f}\")  # Testing accuracy test_pred_ex3 = mlp_ex3.predict(X_test_ex3) test_acc_ex3 = accuracy_score(y_test_ex3, test_pred_ex3) print(f\"Testing Accuracy: {test_acc_ex3:.4f}\")  # Confusion matrix cm_ex3 = confusion_matrix(y_test_ex3, test_pred_ex3) print(f\"\\nConfusion Matrix:\") print(cm_ex3)  # Per-class accuracy for i in range(3):     class_mask = y_test_ex3 == i     if np.any(class_mask):         class_acc = accuracy_score(y_test_ex3[class_mask], test_pred_ex3[class_mask])         print(f\"Class {i} Accuracy: {class_acc:.4f}\") <pre>Training Accuracy: 0.9925\nTesting Accuracy: 0.9867\n\nConfusion Matrix:\n[[109   0   1]\n [  0  94   0]\n [  3   0  93]]\nClass 0 Accuracy: 0.9909\nClass 1 Accuracy: 1.0000\nClass 2 Accuracy: 0.9688\n</pre> In\u00a0[272]: Copied! <pre># Visualization for Exercise 3\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Plot 1: Training Loss\naxes[0, 0].plot(losses_ex3)\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Categorical Cross-Entropy Loss')\naxes[0, 0].set_title('Training Loss Over Time')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Confusion Matrix\nsns.heatmap(cm_ex3, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\naxes[0, 1].set_xlabel('Predicted')\naxes[0, 1].set_ylabel('Actual')\naxes[0, 1].set_title('Confusion Matrix - Test Set')\n\n# Plot 3: Feature space visualization (first 2 features)\nscatter = axes[1, 0].scatter(X_train_ex3[:, 0], X_train_ex3[:, 1], \n                            c=y_train_ex3, cmap='viridis', alpha=0.6)\naxes[1, 0].set_xlabel('Feature 1')\naxes[1, 0].set_ylabel('Feature 2')\naxes[1, 0].set_title('Training Data (First 2 Features)')\nplt.colorbar(scatter, ax=axes[1, 0])\n\n# Plot 4: Predicted vs Actual for test set\naxes[1, 1].scatter(range(len(y_test_ex3)), y_test_ex3, \n                   alpha=0.5, label='Actual', s=30)\naxes[1, 1].scatter(range(len(test_pred_ex3)), test_pred_ex3, \n                   alpha=0.5, label='Predicted', s=30)\naxes[1, 1].set_xlabel('Sample Index')\naxes[1, 1].set_ylabel('Class')\naxes[1, 1].set_title(f'Test Set: Actual vs Predicted (Accuracy: {test_acc_ex3:.2%})')\naxes[1, 1].legend()\naxes[1, 1].set_yticks([0, 1, 2])\n\nplt.tight_layout()\nplt.show()\n</pre> # Visualization for Exercise 3 fig, axes = plt.subplots(2, 2, figsize=(14, 12))  # Plot 1: Training Loss axes[0, 0].plot(losses_ex3) axes[0, 0].set_xlabel('Epoch') axes[0, 0].set_ylabel('Categorical Cross-Entropy Loss') axes[0, 0].set_title('Training Loss Over Time') axes[0, 0].grid(True, alpha=0.3)  # Plot 2: Confusion Matrix sns.heatmap(cm_ex3, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1]) axes[0, 1].set_xlabel('Predicted') axes[0, 1].set_ylabel('Actual') axes[0, 1].set_title('Confusion Matrix - Test Set')  # Plot 3: Feature space visualization (first 2 features) scatter = axes[1, 0].scatter(X_train_ex3[:, 0], X_train_ex3[:, 1],                              c=y_train_ex3, cmap='viridis', alpha=0.6) axes[1, 0].set_xlabel('Feature 1') axes[1, 0].set_ylabel('Feature 2') axes[1, 0].set_title('Training Data (First 2 Features)') plt.colorbar(scatter, ax=axes[1, 0])  # Plot 4: Predicted vs Actual for test set axes[1, 1].scatter(range(len(y_test_ex3)), y_test_ex3,                     alpha=0.5, label='Actual', s=30) axes[1, 1].scatter(range(len(test_pred_ex3)), test_pred_ex3,                     alpha=0.5, label='Predicted', s=30) axes[1, 1].set_xlabel('Sample Index') axes[1, 1].set_ylabel('Class') axes[1, 1].set_title(f'Test Set: Actual vs Predicted (Accuracy: {test_acc_ex3:.2%})') axes[1, 1].legend() axes[1, 1].set_yticks([0, 1, 2])  plt.tight_layout() plt.show() <p>The reusable MLP successfully handles multi-class classification using softmax output and categorical cross-entropy loss. The model learns to distinguish between the three classes despite their varying cluster structures.</p> In\u00a0[273]: Copied! <pre># Use the same data as Exercise 3\nX_ex4, y_ex4 = X_ex3.copy(), y_ex3.copy()\nX_train_ex4, X_test_ex4 = X_train_ex3.copy(), X_test_ex3.copy()\ny_train_ex4, y_test_ex4 = y_train_ex3.copy(), y_test_ex3.copy()\n\nprint(f\"\\nUsing the same dataset as Exercise 3:\")\nprint(f\"Training samples: {len(X_train_ex4)}\")\nprint(f\"Testing samples: {len(X_test_ex4)}\")\nprint(f\"Features: {X_ex4.shape[1]}\")\nprint(f\"Classes: {len(np.unique(y_ex4))}\")\n\n# Create and train a DEEPER MLP for Exercise 4\nprint(\"\\nTraining DEEPER MLP for Multi-Class Classification\")\nprint(\"Architecture: Input(4) -&gt; Hidden(32) -&gt; Hidden(16) -&gt; Hidden(8) -&gt; Output(3)\")\nprint(\"-\" * 40)\n\nmlp_ex4 = MLPMultiClassifier(\n    input_size=4,\n    hidden_sizes=[32, 16, 8],  # 3 hidden layers (deeper architecture)\n    output_size=3,\n    learning_rate=0.1,\n    activation='tanh'\n)\n\n# Train the deeper model\nlosses_ex4 = mlp_ex4.train(X_train_ex4, y_train_ex4, epochs=500, verbose=True)\n</pre> # Use the same data as Exercise 3 X_ex4, y_ex4 = X_ex3.copy(), y_ex3.copy() X_train_ex4, X_test_ex4 = X_train_ex3.copy(), X_test_ex3.copy() y_train_ex4, y_test_ex4 = y_train_ex3.copy(), y_test_ex3.copy()  print(f\"\\nUsing the same dataset as Exercise 3:\") print(f\"Training samples: {len(X_train_ex4)}\") print(f\"Testing samples: {len(X_test_ex4)}\") print(f\"Features: {X_ex4.shape[1]}\") print(f\"Classes: {len(np.unique(y_ex4))}\")  # Create and train a DEEPER MLP for Exercise 4 print(\"\\nTraining DEEPER MLP for Multi-Class Classification\") print(\"Architecture: Input(4) -&gt; Hidden(32) -&gt; Hidden(16) -&gt; Hidden(8) -&gt; Output(3)\") print(\"-\" * 40)  mlp_ex4 = MLPMultiClassifier(     input_size=4,     hidden_sizes=[32, 16, 8],  # 3 hidden layers (deeper architecture)     output_size=3,     learning_rate=0.1,     activation='tanh' )  # Train the deeper model losses_ex4 = mlp_ex4.train(X_train_ex4, y_train_ex4, epochs=500, verbose=True) <pre>\nUsing the same dataset as Exercise 3:\nTraining samples: 1200\nTesting samples: 300\nFeatures: 4\nClasses: 3\n\nTraining DEEPER MLP for Multi-Class Classification\nArchitecture: Input(4) -&gt; Hidden(32) -&gt; Hidden(16) -&gt; Hidden(8) -&gt; Output(3)\n----------------------------------------\nEpoch 50/500 - Loss: 0.9281 - Accuracy: 0.6300\nEpoch 100/500 - Loss: 0.5612 - Accuracy: 0.8792\nEpoch 150/500 - Loss: 0.3109 - Accuracy: 0.9808\nEpoch 200/500 - Loss: 0.1816 - Accuracy: 0.9875\nEpoch 250/500 - Loss: 0.1201 - Accuracy: 0.9867\nEpoch 300/500 - Loss: 0.0879 - Accuracy: 0.9917\nEpoch 350/500 - Loss: 0.0687 - Accuracy: 0.9917\nEpoch 400/500 - Loss: 0.0561 - Accuracy: 0.9925\nEpoch 450/500 - Loss: 0.0472 - Accuracy: 0.9942\nEpoch 500/500 - Loss: 0.0406 - Accuracy: 0.9950\n</pre> In\u00a0[274]: Copied! <pre># Training accuracy\ntrain_pred_ex4 = mlp_ex4.predict(X_train_ex4)\ntrain_acc_ex4 = accuracy_score(y_train_ex4, train_pred_ex4)\nprint(f\"Training Accuracy: {train_acc_ex4:.4f}\")\n\n# Testing accuracy\ntest_pred_ex4 = mlp_ex4.predict(X_test_ex4)\ntest_acc_ex4 = accuracy_score(y_test_ex4, test_pred_ex4)\nprint(f\"Testing Accuracy: {test_acc_ex4:.4f}\")\n\n# Confusion matrix\ncm_ex4 = confusion_matrix(y_test_ex4, test_pred_ex4)\nprint(f\"\\nConfusion Matrix:\")\nprint(cm_ex4)\n\n# Per-class accuracy\nfor i in range(3):\n    class_mask = y_test_ex4 == i\n    if np.any(class_mask):\n        class_acc = accuracy_score(y_test_ex4[class_mask], test_pred_ex4[class_mask])\n        print(f\"Class {i} Accuracy: {class_acc:.4f}\")\n\nprint(\"\\nArchitecture Comparison:\")\nprint(f\"Exercise 3 (1 hidden layer): Test Accuracy = {test_acc_ex3:.4f}\")\nprint(f\"Exercise 4 (3 hidden layers): Test Accuracy = {test_acc_ex4:.4f}\")\nprint(f\"Improvement: {(test_acc_ex4 - test_acc_ex3)*100:.2f}%\")\n</pre> # Training accuracy train_pred_ex4 = mlp_ex4.predict(X_train_ex4) train_acc_ex4 = accuracy_score(y_train_ex4, train_pred_ex4) print(f\"Training Accuracy: {train_acc_ex4:.4f}\")  # Testing accuracy test_pred_ex4 = mlp_ex4.predict(X_test_ex4) test_acc_ex4 = accuracy_score(y_test_ex4, test_pred_ex4) print(f\"Testing Accuracy: {test_acc_ex4:.4f}\")  # Confusion matrix cm_ex4 = confusion_matrix(y_test_ex4, test_pred_ex4) print(f\"\\nConfusion Matrix:\") print(cm_ex4)  # Per-class accuracy for i in range(3):     class_mask = y_test_ex4 == i     if np.any(class_mask):         class_acc = accuracy_score(y_test_ex4[class_mask], test_pred_ex4[class_mask])         print(f\"Class {i} Accuracy: {class_acc:.4f}\")  print(\"\\nArchitecture Comparison:\") print(f\"Exercise 3 (1 hidden layer): Test Accuracy = {test_acc_ex3:.4f}\") print(f\"Exercise 4 (3 hidden layers): Test Accuracy = {test_acc_ex4:.4f}\") print(f\"Improvement: {(test_acc_ex4 - test_acc_ex3)*100:.2f}%\") <pre>Training Accuracy: 0.9950\nTesting Accuracy: 0.9967\n\nConfusion Matrix:\n[[110   0   0]\n [  0  94   0]\n [  1   0  95]]\nClass 0 Accuracy: 1.0000\nClass 1 Accuracy: 1.0000\nClass 2 Accuracy: 0.9896\n\nArchitecture Comparison:\nExercise 3 (1 hidden layer): Test Accuracy = 0.9867\nExercise 4 (3 hidden layers): Test Accuracy = 0.9967\nImprovement: 1.00%\n</pre> In\u00a0[275]: Copied! <pre># Cleaner visualization for Exercise 4\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Plot 1: Training Loss Comparison\naxes[0].plot(losses_ex3, label='Exercise 3 (Shallow)', alpha=0.7)\naxes[0].plot(losses_ex4, label='Exercise 4 (Deep)', alpha=0.7)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss Comparison')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Plot 2: Confusion Matrix for Exercise 4\nsns.heatmap(cm_ex4, annot=True, fmt='d', cmap='Blues', ax=axes[1])\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('Actual')\naxes[1].set_title('Confusion Matrix - Exercise 4')\n\n# Plot 3: Accuracy Comparison (Ex3 vs Ex4)\nmodels = ['Ex3\\n(1 Hidden)', 'Ex4\\n(3 Hidden)']\ntrain_accs = [train_acc_ex3, train_acc_ex4]\ntest_accs = [test_acc_ex3, test_acc_ex4]\n\nx = np.arange(len(models))\nwidth = 0.35\n\nbars1 = axes[2].bar(x - width/2, train_accs, width, label='Train', color='skyblue')\nbars2 = axes[2].bar(x + width/2, test_accs, width, label='Test', color='lightcoral')\n\naxes[2].set_ylabel('Accuracy')\naxes[2].set_title('Model Accuracy Comparison')\naxes[2].set_xticks(x)\naxes[2].set_xticklabels(models)\naxes[2].legend()\naxes[2].set_ylim([0, 1])\n\n# Add accuracy values on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        axes[2].annotate(f'{height:.3f}',\n                        xy=(bar.get_x() + bar.get_width() / 2, height/2),\n                        xytext=(0, 3),\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n</pre> # Cleaner visualization for Exercise 4 fig, axes = plt.subplots(1, 3, figsize=(18, 5))  # Plot 1: Training Loss Comparison axes[0].plot(losses_ex3, label='Exercise 3 (Shallow)', alpha=0.7) axes[0].plot(losses_ex4, label='Exercise 4 (Deep)', alpha=0.7) axes[0].set_xlabel('Epoch') axes[0].set_ylabel('Loss') axes[0].set_title('Training Loss Comparison') axes[0].legend() axes[0].grid(True, alpha=0.3)  # Plot 2: Confusion Matrix for Exercise 4 sns.heatmap(cm_ex4, annot=True, fmt='d', cmap='Blues', ax=axes[1]) axes[1].set_xlabel('Predicted') axes[1].set_ylabel('Actual') axes[1].set_title('Confusion Matrix - Exercise 4')  # Plot 3: Accuracy Comparison (Ex3 vs Ex4) models = ['Ex3\\n(1 Hidden)', 'Ex4\\n(3 Hidden)'] train_accs = [train_acc_ex3, train_acc_ex4] test_accs = [test_acc_ex3, test_acc_ex4]  x = np.arange(len(models)) width = 0.35  bars1 = axes[2].bar(x - width/2, train_accs, width, label='Train', color='skyblue') bars2 = axes[2].bar(x + width/2, test_accs, width, label='Test', color='lightcoral')  axes[2].set_ylabel('Accuracy') axes[2].set_title('Model Accuracy Comparison') axes[2].set_xticks(x) axes[2].set_xticklabels(models) axes[2].legend() axes[2].set_ylim([0, 1])  # Add accuracy values on bars for bars in [bars1, bars2]:     for bar in bars:         height = bar.get_height()         axes[2].annotate(f'{height:.3f}',                         xy=(bar.get_x() + bar.get_width() / 2, height/2),                         xytext=(0, 3),                         textcoords=\"offset points\",                         ha='center', va='bottom')  plt.tight_layout() plt.show()  <p>The deeper MLP improved accuracy slightly compared to the shallow one, confirming that additional layers can enhance the model\u2019s ability to capture complex decision boundaries. However, the gain is small, showing that deeper is not always necessary when the data is already well-separated.</p>"},{"location":"exercises/mlp/notebook/#multi-layer-perceptrons-mlps","title":"Multi-Layer Perceptrons (MLPs)\u00b6","text":"<p>This notebook implements Multi-Layer Perceptrons from scratch, covering manual calculations, binary classification, multi-class classification, and deeper architectures. All implementations use only NumPy for matrix operations, with activation functions, loss calculations, and gradients implemented manually.</p>"},{"location":"exercises/mlp/notebook/#exercise-1-manual-calculation-of-mlp-steps","title":"Exercise 1: Manual Calculation of MLP Steps\u00b6","text":"<p>We'll manually calculate the forward pass, loss computation, backpropagation, and parameter updates for a simple MLP with 2 input features, 1 hidden layer containing 2 neurons, and 1 output neuron.</p>"},{"location":"exercises/mlp/notebook/#exercise-2-binary-classification-with-synthetic-data-and-scratch-mlp","title":"Exercise 2: Binary Classification with Synthetic Data and Scratch MLP\u00b6","text":"<p>We'll generate a synthetic dataset with 1 cluster for class 0 and 2 clusters for class 1, then implement an MLP from scratch for binary classification.</p>"},{"location":"exercises/mlp/notebook/#exercise-3-multi-class-classification-with-synthetic-data-and-reusable-mlp","title":"Exercise 3: Multi-Class Classification with Synthetic Data and Reusable MLP\u00b6","text":"<p>We'll generate a 3-class dataset with varying numbers of clusters per class (2, 3, and 4 clusters) and implement a reusable MLP that can handle both binary and multi-class classification.</p>"},{"location":"exercises/mlp/notebook/#exercise-4-multi-class-classification-with-deeper-mlp","title":"Exercise 4: Multi-Class Classification with Deeper MLP\u00b6","text":"<p>Using the same dataset as Exercise 3 but with a deeper architecture (at least 2 hidden layers) to demonstrate the effect of network depth on performance.</p>"},{"location":"exercises/mlp/notebook/#ai-assistance","title":"AI Assistance\u00b6","text":"<p>I used AI (Claude and ChatGPT) to help with:</p> <ul> <li>Reviewing my code and outputs for clarity.</li> <li>Suggesting improvements to formatting and readability.</li> </ul> <p>All code was executed, tested, and validated locally by me. Nothing was done with AI that I didn't understand.</p>"},{"location":"exercises/perceptron/notebook/","title":"2. Perceptron","text":"<p>For this exercise, we will implement a simple Perceptron model from scratch using only NumPy for basic linear algebra operations. Below is the perceptron class, along with functions to visualize the decision boundary and accuracy over epochs.</p> In\u00a0[24]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Random seed for reproducibility\nnp.random.seed(24)\n\n# Perceptron Implementation\nclass Perceptron:\n    def __init__(self, input_dim=2, learning_rate=0.01, max_epochs=100):\n        self.w = np.zeros(input_dim)   # weights\n        self.b = 0.0                   # bias\n        self.lr = learning_rate\n        self.max_epochs = max_epochs\n        self.accuracy_history = []\n\n    # Prediction function\n    def predict(self, X):\n        linear_output = np.dot(X, self.w) + self.b\n        return np.where(linear_output &gt;= 0, 1, -1)\n\n    # Training function\n    def fit(self, X, y):\n        for epoch in range(self.max_epochs):\n            errors = 0\n            for xi, target in zip(X, y):\n                pred = self.predict(xi)\n                if pred != target: # Misclassification\n                    update = self.lr * target\n                    self.w += update * xi\n                    self.b += update\n                    errors += 1\n            acc = self.evaluate(X, y)\n            self.accuracy_history.append(acc)\n            if errors == 0:\n                break\n    \n    # Evaluation function\n    def evaluate(self, X, y):\n        predictions = self.predict(X)\n        return np.mean(predictions == y)\n\n# Utility functions\ndef plot_data(X, y, title=\"Data Distribution\"):\n    plt.figure(figsize=(6,6))\n    plt.scatter(X[y==1,0], X[y==1,1], color=\"red\", label=\"Class 0\", alpha=0.6)\n    plt.scatter(X[y==-1,0], X[y==-1,1], color=\"blue\", label=\"Class 1\", alpha=0.6)\n    plt.legend()\n    plt.title(title)\n    plt.xlabel(\"X1\")\n    plt.ylabel(\"X2\")\n    plt.grid(True)\n    plt.show()\n\n# Decision boundary plotting\ndef plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n    plt.figure(figsize=(6,6))\n    plt.scatter(X[y==1,0], X[y==1,1], color=\"red\", label=\"Class 0\", alpha=0.6)\n    plt.scatter(X[y==-1,0], X[y==-1,1], color=\"blue\", label=\"Class 1\", alpha=0.6)\n\n    # Decision boundary\n    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n    xx = np.linspace(x_min, x_max, 200)\n    if model.w[1] != 0: \n        yy = -(model.w[0] * xx + model.b) / model.w[1]\n        plt.plot(xx, yy, \"k--\", label=\"Decision boundary\")\n\n    # Highlight misclassified\n    preds = model.predict(X)\n    misclassified = X[preds != y]\n    if len(misclassified) &gt; 0:\n        plt.scatter(misclassified[:,0], misclassified[:,1], \n                    facecolors='none', edgecolors='yellow', s=80, label=\"Misclassified\")\n\n    plt.legend()\n    plt.title(title)\n    plt.xlabel(\"X1\")\n    plt.ylabel(\"X2\")\n    plt.grid(True)\n    plt.show()\n\n# Accuracy over epochs plotting\ndef plot_accuracy(history, title=\"Accuracy over Epochs\"):\n    plt.figure()\n    plt.plot(range(1, len(history)+1), history, marker=\"o\")\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.grid(True)\n    plt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  # Random seed for reproducibility np.random.seed(24)  # Perceptron Implementation class Perceptron:     def __init__(self, input_dim=2, learning_rate=0.01, max_epochs=100):         self.w = np.zeros(input_dim)   # weights         self.b = 0.0                   # bias         self.lr = learning_rate         self.max_epochs = max_epochs         self.accuracy_history = []      # Prediction function     def predict(self, X):         linear_output = np.dot(X, self.w) + self.b         return np.where(linear_output &gt;= 0, 1, -1)      # Training function     def fit(self, X, y):         for epoch in range(self.max_epochs):             errors = 0             for xi, target in zip(X, y):                 pred = self.predict(xi)                 if pred != target: # Misclassification                     update = self.lr * target                     self.w += update * xi                     self.b += update                     errors += 1             acc = self.evaluate(X, y)             self.accuracy_history.append(acc)             if errors == 0:                 break          # Evaluation function     def evaluate(self, X, y):         predictions = self.predict(X)         return np.mean(predictions == y)  # Utility functions def plot_data(X, y, title=\"Data Distribution\"):     plt.figure(figsize=(6,6))     plt.scatter(X[y==1,0], X[y==1,1], color=\"red\", label=\"Class 0\", alpha=0.6)     plt.scatter(X[y==-1,0], X[y==-1,1], color=\"blue\", label=\"Class 1\", alpha=0.6)     plt.legend()     plt.title(title)     plt.xlabel(\"X1\")     plt.ylabel(\"X2\")     plt.grid(True)     plt.show()  # Decision boundary plotting def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):     plt.figure(figsize=(6,6))     plt.scatter(X[y==1,0], X[y==1,1], color=\"red\", label=\"Class 0\", alpha=0.6)     plt.scatter(X[y==-1,0], X[y==-1,1], color=\"blue\", label=\"Class 1\", alpha=0.6)      # Decision boundary     x_min, x_max = X[:,0].min()-1, X[:,0].max()+1     y_min, y_max = X[:,1].min()-1, X[:,1].max()+1     xx = np.linspace(x_min, x_max, 200)     if model.w[1] != 0:          yy = -(model.w[0] * xx + model.b) / model.w[1]         plt.plot(xx, yy, \"k--\", label=\"Decision boundary\")      # Highlight misclassified     preds = model.predict(X)     misclassified = X[preds != y]     if len(misclassified) &gt; 0:         plt.scatter(misclassified[:,0], misclassified[:,1],                      facecolors='none', edgecolors='yellow', s=80, label=\"Misclassified\")      plt.legend()     plt.title(title)     plt.xlabel(\"X1\")     plt.ylabel(\"X2\")     plt.grid(True)     plt.show()  # Accuracy over epochs plotting def plot_accuracy(history, title=\"Accuracy over Epochs\"):     plt.figure()     plt.plot(range(1, len(history)+1), history, marker=\"o\")     plt.title(title)     plt.xlabel(\"Epoch\")     plt.ylabel(\"Accuracy\")     plt.grid(True)     plt.show() <p>First we're going to generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions with the parameters that were given:</p> In\u00a0[25]: Copied! <pre># Parameters for linearly separable data\nmean0, cov0 = [1.5, 1.5], [[0.5, 0], [0, 0.5]]\nmean1, cov1 = [5, 5], [[0.5, 0], [0, 0.5]]\n\n# Generate linearly separable data\nclass0 = np.random.multivariate_normal(mean0, cov0, 1000)\nclass1 = np.random.multivariate_normal(mean1, cov1, 1000)\n\n# Combine into dataset\nX1 = np.vstack((class0, class1))\ny1 = np.hstack((-1*np.ones(1000), np.ones(1000)))\n\nplot_data(X1, y1, \"Linearly Separable Data\")\n</pre> # Parameters for linearly separable data mean0, cov0 = [1.5, 1.5], [[0.5, 0], [0, 0.5]] mean1, cov1 = [5, 5], [[0.5, 0], [0, 0.5]]  # Generate linearly separable data class0 = np.random.multivariate_normal(mean0, cov0, 1000) class1 = np.random.multivariate_normal(mean1, cov1, 1000)  # Combine into dataset X1 = np.vstack((class0, class1)) y1 = np.hstack((-1*np.ones(1000), np.ones(1000)))  plot_data(X1, y1, \"Linearly Separable Data\") <p>Now, we're going to implement a single-layer perceptron from scratch to classify the generated data into the two classes, using NumPy only for basic linear algebra operations.</p> In\u00a0[26]: Copied! <pre># Train Perceptron\nperc1 = Perceptron(input_dim=2, learning_rate=0.01, max_epochs=100)\nperc1.fit(X1, y1)\n\nprint(\"Final weights:\", perc1.w)\nprint(\"Final bias:\", perc1.b)\nprint(\"Final accuracy:\", perc1.evaluate(X1, y1))\n\nplot_decision_boundary(perc1, X1, y1, \"Decision Boundary\")\nplot_accuracy(perc1.accuracy_history, \"Accuracy over Epochs\")\n</pre> # Train Perceptron perc1 = Perceptron(input_dim=2, learning_rate=0.01, max_epochs=100) perc1.fit(X1, y1)  print(\"Final weights:\", perc1.w) print(\"Final bias:\", perc1.b) print(\"Final accuracy:\", perc1.evaluate(X1, y1))  plot_decision_boundary(perc1, X1, y1, \"Decision Boundary\") plot_accuracy(perc1.accuracy_history, \"Accuracy over Epochs\") <pre>Final weights: [0.02109579 0.03614885]\nFinal bias: -0.20000000000000004\nFinal accuracy: 1.0\n</pre> <p>The data's separability leads to quick convergence because the two clusters are centered far apart with little overlap, which means a straight line can perfectly separate them. In such a situation, the perceptron learning rule rapidly adjusts the weights after only a few misclassifications, since each update moves the decision boundary closer to an exact separator. Once all points fall on the correct side, no more updates are needed, and the algorithm converges in very few epochs. This clean separation also explains why the model achieves 100% accuracy rather than oscillating or plateauing below it.</p> <p>Now, we'll generate two new classes of 2D data points (1000 samples per class) using multivariate normal distributions with the parameters that were given, which will create partial overlap between the classes:</p> In\u00a0[27]: Copied! <pre>np.random.seed(24)\n# Parameters for overlapping data\nmean0, cov0 = [3, 3], [[1.5, 0], [0, 1.5]]\nmean1, cov1 = [4, 4], [[1.5, 0], [0, 1.5]]\n\n# Generate overlapping data\nclass0 = np.random.multivariate_normal(mean0, cov0, 1000)\nclass1 = np.random.multivariate_normal(mean1, cov1, 1000)\n\n# Combine into dataset\nX2 = np.vstack((class0, class1))\ny2 = np.hstack((-1*np.ones(1000), np.ones(1000)))\n\nplot_data(X2, y2, \"Exercise 2: Overlapping Data\")\n</pre> np.random.seed(24) # Parameters for overlapping data mean0, cov0 = [3, 3], [[1.5, 0], [0, 1.5]] mean1, cov1 = [4, 4], [[1.5, 0], [0, 1.5]]  # Generate overlapping data class0 = np.random.multivariate_normal(mean0, cov0, 1000) class1 = np.random.multivariate_normal(mean1, cov1, 1000)  # Combine into dataset X2 = np.vstack((class0, class1)) y2 = np.hstack((-1*np.ones(1000), np.ones(1000)))  plot_data(X2, y2, \"Exercise 2: Overlapping Data\") In\u00a0[28]: Copied! <pre>n_runs = 5\nresults = []\n\nfor i in range(n_runs):\n    # Reinitialize and train a new perceptron each run\n    perc = Perceptron(input_dim=2, learning_rate=0.01, max_epochs=100)\n    perc.fit(X2, y2)\n    \n    final_acc = perc.evaluate(X2, y2)\n    \n    results.append((perc, final_acc))\n\n# Select best run by highest final accuracy\nbest_run = max(results, key=lambda x: x[1])\nbest_perc, best_final_acc = best_run\n\nprint(\"Best Run:\")\nprint(\"Final weights:\", best_perc.w)\nprint(\"Final bias:\", best_perc.b)\nprint(f\"Final accuracy: {best_final_acc:.2f}\")\n\n# Plot decision boundary for best run\nplot_decision_boundary(best_perc, X2, y2, \"Decision Boundary (Best Run)\")\n\n# Plot accuracy history for best run\nplot_accuracy(best_perc.accuracy_history, \"Accuracy over Epochs (Best Run)\")\n</pre> n_runs = 5 results = []  for i in range(n_runs):     # Reinitialize and train a new perceptron each run     perc = Perceptron(input_dim=2, learning_rate=0.01, max_epochs=100)     perc.fit(X2, y2)          final_acc = perc.evaluate(X2, y2)          results.append((perc, final_acc))  # Select best run by highest final accuracy best_run = max(results, key=lambda x: x[1]) best_perc, best_final_acc = best_run  print(\"Best Run:\") print(\"Final weights:\", best_perc.w) print(\"Final bias:\", best_perc.b) print(f\"Final accuracy: {best_final_acc:.2f}\")  # Plot decision boundary for best run plot_decision_boundary(best_perc, X2, y2, \"Decision Boundary (Best Run)\")  # Plot accuracy history for best run plot_accuracy(best_perc.accuracy_history, \"Accuracy over Epochs (Best Run)\") <pre>Best Run:\nFinal weights: [0.06944049 0.09727756]\nFinal bias: -0.20000000000000004\nFinal accuracy: 0.51\n</pre> <p>Because the classes overlap, the perceptron cannot find a perfect linear separator. Unlike in Exercise 1, where convergence was guaranteed, here the updates never stabilize: correcting errors on one side inevitably creates misclassifications on the other. As a result, training does not converge, the accuracy hovers just above chance (around 50%), and many points remain incorrectly classified. This illustrates the perceptron\u2019s fundamental limitation: it only guarantees convergence when the data is linearly separable.</p>"},{"location":"exercises/perceptron/notebook/#perceptron","title":"Perceptron\u00b6","text":""},{"location":"exercises/perceptron/notebook/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"exercises/perceptron/notebook/#data-generation-task","title":"Data Generation Task\u00b6","text":""},{"location":"exercises/perceptron/notebook/#perceptron-implementation-task","title":"Perceptron Implementation Task\u00b6","text":""},{"location":"exercises/perceptron/notebook/#exercise-2","title":"Exercise 2\u00b6","text":""},{"location":"exercises/perceptron/notebook/#data-generation-task","title":"Data Generation Task\u00b6","text":""},{"location":"exercises/perceptron/notebook/#perceptron-implementation-task","title":"Perceptron Implementation Task\u00b6","text":""},{"location":"exercises/perceptron/notebook/#ai-assistance","title":"AI Assistance\u00b6","text":"<p>I used AI (ChatGPT) to help with:</p> <ul> <li>Reviewing my code and outputs for clarity.</li> <li>Suggesting improvements to formatting and readability.</li> </ul> <p>All code was executed, tested, and validated locally by me. Nothing was done with AI that I didn't understand.</p>"},{"location":"projects/classification/notebook/","title":"1. Classification","text":"In\u00a0[145]: Copied! <pre># Basic imports &amp; reproducibility\nimport os\nimport random\nfrom pathlib import Path\nimport warnings\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport copy\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator\n\n# stats / util\nfrom scipy import stats\nfrom tqdm.auto import tqdm\n\n# preprocessing &amp; modeling\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# metrics\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, roc_auc_score, roc_curve,\n    precision_recall_curve, average_precision_score, classification_report\n)\n\n# notebook display\n%matplotlib inline\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 200)\nwarnings.filterwarnings('ignore')\n\n# plotting style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_context('notebook')\n\n# seeds for reproducibility (use consistently across notebook)\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\n</pre> # Basic imports &amp; reproducibility import os import random from pathlib import Path import warnings  import math import numpy as np import pandas as pd import itertools import copy  # plotting import matplotlib.pyplot as plt import seaborn as sns from matplotlib.ticker import MaxNLocator  # stats / util from scipy import stats from tqdm.auto import tqdm  # preprocessing &amp; modeling from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split, StratifiedKFold from sklearn.utils.class_weight import compute_class_weight  # metrics from sklearn.metrics import (     accuracy_score, precision_score, recall_score, f1_score,     confusion_matrix, roc_auc_score, roc_curve,     precision_recall_curve, average_precision_score, classification_report )  # notebook display %matplotlib inline pd.set_option('display.max_columns', None) pd.set_option('display.width', 200) warnings.filterwarnings('ignore')  # plotting style plt.style.use('seaborn-v0_8-whitegrid') sns.set_context('notebook')  # seeds for reproducibility (use consistently across notebook) SEED = 42 random.seed(SEED) np.random.seed(SEED) os.environ['PYTHONHASHSEED'] = str(SEED) In\u00a0[146]: Copied! <pre># Load dataset\nDATA_DIR = Path(\"data\")\ntrain = pd.read_csv(DATA_DIR / \"train.csv\")\ntest = pd.read_csv(DATA_DIR / \"test.csv\")\n\nprint(\"Dataset Shape:\")\nprint(f\"  Training: {train.shape}\")\nprint(f\"  Test: {test.shape}\")\n\nprint(\"\\nFirst 5 rows:\")\ndisplay(train.head())\n\nprint(\"\\nData Types:\")\nprint(train.dtypes)\n</pre> # Load dataset DATA_DIR = Path(\"data\") train = pd.read_csv(DATA_DIR / \"train.csv\") test = pd.read_csv(DATA_DIR / \"test.csv\")  print(\"Dataset Shape:\") print(f\"  Training: {train.shape}\") print(f\"  Test: {test.shape}\")  print(\"\\nFirst 5 rows:\") display(train.head())  print(\"\\nData Types:\") print(train.dtypes) <pre>Dataset Shape:\n  Training: (750000, 18)\n  Test: (250000, 17)\n\nFirst 5 rows:\n</pre> id age job marital education default balance housing loan contact day month duration campaign pdays previous poutcome y 0 0 42 technician married secondary no 7 no no cellular 25 aug 117 3 -1 0 unknown 0 1 1 38 blue-collar married secondary no 514 no no unknown 18 jun 185 1 -1 0 unknown 0 2 2 36 blue-collar married secondary no 602 yes no unknown 14 may 111 2 -1 0 unknown 0 3 3 27 student single secondary no 34 yes no unknown 28 may 10 2 -1 0 unknown 0 4 4 26 technician married secondary no 889 yes no cellular 3 feb 902 1 -1 0 unknown 1 <pre>\nData Types:\nid            int64\nage           int64\njob          object\nmarital      object\neducation    object\ndefault      object\nbalance       int64\nhousing      object\nloan         object\ncontact      object\nday           int64\nmonth        object\nduration      int64\ncampaign      int64\npdays         int64\nprevious      int64\npoutcome     object\ny             int64\ndtype: object\n</pre> In\u00a0[147]: Copied! <pre># Identify feature types\ntarget_col = 'y'\nid_col = 'id'\n\nnumerical_features = train.select_dtypes(include=['number']).columns.tolist()\nnumerical_features = [col for col in numerical_features if col not in [id_col, target_col]]\n\ncategorical_features = train.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\nprint(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n</pre> # Identify feature types target_col = 'y' id_col = 'id'  numerical_features = train.select_dtypes(include=['number']).columns.tolist() numerical_features = [col for col in numerical_features if col not in [id_col, target_col]]  categorical_features = train.select_dtypes(include=['object']).columns.tolist()  print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\") print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\") <pre>Numerical features (7): ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n\nCategorical features (9): ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n</pre> In\u00a0[148]: Copied! <pre># Summary statistics\ndisplay(train[numerical_features].describe())\n\nprint(\"\\nTarget Variable Distribution:\")\nprint(train[target_col].value_counts())\nprint(f\"\\nClass Balance: {train[target_col].value_counts(normalize=True)}\")\n</pre> # Summary statistics display(train[numerical_features].describe())  print(\"\\nTarget Variable Distribution:\") print(train[target_col].value_counts()) print(f\"\\nClass Balance: {train[target_col].value_counts(normalize=True)}\") age balance day duration campaign pdays previous count 750000.000000 750000.000000 750000.000000 750000.000000 750000.000000 750000.000000 750000.000000 mean 40.926395 1204.067397 16.117209 256.229144 2.577008 22.412733 0.298545 std 10.098829 2836.096759 8.250832 272.555662 2.718514 77.319998 1.335926 min 18.000000 -8019.000000 1.000000 1.000000 1.000000 -1.000000 0.000000 25% 33.000000 0.000000 9.000000 91.000000 1.000000 -1.000000 0.000000 50% 39.000000 634.000000 17.000000 133.000000 2.000000 -1.000000 0.000000 75% 48.000000 1390.000000 21.000000 361.000000 3.000000 -1.000000 0.000000 max 95.000000 99717.000000 31.000000 4918.000000 63.000000 871.000000 200.000000 <pre>\nTarget Variable Distribution:\ny\n0    659512\n1     90488\nName: count, dtype: int64\n\nClass Balance: y\n0    0.879349\n1    0.120651\nName: proportion, dtype: float64\n</pre> In\u00a0[149]: Copied! <pre># Check for missing values\nprint(\"Missing Values:\")\nprint(train.isnull().sum())\n\n# Check for duplicates\nprint(f\"\\nDuplicate Rows: {train.duplicated().sum()}\")\n</pre> # Check for missing values print(\"Missing Values:\") print(train.isnull().sum())  # Check for duplicates print(f\"\\nDuplicate Rows: {train.duplicated().sum()}\") <pre>Missing Values:\nid           0\nage          0\njob          0\nmarital      0\neducation    0\ndefault      0\nbalance      0\nhousing      0\nloan         0\ncontact      0\nday          0\nmonth        0\nduration     0\ncampaign     0\npdays        0\nprevious     0\npoutcome     0\ny            0\ndtype: int64\n\nDuplicate Rows: 0\n</pre> In\u00a0[150]: Copied! <pre># Check for outliers (beyond 3 standard deviations)\noutlier_summary = {}\nfor col in numerical_features:\n    z = np.abs(stats.zscore(train[col]))\n    outlier_summary[col] = (z &gt; 3).mean() * 100\n\noutlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['% Outliers'])\ndisplay(outlier_df.sort_values('% Outliers', ascending=False))\n</pre> # Check for outliers (beyond 3 standard deviations) outlier_summary = {} for col in numerical_features:     z = np.abs(stats.zscore(train[col]))     outlier_summary[col] = (z &gt; 3).mean() * 100  outlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['% Outliers']) display(outlier_df.sort_values('% Outliers', ascending=False)) % Outliers pdays 4.114667 campaign 2.006000 duration 1.752133 previous 1.625867 balance 1.238267 age 0.547867 day 0.000000 In\u00a0[151]: Copied! <pre># Feature skewness\nskewness = train[numerical_features].skew().sort_values(ascending=False)\nprint(\"Feature Skewness:\")\ndisplay(skewness)\n</pre> # Feature skewness skewness = train[numerical_features].skew().sort_values(ascending=False) print(\"Feature Skewness:\") display(skewness) <pre>Feature Skewness:\n</pre> <pre>previous    13.749885\nbalance     12.304123\ncampaign     4.810437\npdays        3.625049\nduration     2.048776\nage          0.586137\nday          0.054014\ndtype: float64</pre> In\u00a0[152]: Copied! <pre>fig, axes = plt.subplots(3, 3, figsize=(15, 12))\naxes = axes.flatten()\n\nfor i, col in enumerate(numerical_features):\n    data = train[col].dropna()\n    # histogram with density overlay (optional)\n    axes[i].hist(data, bins=50, color=\"#d1495b\", edgecolor=\"black\", alpha=0.7)\n    axes[i].set_title(f\"{col} (mean={data.mean():.1f})\", fontsize=10)\n    axes[i].grid(alpha=0.3)\n    if data.max() &gt; 5000:\n        axes[i].set_xscale(\"log\")\n    axes[i].set_xlabel(\"\")\n    axes[i].set_ylabel(\"\")\n\n# hide any unused subplots\nfor j in range(i + 1, len(axes)):\n    axes[j].axis(\"off\")\n\nfig.suptitle(\"Numerical Features Distribution\", fontsize=16, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(3, 3, figsize=(15, 12)) axes = axes.flatten()  for i, col in enumerate(numerical_features):     data = train[col].dropna()     # histogram with density overlay (optional)     axes[i].hist(data, bins=50, color=\"#d1495b\", edgecolor=\"black\", alpha=0.7)     axes[i].set_title(f\"{col} (mean={data.mean():.1f})\", fontsize=10)     axes[i].grid(alpha=0.3)     if data.max() &gt; 5000:         axes[i].set_xscale(\"log\")     axes[i].set_xlabel(\"\")     axes[i].set_ylabel(\"\")  # hide any unused subplots for j in range(i + 1, len(axes)):     axes[j].axis(\"off\")  fig.suptitle(\"Numerical Features Distribution\", fontsize=16, fontweight=\"bold\", y=1.02) plt.tight_layout() plt.show() In\u00a0[153]: Copied! <pre># Correlation matrix\nplt.figure(figsize=(10, 8))\ncorrelation = train[numerical_features + [target_col]].corr()\nsns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True)\nplt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMost correlated features with target:\")\nprint(correlation[target_col].sort_values(ascending=False))\n</pre> # Correlation matrix plt.figure(figsize=(10, 8)) correlation = train[numerical_features + [target_col]].corr() sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True) plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold') plt.tight_layout() plt.show()  print(\"\\nMost correlated features with target:\") print(correlation[target_col].sort_values(ascending=False)) <pre>\nMost correlated features with target:\ny           1.000000\nduration    0.519283\nbalance     0.122513\nprevious    0.119552\npdays       0.089277\nage         0.009523\nday        -0.049625\ncampaign   -0.075829\nName: y, dtype: float64\n</pre> In\u00a0[154]: Copied! <pre>detailed = []\nfor c in categorical_features:\n    vc = train[c].value_counts(dropna=False)\n    top1 = f\"{vc.index[0]}\"\n    detailed.append({\n        'feature': c,\n        'n_unique': train[c].nunique(dropna=False),\n        'most_freq': top1,\n        'total_rows': len(train)\n    })\ncat_summary_full = pd.DataFrame(detailed).set_index('feature')\ndisplay(cat_summary_full)\n</pre> detailed = [] for c in categorical_features:     vc = train[c].value_counts(dropna=False)     top1 = f\"{vc.index[0]}\"     detailed.append({         'feature': c,         'n_unique': train[c].nunique(dropna=False),         'most_freq': top1,         'total_rows': len(train)     }) cat_summary_full = pd.DataFrame(detailed).set_index('feature') display(cat_summary_full) n_unique most_freq total_rows feature job 12 management 750000 marital 3 married 750000 education 4 secondary 750000 default 2 no 750000 housing 2 yes 750000 loan 2 no 750000 contact 3 cellular 750000 month 12 may 750000 poutcome 4 unknown 750000 In\u00a0[155]: Copied! <pre>cat_cols = categorical_features\nn_features = len(cat_cols)\n\n# Choose subplot grid layout (3 columns works well)\ncols = 3\nrows = math.ceil(n_features / cols)\n\nfig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 4))\naxes = axes.flatten()\n\nfor i, c in enumerate(cat_cols):\n    ax = axes[i]\n    vals = train[c].value_counts(dropna=False)\n    categories = vals.index.astype(str).tolist()\n    counts = vals.values\n\n    # Compute target rate per category (align with counts)\n    rates = train.groupby(c)[target_col].mean().reindex(vals.index).fillna(0).values\n\n    # Plot count bars\n    ax.bar(range(len(categories)), counts, alpha=0.8, color=\"#d1495b\", label=\"count\")\n\n    ax.set_xticks(range(len(categories)))\n    ax.set_xticklabels(categories, rotation=90, fontsize=7)\n    ax.set_ylabel(\"count\")\n    ax.set_title(f'{c} (n={len(categories)})', fontsize=10)\n    ax.grid(alpha=0.2)\n    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\n# Hide unused subplots if any\nfor j in range(i + 1, len(axes)):\n    axes[j].axis(\"off\")\n\nfig.suptitle(\"Categorical Feature Distributions\", fontsize=16, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()\n</pre> cat_cols = categorical_features n_features = len(cat_cols)  # Choose subplot grid layout (3 columns works well) cols = 3 rows = math.ceil(n_features / cols)  fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 4)) axes = axes.flatten()  for i, c in enumerate(cat_cols):     ax = axes[i]     vals = train[c].value_counts(dropna=False)     categories = vals.index.astype(str).tolist()     counts = vals.values      # Compute target rate per category (align with counts)     rates = train.groupby(c)[target_col].mean().reindex(vals.index).fillna(0).values      # Plot count bars     ax.bar(range(len(categories)), counts, alpha=0.8, color=\"#d1495b\", label=\"count\")      ax.set_xticks(range(len(categories)))     ax.set_xticklabels(categories, rotation=90, fontsize=7)     ax.set_ylabel(\"count\")     ax.set_title(f'{c} (n={len(categories)})', fontsize=10)     ax.grid(alpha=0.2)     ax.yaxis.set_major_locator(MaxNLocator(integer=True))  # Hide unused subplots if any for j in range(i + 1, len(axes)):     axes[j].axis(\"off\")  fig.suptitle(\"Categorical Feature Distributions\", fontsize=16, fontweight=\"bold\", y=1.02) plt.tight_layout() plt.show() In\u00a0[156]: Copied! <pre>plt.figure(figsize=(5,4))\nsns.barplot(x=train[target_col].value_counts().index, \n            y=train[target_col].value_counts().values, palette='pastel')\nplt.title('Target Variable Distribution')\nplt.xlabel('Subscribed (y)')\nplt.ylabel('Count')\nplt.xticks([0, 1], ['No (0)', 'Yes (1)'])\nplt.grid(alpha=0.2)\nplt.show()\n</pre> plt.figure(figsize=(5,4)) sns.barplot(x=train[target_col].value_counts().index,              y=train[target_col].value_counts().values, palette='pastel') plt.title('Target Variable Distribution') plt.xlabel('Subscribed (y)') plt.ylabel('Count') plt.xticks([0, 1], ['No (0)', 'Yes (1)']) plt.grid(alpha=0.2) plt.show() In\u00a0[157]: Copied! <pre># Cap outliers (1st/99th percentile)\ntrain_work = train.copy()\n\nfor col in numerical_features:\n    lower, upper = np.percentile(train_work[col], [1, 99])\n    train_work[col] = np.clip(train_work[col], lower, upper)\n</pre> # Cap outliers (1st/99th percentile) train_work = train.copy()  for col in numerical_features:     lower, upper = np.percentile(train_work[col], [1, 99])     train_work[col] = np.clip(train_work[col], lower, upper) In\u00a0[158]: Copied! <pre># Handle skewness with log1p\n# Replace -1 with 0 for pdays (no previous contact)\nif \"pdays\" in train_work.columns:\n    train_work[\"pdays\"] = train_work[\"pdays\"].replace(-1, 0)\n\n# Automatically detect skewed columns (|skew| \u2265 1)\nskewness = train_work[numerical_features].skew()\nskewed_cols = skewness[skewness.abs() &gt;= 1.0].index.tolist()\ntrain_work[skewed_cols] = np.log1p(train_work[skewed_cols].clip(lower=0))\n\nprint(f\"Highly skewed features transformed: {skewed_cols}\")\n</pre> # Handle skewness with log1p # Replace -1 with 0 for pdays (no previous contact) if \"pdays\" in train_work.columns:     train_work[\"pdays\"] = train_work[\"pdays\"].replace(-1, 0)  # Automatically detect skewed columns (|skew| \u2265 1) skewness = train_work[numerical_features].skew() skewed_cols = skewness[skewness.abs() &gt;= 1.0].index.tolist() train_work[skewed_cols] = np.log1p(train_work[skewed_cols].clip(lower=0))  print(f\"Highly skewed features transformed: {skewed_cols}\") <pre>Highly skewed features transformed: ['balance', 'duration', 'campaign', 'pdays', 'previous']\n</pre> In\u00a0[159]: Copied! <pre># Define features and target\nX_full = train_work.drop(columns=[target_col, id_col])\ny_full = train_work[target_col].values\n</pre> # Define features and target X_full = train_work.drop(columns=[target_col, id_col]) y_full = train_work[target_col].values In\u00a0[160]: Copied! <pre># Split into train / val / test\n# First split train+val vs test\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X_full, y_full, test_size=0.15, stratify=y_full, random_state=SEED\n)\n# Then split train vs val (from the remaining 85%)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.1765, stratify=y_temp, random_state=SEED\n)  # 0.1765 * 0.85 \u2248 0.15 \u2192 70/15/15 overall\n\nprint(\"Final splits:\")\nprint(f\" Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\nprint(\" Class balance (train):\", np.bincount(y_train))\nprint(\" Class balance (val):  \", np.bincount(y_val))\nprint(\" Class balance (test): \", np.bincount(y_test))\n</pre> # Split into train / val / test # First split train+val vs test X_temp, X_test, y_temp, y_test = train_test_split(     X_full, y_full, test_size=0.15, stratify=y_full, random_state=SEED ) # Then split train vs val (from the remaining 85%) X_train, X_val, y_train, y_val = train_test_split(     X_temp, y_temp, test_size=0.1765, stratify=y_temp, random_state=SEED )  # 0.1765 * 0.85 \u2248 0.15 \u2192 70/15/15 overall  print(\"Final splits:\") print(f\" Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\") print(\" Class balance (train):\", np.bincount(y_train)) print(\" Class balance (val):  \", np.bincount(y_val)) print(\" Class balance (test): \", np.bincount(y_test)) <pre>Final splits:\n Train: (524981, 16), Val: (112519, 16), Test: (112500, 16)\n Class balance (train): [461642  63339]\n Class balance (val):   [98943 13576]\n Class balance (test):  [98927 13573]\n</pre> In\u00a0[161]: Copied! <pre># Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numerical_features),\n        (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=True, handle_unknown=\"ignore\"), categorical_features),\n    ],\n    remainder=\"drop\",\n)\n\n# Fit only on training data\npreprocessor.fit(X_train)\n\n# Transform splits\nX_train_proc = preprocessor.transform(X_train)\nX_val_proc   = preprocessor.transform(X_val)\nX_test_proc  = preprocessor.transform(X_test)\n\nprint(\"\\nProcessed feature dimensions:\")\nprint(f\" Train: {X_train_proc.shape}\")\nprint(f\" Val:   {X_val_proc.shape}\")\nprint(f\" Test:  {X_test_proc.shape}\")\n</pre> # Preprocessing pipeline preprocessor = ColumnTransformer(     transformers=[         (\"num\", StandardScaler(), numerical_features),         (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=True, handle_unknown=\"ignore\"), categorical_features),     ],     remainder=\"drop\", )  # Fit only on training data preprocessor.fit(X_train)  # Transform splits X_train_proc = preprocessor.transform(X_train) X_val_proc   = preprocessor.transform(X_val) X_test_proc  = preprocessor.transform(X_test)  print(\"\\nProcessed feature dimensions:\") print(f\" Train: {X_train_proc.shape}\") print(f\" Val:   {X_val_proc.shape}\") print(f\" Test:  {X_test_proc.shape}\") <pre>\nProcessed feature dimensions:\n Train: (524981, 42)\n Val:   (112519, 42)\n Test:  (112500, 42)\n</pre> In\u00a0[162]: Copied! <pre># Class weights (for imbalance)\ncw = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\nclass_weight = {0: cw[0], 1: cw[1]}\nprint(\"\\nClass weights:\", class_weight)\n</pre> # Class weights (for imbalance) cw = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train) class_weight = {0: cw[0], 1: cw[1]} print(\"\\nClass weights:\", class_weight) <pre>\nClass weights: {0: np.float64(0.5686018603160025), 1: np.float64(4.144216043827657)}\n</pre> In\u00a0[163]: Copied! <pre>class MLP:\n    \"\"\"\n    Multi-Layer Perceptron (NumPy) for binary classification.\n    Architecture: Input \u2192 Hidden Layer(s) \u2192 Output\n    Activation: ReLU (hidden), Sigmoid (output)\n    Loss: Binary Cross-Entropy with optional L2 regularization\n    Optimizer: SGD (parameter updates handled externally)\n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_sizes=[128, 64, 32], l2_lambda=1e-4, random_state=42):\n        np.random.seed(random_state)\n        self.l2_lambda = l2_lambda\n        \n        # Define layer sizes: input, hidden(s), output\n        layer_sizes = [input_dim] + hidden_sizes + [1]\n        self.num_layers = len(layer_sizes) - 1\n        \n        # Initialize parameters with He initialization (good for ReLU)\n        self.weights = []\n        self.biases = []\n        for i in range(self.num_layers):\n            n_in, n_out = layer_sizes[i], layer_sizes[i+1]\n            std = np.sqrt(2.0 / n_in)\n            self.weights.append(np.random.randn(n_in, n_out) * std)\n            self.biases.append(np.zeros((1, n_out)))\n\n    # Activation functions\n    def relu(self, x):\n        \"\"\"ReLU activation\"\"\"\n        return np.maximum(0, x)\n\n    def relu_derivative(self, x):\n        \"\"\"Derivative of ReLU\"\"\"\n        return (x &gt; 0).astype(float)\n\n    def sigmoid(self, x):\n        \"\"\"Numerically stable sigmoid\"\"\"\n        out = np.empty_like(x)\n        pos = x &gt;= 0\n        out[pos] = 1 / (1 + np.exp(-x[pos]))\n        neg = ~pos\n        exp_x = np.exp(x[neg])\n        out[neg] = exp_x / (1 + exp_x)\n        return out\n\n    # Forward propagation\n    def forward(self, X):\n        \"\"\"\n        Forward pass through all layers.\n        Returns:\n            y_pred: predicted probabilities\n            cache: activations for backpropagation\n        \"\"\"\n        cache = {'A0': X}\n        A = X\n        for i in range(self.num_layers - 1):\n            Z = A @ self.weights[i] + self.biases[i]\n            A = self.relu(Z)\n            cache[f'Z{i+1}'] = Z\n            cache[f'A{i+1}'] = A\n        Z_out = A @ self.weights[-1] + self.biases[-1]\n        A_out = self.sigmoid(Z_out)\n        cache[f'Z{self.num_layers}'] = Z_out\n        cache[f'A{self.num_layers}'] = A_out\n        return A_out, cache\n\n    # Loss computation\n    def compute_loss(self, y_true, y_pred):\n        \"\"\"\n        Binary Cross-Entropy + optional L2 penalty.\n        \"\"\"\n        m = y_true.shape[0]\n        y_true = y_true.reshape(-1, 1)\n        y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n        bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        l2 = sum(np.sum(W**2) for W in self.weights) * (self.l2_lambda / (2 * m))\n        return bce + l2\n\n    # Backpropagation\n    def backward(self, cache, y_true):\n        \"\"\"\n        Compute gradients for all parameters using backpropagation.\n        \"\"\"\n        y_true = y_true.reshape(-1, 1)\n        m = y_true.shape[0]\n        grads_w, grads_b = [], []\n\n        # Gradient for output layer\n        y_pred = cache[f'A{self.num_layers}']\n        dZ = y_pred - y_true\n\n        for i in range(self.num_layers - 1, -1, -1):\n            A_prev = cache[f'A{i}']\n            dW = (A_prev.T @ dZ) / m + (self.l2_lambda / m) * self.weights[i]\n            db = np.mean(dZ, axis=0, keepdims=True)\n            grads_w.insert(0, dW)\n            grads_b.insert(0, db)\n\n            if i &gt; 0:\n                dA = dZ @ self.weights[i].T\n                dZ = dA * self.relu_derivative(cache[f'Z{i}'])\n\n        return grads_w, grads_b\n\n    # Parameter update\n    def update_parameters(self, grads_w, grads_b, learning_rate):\n        \"\"\"Apply gradient descent step.\"\"\"\n        for i in range(self.num_layers):\n            self.weights[i] -= learning_rate * grads_w[i]\n            self.biases[i]  -= learning_rate * grads_b[i]\n\n        # Prediction helpers\n    def predict_proba(self, X):\n        \"\"\"\n        Compute output probabilities for given inputs.\n        \"\"\"\n        y_pred, _ = self.forward(X)\n        return y_pred.reshape(-1)\n\n    def predict(self, X, threshold=0.5):\n        \"\"\"\n        Predict binary class labels (0/1).\n        \"\"\"\n        return (self.predict_proba(X) &gt;= threshold).astype(int)\n\n\nprint(\"MLP class implemented successfully.\")\n</pre> class MLP:     \"\"\"     Multi-Layer Perceptron (NumPy) for binary classification.     Architecture: Input \u2192 Hidden Layer(s) \u2192 Output     Activation: ReLU (hidden), Sigmoid (output)     Loss: Binary Cross-Entropy with optional L2 regularization     Optimizer: SGD (parameter updates handled externally)     \"\"\"          def __init__(self, input_dim, hidden_sizes=[128, 64, 32], l2_lambda=1e-4, random_state=42):         np.random.seed(random_state)         self.l2_lambda = l2_lambda                  # Define layer sizes: input, hidden(s), output         layer_sizes = [input_dim] + hidden_sizes + [1]         self.num_layers = len(layer_sizes) - 1                  # Initialize parameters with He initialization (good for ReLU)         self.weights = []         self.biases = []         for i in range(self.num_layers):             n_in, n_out = layer_sizes[i], layer_sizes[i+1]             std = np.sqrt(2.0 / n_in)             self.weights.append(np.random.randn(n_in, n_out) * std)             self.biases.append(np.zeros((1, n_out)))      # Activation functions     def relu(self, x):         \"\"\"ReLU activation\"\"\"         return np.maximum(0, x)      def relu_derivative(self, x):         \"\"\"Derivative of ReLU\"\"\"         return (x &gt; 0).astype(float)      def sigmoid(self, x):         \"\"\"Numerically stable sigmoid\"\"\"         out = np.empty_like(x)         pos = x &gt;= 0         out[pos] = 1 / (1 + np.exp(-x[pos]))         neg = ~pos         exp_x = np.exp(x[neg])         out[neg] = exp_x / (1 + exp_x)         return out      # Forward propagation     def forward(self, X):         \"\"\"         Forward pass through all layers.         Returns:             y_pred: predicted probabilities             cache: activations for backpropagation         \"\"\"         cache = {'A0': X}         A = X         for i in range(self.num_layers - 1):             Z = A @ self.weights[i] + self.biases[i]             A = self.relu(Z)             cache[f'Z{i+1}'] = Z             cache[f'A{i+1}'] = A         Z_out = A @ self.weights[-1] + self.biases[-1]         A_out = self.sigmoid(Z_out)         cache[f'Z{self.num_layers}'] = Z_out         cache[f'A{self.num_layers}'] = A_out         return A_out, cache      # Loss computation     def compute_loss(self, y_true, y_pred):         \"\"\"         Binary Cross-Entropy + optional L2 penalty.         \"\"\"         m = y_true.shape[0]         y_true = y_true.reshape(-1, 1)         y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)         bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))         l2 = sum(np.sum(W**2) for W in self.weights) * (self.l2_lambda / (2 * m))         return bce + l2      # Backpropagation     def backward(self, cache, y_true):         \"\"\"         Compute gradients for all parameters using backpropagation.         \"\"\"         y_true = y_true.reshape(-1, 1)         m = y_true.shape[0]         grads_w, grads_b = [], []          # Gradient for output layer         y_pred = cache[f'A{self.num_layers}']         dZ = y_pred - y_true          for i in range(self.num_layers - 1, -1, -1):             A_prev = cache[f'A{i}']             dW = (A_prev.T @ dZ) / m + (self.l2_lambda / m) * self.weights[i]             db = np.mean(dZ, axis=0, keepdims=True)             grads_w.insert(0, dW)             grads_b.insert(0, db)              if i &gt; 0:                 dA = dZ @ self.weights[i].T                 dZ = dA * self.relu_derivative(cache[f'Z{i}'])          return grads_w, grads_b      # Parameter update     def update_parameters(self, grads_w, grads_b, learning_rate):         \"\"\"Apply gradient descent step.\"\"\"         for i in range(self.num_layers):             self.weights[i] -= learning_rate * grads_w[i]             self.biases[i]  -= learning_rate * grads_b[i]          # Prediction helpers     def predict_proba(self, X):         \"\"\"         Compute output probabilities for given inputs.         \"\"\"         y_pred, _ = self.forward(X)         return y_pred.reshape(-1)      def predict(self, X, threshold=0.5):         \"\"\"         Predict binary class labels (0/1).         \"\"\"         return (self.predict_proba(X) &gt;= threshold).astype(int)   print(\"MLP class implemented successfully.\") <pre>MLP class implemented successfully.\n</pre> In\u00a0[\u00a0]: Copied! <pre>def train_mlp(model, X_train, y_train, X_val, y_val,\n              epochs=50, batch_size=512, learning_rate=1e-2,\n              early_stopping=5, verbose=True):\n    \"\"\"\n    Mini-batch SGD training loop that handles pandas DataFrames and sparse matrices.\n    \"\"\"\n    # Normalize input types: convert pandas DataFrame -&gt; numpy array; sparse -&gt; dense if needed\n    if isinstance(X_train, pd.DataFrame):\n        X_train = X_train.values\n    if isinstance(X_val, pd.DataFrame):\n        X_val = X_val.values\n\n    if hasattr(X_train, \"toarray\"):\n        X_train = X_train.toarray()\n    if hasattr(X_val, \"toarray\"):\n        X_val = X_val.toarray()\n\n    y_train = np.asarray(y_train).reshape(-1, 1)\n    y_val   = np.asarray(y_val).reshape(-1, 1)\n\n    n = X_train.shape[0]\n    history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_ap': []}\n    best_val_loss = np.inf\n    patience = 0\n    best_weights = None\n\n    for epoch in range(1, epochs + 1):\n        # Shuffle training data (use permutation index and apply to numpy arrays)\n        idx = np.random.permutation(n)\n        X_shuf = X_train[idx]\n        y_shuf = y_train[idx]\n\n        epoch_loss = 0.0\n\n        # Mini-batch training\n        for start in range(0, n, batch_size):\n            end = min(start + batch_size, n)\n            Xb = X_shuf[start:end]\n            yb = y_shuf[start:end]\n\n            y_pred, cache = model.forward(Xb)\n            loss = model.compute_loss(yb, y_pred)\n            epoch_loss += loss * (end - start)\n\n            grads_w, grads_b = model.backward(cache, yb)\n            model.update_parameters(grads_w, grads_b, learning_rate)\n\n        epoch_loss /= n\n        history['train_loss'].append(epoch_loss)\n\n        # Validation\n        y_val_pred, _ = model.forward(X_val)\n        val_loss = model.compute_loss(y_val, y_val_pred)\n        try:\n            val_auc = roc_auc_score(y_val.ravel(), y_val_pred.ravel())\n            val_ap  = average_precision_score(y_val.ravel(), y_val_pred.ravel())\n        except Exception:\n            val_auc = np.nan\n            val_ap  = np.nan\n\n        history['val_loss'].append(val_loss)\n        history['val_auc'].append(val_auc)\n        history['val_ap'].append(val_ap)\n\n        if verbose:\n            print(f\"Epoch {epoch:03d} | train_loss={epoch_loss:.5f} | val_loss={val_loss:.5f} \"\n                  f\"| val_auc={val_auc:.4f} | val_ap={val_ap:.4f}\")\n\n        # Early stopping\n        if val_loss &lt; best_val_loss - 1e-6:\n            best_val_loss = val_loss\n            best_weights = ([W.copy() for W in model.weights],\n                            [b.copy() for b in model.biases])\n            patience = 0\n        else:\n            patience += 1\n        if patience &gt;= early_stopping:\n            if verbose: print(\"Early stopping triggered.\")\n            break\n\n    if best_weights is not None:\n        model.weights, model.biases = best_weights\n\n    return history\n</pre> def train_mlp(model, X_train, y_train, X_val, y_val,               epochs=50, batch_size=512, learning_rate=1e-2,               early_stopping=5, verbose=True):     \"\"\"     Mini-batch SGD training loop that handles pandas DataFrames and sparse matrices.     \"\"\"     # Normalize input types: convert pandas DataFrame -&gt; numpy array; sparse -&gt; dense if needed     if isinstance(X_train, pd.DataFrame):         X_train = X_train.values     if isinstance(X_val, pd.DataFrame):         X_val = X_val.values      if hasattr(X_train, \"toarray\"):         X_train = X_train.toarray()     if hasattr(X_val, \"toarray\"):         X_val = X_val.toarray()      y_train = np.asarray(y_train).reshape(-1, 1)     y_val   = np.asarray(y_val).reshape(-1, 1)      n = X_train.shape[0]     history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_ap': []}     best_val_loss = np.inf     patience = 0     best_weights = None      for epoch in range(1, epochs + 1):         # Shuffle training data (use permutation index and apply to numpy arrays)         idx = np.random.permutation(n)         X_shuf = X_train[idx]         y_shuf = y_train[idx]          epoch_loss = 0.0          # Mini-batch training         for start in range(0, n, batch_size):             end = min(start + batch_size, n)             Xb = X_shuf[start:end]             yb = y_shuf[start:end]              y_pred, cache = model.forward(Xb)             loss = model.compute_loss(yb, y_pred)             epoch_loss += loss * (end - start)              grads_w, grads_b = model.backward(cache, yb)             model.update_parameters(grads_w, grads_b, learning_rate)          epoch_loss /= n         history['train_loss'].append(epoch_loss)          # Validation         y_val_pred, _ = model.forward(X_val)         val_loss = model.compute_loss(y_val, y_val_pred)         try:             val_auc = roc_auc_score(y_val.ravel(), y_val_pred.ravel())             val_ap  = average_precision_score(y_val.ravel(), y_val_pred.ravel())         except Exception:             val_auc = np.nan             val_ap  = np.nan          history['val_loss'].append(val_loss)         history['val_auc'].append(val_auc)         history['val_ap'].append(val_ap)          if verbose:             print(f\"Epoch {epoch:03d} | train_loss={epoch_loss:.5f} | val_loss={val_loss:.5f} \"                   f\"| val_auc={val_auc:.4f} | val_ap={val_ap:.4f}\")          # Early stopping         if val_loss &lt; best_val_loss - 1e-6:             best_val_loss = val_loss             best_weights = ([W.copy() for W in model.weights],                             [b.copy() for b in model.biases])             patience = 0         else:             patience += 1         if patience &gt;= early_stopping:             if verbose: print(\"Early stopping triggered.\")             break      if best_weights is not None:         model.weights, model.biases = best_weights      return history In\u00a0[165]: Copied! <pre># Initialize and train MLP\nmlp_model = MLP(input_dim=X_train_proc.shape[1], hidden_sizes=[128, 64, 32], l2_lambda=1e-5)\nhistory = train_mlp(mlp_model, X_train_proc, y_train, X_val_proc, y_val,\n                    epochs=100, batch_size=256, learning_rate=1e-2,\n                    early_stopping=7, verbose=False)\n</pre> # Initialize and train MLP mlp_model = MLP(input_dim=X_train_proc.shape[1], hidden_sizes=[128, 64, 32], l2_lambda=1e-5) history = train_mlp(mlp_model, X_train_proc, y_train, X_val_proc, y_val,                     epochs=100, batch_size=256, learning_rate=1e-2,                     early_stopping=7, verbose=False) In\u00a0[166]: Copied! <pre># Plot training history\nepochs = np.arange(1, len(history['train_loss']) + 1)\n\nplt.figure(figsize=(12,4))\n\n# Loss plot\nplt.subplot(1,2,1)\nplt.plot(epochs, history['train_loss'], label='train_loss')\nplt.plot(epochs, history['val_loss'],   label='val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss vs Epochs')\nplt.legend()\nplt.grid(alpha=0.2)\n\n# Validation metrics plot (AUC and Average Precision)\nplt.subplot(1,2,2)\nif 'val_auc' in history and any(~np.isnan(history['val_auc'])):\n    plt.plot(epochs, history['val_auc'], label='val_ROC_AUC')\nif 'val_ap' in history and any(~np.isnan(history['val_ap'])):\n    plt.plot(epochs, history['val_ap'], label='val_PR_AUC')\nplt.xlabel('Epoch')\nplt.ylabel('Score')\nplt.ylim(0.0, 1.0)\nplt.title('Validation AUC / PR-AUC vs Epochs')\nplt.legend()\nplt.grid(alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n</pre> # Plot training history epochs = np.arange(1, len(history['train_loss']) + 1)  plt.figure(figsize=(12,4))  # Loss plot plt.subplot(1,2,1) plt.plot(epochs, history['train_loss'], label='train_loss') plt.plot(epochs, history['val_loss'],   label='val_loss') plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('Loss vs Epochs') plt.legend() plt.grid(alpha=0.2)  # Validation metrics plot (AUC and Average Precision) plt.subplot(1,2,2) if 'val_auc' in history and any(~np.isnan(history['val_auc'])):     plt.plot(epochs, history['val_auc'], label='val_ROC_AUC') if 'val_ap' in history and any(~np.isnan(history['val_ap'])):     plt.plot(epochs, history['val_ap'], label='val_PR_AUC') plt.xlabel('Epoch') plt.ylabel('Score') plt.ylim(0.0, 1.0) plt.title('Validation AUC / PR-AUC vs Epochs') plt.legend() plt.grid(alpha=0.2)  plt.tight_layout() plt.show()  <p>Analysis of Training Curves</p> <p>Both training and validation losses decrease steadily and plateau after ~80 epochs, indicating smooth convergence without instability. The small gap between the two curves suggests minimal overfitting.</p> <p>Validation ROC-AUC remains consistently high (~0.96) throughout training, while PR-AUC improves gradually before stabilizing\u2014showing the model learns to balance precision and recall effectively. Overall, the curves demonstrate a well-behaved optimization process and good generalization to unseen data.</p> In\u00a0[167]: Copied! <pre># Convert test data to dense if sparse\nX_test_eval = X_test_proc.toarray() if hasattr(X_test_proc, \"toarray\") else X_test_proc\n\n# Predict probabilities and labels\ny_test_probs = mlp_model.predict_proba(X_test_eval)\ny_test_pred = (y_test_probs &gt;= 0.5).astype(int)\n\n# Metrics\nacc  = accuracy_score(y_test, y_test_pred)\nprec = precision_score(y_test, y_test_pred, zero_division=0)\nrec  = recall_score(y_test, y_test_pred, zero_division=0)\nf1   = f1_score(y_test, y_test_pred, zero_division=0)\nroc  = roc_auc_score(y_test, y_test_probs)\nap   = average_precision_score(y_test, y_test_probs)\n\nprint(\"=== Test Set Performance ===\")\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall: {rec:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\nprint(f\"ROC-AUC: {roc:.4f}\")\nprint(f\"PR-AUC (AP): {ap:.4f}\")\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_test_pred)\nplt.figure(figsize=(5,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix (Test Set)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_test, y_test_probs)\nplt.figure(figsize=(5,4))\nplt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc:.3f})')\nplt.plot([0,1],[0,1],'--',color='gray')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.grid(alpha=0.2)\nplt.show()\n\n# Precision-Recall Curve\nprec_curve, rec_curve, _ = precision_recall_curve(y_test, y_test_probs)\nplt.figure(figsize=(5,4))\nplt.plot(rec_curve, prec_curve, label=f'PR curve (AP = {ap:.3f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.grid(alpha=0.2)\nplt.show()\n</pre> # Convert test data to dense if sparse X_test_eval = X_test_proc.toarray() if hasattr(X_test_proc, \"toarray\") else X_test_proc  # Predict probabilities and labels y_test_probs = mlp_model.predict_proba(X_test_eval) y_test_pred = (y_test_probs &gt;= 0.5).astype(int)  # Metrics acc  = accuracy_score(y_test, y_test_pred) prec = precision_score(y_test, y_test_pred, zero_division=0) rec  = recall_score(y_test, y_test_pred, zero_division=0) f1   = f1_score(y_test, y_test_pred, zero_division=0) roc  = roc_auc_score(y_test, y_test_probs) ap   = average_precision_score(y_test, y_test_probs)  print(\"=== Test Set Performance ===\") print(f\"Accuracy: {acc:.4f}\") print(f\"Precision: {prec:.4f}\") print(f\"Recall: {rec:.4f}\") print(f\"F1-score: {f1:.4f}\") print(f\"ROC-AUC: {roc:.4f}\") print(f\"PR-AUC (AP): {ap:.4f}\")  # Confusion matrix cm = confusion_matrix(y_test, y_test_pred) plt.figure(figsize=(5,4)) sns.heatmap(cm, annot=True, fmt='d', cmap='Blues') plt.title(\"Confusion Matrix (Test Set)\") plt.xlabel(\"Predicted\") plt.ylabel(\"Actual\") plt.show()  # ROC Curve fpr, tpr, _ = roc_curve(y_test, y_test_probs) plt.figure(figsize=(5,4)) plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc:.3f})') plt.plot([0,1],[0,1],'--',color='gray') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('ROC Curve') plt.legend() plt.grid(alpha=0.2) plt.show()  # Precision-Recall Curve prec_curve, rec_curve, _ = precision_recall_curve(y_test, y_test_probs) plt.figure(figsize=(5,4)) plt.plot(rec_curve, prec_curve, label=f'PR curve (AP = {ap:.3f})') plt.xlabel('Recall') plt.ylabel('Precision') plt.title('Precision-Recall Curve') plt.legend() plt.grid(alpha=0.2) plt.show() <pre>=== Test Set Performance ===\nAccuracy: 0.9292\nPrecision: 0.7276\nRecall: 0.6601\nF1-score: 0.6922\nROC-AUC: 0.9606\nPR-AUC (AP): 0.7663\n</pre> <p>On the test set, the MLP achieved 92.92% accuracy, 0.7276 precision, 0.6601 recall, and 0.6922 F1-score, with a strong ROC-AUC = 0.9606 and PR-AUC = 0.7663. These results confirm that the network discriminates well between classes while maintaining reasonable balance between false positives and false negatives. The high ROC-AUC indicates excellent overall separability, while the moderate PR-AUC reflects the challenge of the class imbalance. Overall, the model generalizes effectively to unseen data without significant overfitting, demonstrating a robust fit to this binary classification task.</p> In\u00a0[168]: Copied! <pre># Prepare test set for submission\n# Apply the EXACT same preprocessing as training data\ntest_work = test.copy()\n\n# Cap outliers (1st/99th percentile)\nfor col in numerical_features:\n    lower, upper = np.percentile(train[col], [1, 99])\n    test_work[col] = np.clip(test_work[col], lower, upper)\n\n# Handle pdays special case (replace -1 with 0)\nif \"pdays\" in test_work.columns:\n    test_work[\"pdays\"] = test_work[\"pdays\"].replace(-1, 0)\n\n# Apply log transform to skewed features\ntest_work[skewed_cols] = np.log1p(test_work[skewed_cols].clip(lower=0))\n\n# Apply the fitted preprocessor (StandardScaler + OneHotEncoder)\ntest_work_proc = preprocessor.transform(test_work)\n\n# Convert to dense if sparse\ntest_work_proc = test_work_proc.toarray() if hasattr(test_work_proc, \"toarray\") else test_work_proc\n\n# Predict probabilities\ntest_probs = mlp_model.predict_proba(test_work_proc)\n\n# Prepare submission\nsubmission = pd.DataFrame({\n    'id': test[id_col],\n    'y': test_probs\n})\nsubmission.to_csv(\"mlp_submission.csv\", index=False)\n</pre> # Prepare test set for submission # Apply the EXACT same preprocessing as training data test_work = test.copy()  # Cap outliers (1st/99th percentile) for col in numerical_features:     lower, upper = np.percentile(train[col], [1, 99])     test_work[col] = np.clip(test_work[col], lower, upper)  # Handle pdays special case (replace -1 with 0) if \"pdays\" in test_work.columns:     test_work[\"pdays\"] = test_work[\"pdays\"].replace(-1, 0)  # Apply log transform to skewed features test_work[skewed_cols] = np.log1p(test_work[skewed_cols].clip(lower=0))  # Apply the fitted preprocessor (StandardScaler + OneHotEncoder) test_work_proc = preprocessor.transform(test_work)  # Convert to dense if sparse test_work_proc = test_work_proc.toarray() if hasattr(test_work_proc, \"toarray\") else test_work_proc  # Predict probabilities test_probs = mlp_model.predict_proba(test_work_proc)  # Prepare submission submission = pd.DataFrame({     'id': test[id_col],     'y': test_probs }) submission.to_csv(\"mlp_submission.csv\", index=False) In\u00a0[169]: Copied! <pre># Submitting to Kaggle\n\n!kaggle competitions submit -c playground-series-s5e8 -f mlp_submission.csv -m \"MLP model submission\"\n</pre> # Submitting to Kaggle  !kaggle competitions submit -c playground-series-s5e8 -f mlp_submission.csv -m \"MLP model submission\" <pre>Successfully submitted to Binary Classification with a Bank Dataset\n</pre> <pre>\n  0%|          | 0.00/6.93M [00:00&lt;?, ?B/s]\n  0%|          | 16.0k/6.93M [00:00&lt;03:07, 38.8kB/s]\n  8%|\u258a         | 544k/6.93M [00:00&lt;00:06, 994kB/s]  \n 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 4.55M/6.93M [00:00&lt;00:00, 6.72MB/s]\n 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 5.23M/6.93M [00:01&lt;00:00, 3.85MB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.93M/6.93M [00:02&lt;00:00, 3.18MB/s]\n</pre> <p>Kaggle score: 0.96158 (public leaderboard)</p> <ul> <li>Leaderboard position: Within the top 68% of all public submissions.</li> <li>Reason for not reaching top 50%:<ul> <li>The MLP was implemented from scratch using only NumPy, without high-level optimization libraries like PyTorch or TensorFlow.</li> <li>State-of-the-art Kaggle submissions typically rely on ensemble methods (e.g., LightGBM, CatBoost) and extensive feature engineering, giving them a natural advantage on tabular datasets.</li> </ul> </li> </ul> <p></p>"},{"location":"projects/classification/notebook/#multi-layer-perceptron-mlp-classification","title":"Multi-Layer Perceptron (MLP) Classification\u00b6","text":""},{"location":"projects/classification/notebook/#project-overview","title":"Project Overview\u00b6","text":"<p>This notebook implements a Multi-Layer Perceptron (MLP) neural network from scratch for binary classification on a real-world banking dataset.</p> <p>Authors: Rodrigo Medeiros, Matheus Castellucci, Jo\u00e3o Pedro Rodrigues</p>"},{"location":"projects/classification/notebook/#1-dataset-selection","title":"1. Dataset Selection\u00b6","text":""},{"location":"projects/classification/notebook/#dataset-information","title":"Dataset Information\u00b6","text":"<p>Name: Binary Classification with a Bank Dataset Source: Kaggle Playground Series S5E8 Original Dataset: Bank Marketing Dataset</p> <p>Size:</p> <ul> <li>Training set: 750,000 rows \u00d7 18 columns</li> <li>Test set: 250,000 rows \u00d7 17 columns</li> <li>Features: 16 (7 numerical + 9 categorical)</li> <li>Target: Binary (subscription to term deposit: yes/no)</li> </ul> <p>Why this dataset?</p> <ul> <li>Real-world banking application (predicting term deposit subscriptions)</li> <li>Sufficient complexity with mixed feature types</li> <li>Class imbalance - realistic scenario</li> <li>Relevant to marketing and customer behavior prediction</li> </ul>"},{"location":"projects/classification/notebook/#2-dataset-explanation","title":"2. Dataset Explanation\u00b6","text":""},{"location":"projects/classification/notebook/#feature-descriptions","title":"Feature Descriptions\u00b6","text":"<p>Numerical Features (7):</p> <ul> <li><code>age</code>: Client's age</li> <li><code>balance</code>: Average yearly balance (euros)</li> <li><code>day</code>: Last contact day of month</li> <li><code>duration</code>: Last contact duration (seconds)</li> <li><code>campaign</code>: Number of contacts during this campaign</li> <li><code>pdays</code>: Days since last contact from previous campaign (-1 = not contacted)</li> <li><code>previous</code>: Number of contacts before this campaign</li> </ul> <p>Categorical Features (9):</p> <ul> <li><code>job</code>: Type of job</li> <li><code>marital</code>: Marital status</li> <li><code>education</code>: Education level</li> <li><code>default</code>: Has credit in default?</li> <li><code>housing</code>: Has housing loan?</li> <li><code>loan</code>: Has personal loan?</li> <li><code>contact</code>: Contact communication type</li> <li><code>month</code>: Last contact month</li> <li><code>poutcome</code>: Outcome of previous campaign</li> </ul> <p>Target Variable:</p> <ul> <li><code>y</code>: Subscribed to term deposit? (0 = no, 1 = yes)</li> </ul>"},{"location":"projects/classification/notebook/#numerical-feature-summary","title":"Numerical Feature Summary\u00b6","text":""},{"location":"projects/classification/notebook/#categorical-feature-summary","title":"Categorical Feature Summary\u00b6","text":""},{"location":"projects/classification/notebook/#target-variable-distribution","title":"Target Variable Distribution\u00b6","text":""},{"location":"projects/classification/notebook/#potential-issues-identified","title":"Potential Issues Identified\u00b6","text":"<ul> <li><p>Class Imbalance: ~88% negative class, ~12% positive class</p> </li> <li><p>Outliers: Several numerical features have extreme values (detected via histograms)</p> </li> <li><p>Skewed Distributions: Most numerical features are right-skewed</p> </li> <li><p>Mixed Feature Types: Requires encoding for categorical variables</p> </li> <li><p>Missing Values: No missing values detected</p> </li> <li><p>Duplicates: No duplicate rows detected</p> </li> </ul>"},{"location":"projects/classification/notebook/#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization\u00b6","text":""},{"location":"projects/classification/notebook/#preprocessing-justification","title":"Preprocessing Justification\u00b6","text":"<ul> <li><p>Outlier Capping (1st\u201399th Percentile): Limits the influence of extreme values while preserving all rows, avoiding bias and data loss.</p> </li> <li><p>Log Transform (|skew| \u2265 1): Reduces heavy right skew in numerical features for more stable gradient updates.</p> </li> <li><p>StandardScaler: Centers and scales numeric features to improve MLP convergence.</p> </li> <li><p>OneHotEncoder: Encodes categorical variables as binary vectors; drop='first' prevents redundancy and handle_unknown='ignore' ensures consistency on new data.</p> </li> <li><p>Sparse Output: Saves memory with large one-hot matrices, converted to dense only if needed for training.</p> </li> <li><p>Stratified 70/15/15 Split: Preserves class proportions across train, validation, and test sets for unbiased evaluation.</p> </li> <li><p>Class Weights: Counteracts class imbalance (~12% positive) by adjusting loss contributions between classes.</p> </li> </ul>"},{"location":"projects/classification/notebook/#4-mlp-implementation","title":"4. MLP Implementation\u00b6","text":""},{"location":"projects/classification/notebook/#hyperparameters","title":"Hyperparameters\u00b6","text":"<ul> <li><p>Input Layer: matches the number of preprocessed features.</p> </li> <li><p>Hidden Layers: three fully connected layers with 128 and 64 neurons, each using ReLU activation to introduce non-linearity and mitigate vanishing gradients.</p> </li> <li><p>Output Layer: a single neuron with sigmoid activation to output probabilities for the binary target.</p> </li> <li><p>Initialization: weights are initialized with He initialization (N(0, sqrt(2/n_in))) to maintain stable activation variance across layers.</p> </li> <li><p>Loss Function: Binary Cross-Entropy (BCE) with optional L2 regularization, penalizing large weights to improve generalization.</p> </li> <li><p>Optimizer: standard Stochastic Gradient Descent (SGD) is used for parameter updates; gradient descent steps are handled in the training loop.</p> </li> </ul>"},{"location":"projects/classification/notebook/#5-model-training","title":"5. Model Training\u00b6","text":""},{"location":"projects/classification/notebook/#training-procedure","title":"Training Procedure\u00b6","text":"<p>The MLP was trained using mini-batch Stochastic Gradient Descent (SGD) implemented from scratch. Each epoch consists of the following steps:</p> <ol> <li><p>Data Shuffling and Mini-Batches \u2014 At the start of every epoch, the training data are randomly shuffled and divided into batches to improve gradient estimation and generalization.</p> </li> <li><p>Forward Propagation \u2014 For each batch, the model performs matrix multiplications and ReLU activations across layers to compute predicted probabilities.</p> </li> <li><p>Loss Computation \u2014 The Binary Cross-Entropy (BCE) loss is computed between predictions and true labels, with an additional L2 penalty on the weights to discourage overfitting.</p> </li> <li><p>Backpropagation \u2014 Gradients of the BCE + L2 loss with respect to every weight and bias are obtained via the chain rule.</p> <ul> <li>The ReLU derivative (<code>1 if z &gt; 0 else 0</code>) prevents vanishing gradients common in sigmoid/tanh activations.</li> <li>Intermediate activations are stored in a <code>cache</code> dictionary for reuse during gradient computation.</li> </ul> </li> <li><p>Parameter Update \u2014 Each layer\u2019s weights and biases are updated by</p> <p>$$ W \\leftarrow W - \\eta\\,\\nabla_W L,\\qquad b \\leftarrow b - \\eta\\,\\nabla_b L $$</p> <p>where the learning rate controls the step size.</p> </li> <li><p>Validation Evaluation \u2014 After every epoch, the model runs a forward pass on the validation set to track loss, ROC-AUC, and PR-AUC.</p> </li> <li><p>Early Stopping \u2014 If the validation loss fails to improve for a number of epochs, training stops and the best weights (lowest validation loss) are restored.</p> </li> </ol>"},{"location":"projects/classification/notebook/#training-challenges-and-solutions","title":"Training Challenges and Solutions\u00b6","text":"<ul> <li>Vanishing Gradients: Addressed by using ReLU activations and He initialization, which preserve gradient scale across layers.</li> <li>Overfitting: Controlled with L2 regularization and early stopping; validation loss and AUC were monitored each epoch.</li> <li>Instability in Loss: Occasional fluctuations caused by mini-batch noise; mitigated by averaging losses over all batches per epoch.</li> <li>Imbalanced Classes: Since positives represent only \u2248 12 % of the data, performance was evaluated primarily with ROC-AUC and PR-AUC rather than accuracy.</li> </ul>"},{"location":"projects/classification/notebook/#6-training-and-testing-strategy","title":"6. Training and Testing Strategy\u00b6","text":"<p>The dataset was already divided into train / validation / test sets using a 70 / 15 / 15 split. All splits were stratified to preserve the original class distribution (~12 % positive class).</p>"},{"location":"projects/classification/notebook/#validation-role-in-hyperparameter-tuning","title":"Validation role in hyperparameter tuning\u00b6","text":"<ul> <li>The validation set is crucial for evaluating alternative model configurations such as learning rate, hidden layer size, L2 regularization strength, and batch size.</li> <li>The main selection metric is validation ROC-AUC (<code>val_auc</code>), since accuracy is unreliable under class imbalance. PR-AUC serves as a secondary indicator of precision\u2013recall trade-offs.</li> <li>Typical tuning workflow:<ol> <li>Train each candidate model on the training set only.</li> <li>Evaluate on the validation set after every epoch to track learning progress.</li> <li>Select the configuration that yields the highest validation ROC-AUC.</li> <li>Retrain the best model on the combined training + validation data before the final test evaluation.</li> </ol> </li> <li>Early stopping monitors the same validation metric (<code>val_auc</code>) and halts training when no improvement is observed for a fixed number of epochs (patience = 7), avoiding unnecessary computation and overfitting.</li> </ul>"},{"location":"projects/classification/notebook/#7-error-curves-and-visualization","title":"7. Error Curves and Visualization\u00b6","text":""},{"location":"projects/classification/notebook/#8-evaluation-metrics","title":"8. Evaluation Metrics\u00b6","text":""},{"location":"projects/classification/notebook/#kaggle-submission","title":"Kaggle Submission\u00b6","text":""},{"location":"projects/classification/notebook/#conclusion-and-references","title":"Conclusion and References\u00b6","text":""},{"location":"projects/classification/notebook/#overall-findings","title":"Overall Findings\u00b6","text":"<p>This project successfully implemented a Multi-Layer Perceptron (MLP) from scratch using NumPy for binary classification on a real-world bank marketing dataset from Kaggle. The model achieved strong generalization with a ROC-AUC of 0.96 and a PR-AUC of 0.77 on the test set \u2014 values that demonstrate reliable separation of the two classes despite significant class imbalance (\u224812% positives).</p> <p>The MLP\u2019s final configuration \u2014 <code>[128, 64, 32]</code> hidden layers, ReLU activations, He initialization, L2 regularization, and early stopping \u2014 provided an effective balance between learning capacity and overfitting control. Training was stable, and validation curves showed smooth convergence without divergence or oscillation. Using validation ROC-AUC as the monitored metric ensured that model selection aligned with the project\u2019s primary goal: robust ranking performance under imbalance.</p>"},{"location":"projects/classification/notebook/#limitations","title":"Limitations\u00b6","text":"<ul> <li>Model simplicity: The MLP, while effective, lacks the representational efficiency of more advanced architectures like gradient-boosted trees (XGBoost/LightGBM) or deep ensembles commonly used for tabular data.</li> <li>Manual optimization: Without adaptive optimizers (e.g., Adam) or learning-rate schedules beyond simple decay, convergence speed and optimality might be limited.</li> <li>Feature interactions: The MLP relied solely on basic preprocessing; engineered interactions or embeddings could further enhance predictive performance.</li> <li>Computation time: Implementing backpropagation purely in NumPy is slower than using vectorized deep learning frameworks (e.g., PyTorch, TensorFlow).</li> </ul>"},{"location":"projects/classification/notebook/#references","title":"References\u00b6","text":"<ul> <li>Kaggle: Playground Series S5E8 \u2014 Bank Marketing Dataset</li> <li>UCI Machine Learning Repository: Bank Marketing Data Set</li> </ul>"},{"location":"projects/classification/notebook/#ai-usage","title":"AI Usage\u00b6","text":"<p>AI was used to help organize the notebook and debug code. Everything was verified and corrected by the authors.</p>"},{"location":"projects/regression/notebook/","title":"2. Regression","text":"In\u00a0[40]: Copied! <pre># Basic imports &amp; reproducibility\nimport os\nimport random\nfrom pathlib import Path\nimport warnings\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport copy\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator\n\n# stats / util\nfrom scipy import stats\nfrom tqdm.auto import tqdm\n\n# preprocessing &amp; modeling\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# metrics\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, roc_auc_score, roc_curve,\n    precision_recall_curve, average_precision_score, classification_report\n)\n\n# notebook display\n%matplotlib inline\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 200)\nwarnings.filterwarnings('ignore')\n\n# plotting style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_context('notebook')\n\n# seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\n</pre> # Basic imports &amp; reproducibility import os import random from pathlib import Path import warnings  import math import numpy as np import pandas as pd import itertools import copy  # plotting import matplotlib.pyplot as plt import seaborn as sns from matplotlib.ticker import MaxNLocator  # stats / util from scipy import stats from tqdm.auto import tqdm  # preprocessing &amp; modeling from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split, StratifiedKFold from sklearn.utils.class_weight import compute_class_weight  # metrics from sklearn.metrics import (     accuracy_score, precision_score, recall_score, f1_score,     confusion_matrix, roc_auc_score, roc_curve,     precision_recall_curve, average_precision_score, classification_report )  # notebook display %matplotlib inline pd.set_option('display.max_columns', None) pd.set_option('display.width', 200) warnings.filterwarnings('ignore')  # plotting style plt.style.use('seaborn-v0_8-whitegrid') sns.set_context('notebook')  # seeds for reproducibility SEED = 42 random.seed(SEED) np.random.seed(SEED) os.environ['PYTHONHASHSEED'] = str(SEED) In\u00a0[41]: Copied! <pre># Load dataset\nDATA_DIR = Path(\"data\")\ntrain = pd.read_csv(DATA_DIR / \"train.csv\")\ntest = pd.read_csv(DATA_DIR / \"test.csv\")\n\nprint(\"Dataset Shape:\")\nprint(f\"  Training: {train.shape}\")\nprint(f\"  Test: {test.shape}\")\n\nprint(\"\\nFirst 5 rows:\")\ndisplay(train.head())\n\nprint(\"\\nData Types:\")\nprint(train.dtypes)\n</pre> # Load dataset DATA_DIR = Path(\"data\") train = pd.read_csv(DATA_DIR / \"train.csv\") test = pd.read_csv(DATA_DIR / \"test.csv\")  print(\"Dataset Shape:\") print(f\"  Training: {train.shape}\") print(f\"  Test: {test.shape}\")  print(\"\\nFirst 5 rows:\") display(train.head())  print(\"\\nData Types:\") print(train.dtypes) <pre>Dataset Shape:\n  Training: (750000, 9)\n  Test: (250000, 8)\n\nFirst 5 rows:\n</pre> id Sex Age Height Weight Duration Heart_Rate Body_Temp Calories 0 0 male 36 189.0 82.0 26.0 101.0 41.0 150.0 1 1 female 64 163.0 60.0 8.0 85.0 39.7 34.0 2 2 female 51 161.0 64.0 7.0 84.0 39.8 29.0 3 3 male 20 192.0 90.0 25.0 105.0 40.7 140.0 4 4 female 38 166.0 61.0 25.0 102.0 40.6 146.0 <pre>\nData Types:\nid              int64\nSex            object\nAge             int64\nHeight        float64\nWeight        float64\nDuration      float64\nHeart_Rate    float64\nBody_Temp     float64\nCalories      float64\ndtype: object\n</pre> In\u00a0[42]: Copied! <pre># Identify feature types\ntarget_col = 'Calories'\nid_col = 'id'\n\nnumerical_features = train.select_dtypes(include=['number']).columns.tolist()\nnumerical_features = [col for col in numerical_features if col not in [id_col, target_col]]\n\ncategorical_features = train.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\nprint(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n</pre> # Identify feature types target_col = 'Calories' id_col = 'id'  numerical_features = train.select_dtypes(include=['number']).columns.tolist() numerical_features = [col for col in numerical_features if col not in [id_col, target_col]]  categorical_features = train.select_dtypes(include=['object']).columns.tolist()  print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\") print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\") <pre>Numerical features (6): ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n\nCategorical features (1): ['Sex']\n</pre> In\u00a0[43]: Copied! <pre># Summary statistics\ndisplay(train[numerical_features].describe())\n\n# Check for missing values\nprint(\"Missing Values:\")\nprint(train.isnull().sum())\n\n# Check for duplicates\nprint(f\"\\nDuplicate Rows: {train.duplicated().sum()}\")\n</pre> # Summary statistics display(train[numerical_features].describe())  # Check for missing values print(\"Missing Values:\") print(train.isnull().sum())  # Check for duplicates print(f\"\\nDuplicate Rows: {train.duplicated().sum()}\") Age Height Weight Duration Heart_Rate Body_Temp count 750000.000000 750000.000000 750000.000000 750000.000000 750000.000000 750000.000000 mean 41.420404 174.697685 75.145668 15.421015 95.483995 40.036253 std 15.175049 12.824496 13.982704 8.354095 9.449845 0.779875 min 20.000000 126.000000 36.000000 1.000000 67.000000 37.100000 25% 28.000000 164.000000 63.000000 8.000000 88.000000 39.600000 50% 40.000000 174.000000 74.000000 15.000000 95.000000 40.300000 75% 52.000000 185.000000 87.000000 23.000000 103.000000 40.700000 max 79.000000 222.000000 132.000000 30.000000 128.000000 41.500000 <pre>Missing Values:\nid            0\nSex           0\nAge           0\nHeight        0\nWeight        0\nDuration      0\nHeart_Rate    0\nBody_Temp     0\nCalories      0\ndtype: int64\n\nDuplicate Rows: 0\n</pre> In\u00a0[44]: Copied! <pre># Check for outliers (beyond 3 standard deviations)\noutlier_summary = {}\nfor col in numerical_features:\n    z = np.abs(stats.zscore(train[col]))\n    outlier_summary[col] = (z &gt; 3).mean() * 100\n\noutlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['% Outliers'])\ndisplay(outlier_df.sort_values('% Outliers', ascending=False))\n</pre> # Check for outliers (beyond 3 standard deviations) outlier_summary = {} for col in numerical_features:     z = np.abs(stats.zscore(train[col]))     outlier_summary[col] = (z &gt; 3).mean() * 100  outlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['% Outliers']) display(outlier_df.sort_values('% Outliers', ascending=False)) % Outliers Body_Temp 0.449067 Heart_Rate 0.023333 Weight 0.013867 Height 0.005067 Age 0.000000 Duration 0.000000 In\u00a0[45]: Copied! <pre>fig, axes = plt.subplots(3, 2, figsize=(15, 12))\naxes = axes.flatten()\n\nfor i, col in enumerate(numerical_features):\n    data = train[col].dropna()\n    # histogram with density overlay (optional)\n    axes[i].hist(data, bins=40, color=\"#a149d1\", edgecolor=\"black\", alpha=0.7)\n    axes[i].set_title(f\"{col} (mean={data.mean():.1f})\", fontsize=10)\n    axes[i].grid(alpha=0.3)\n    if data.max() &gt; 5000:\n        axes[i].set_xscale(\"log\")\n    axes[i].set_xlabel(\"\")\n    axes[i].set_ylabel(\"\")\n\n# hide any unused subplots\nfor j in range(i + 1, len(axes)):\n    axes[j].axis(\"off\")\n\nfig.suptitle(\"Numerical Features Distribution\", fontsize=16, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(3, 2, figsize=(15, 12)) axes = axes.flatten()  for i, col in enumerate(numerical_features):     data = train[col].dropna()     # histogram with density overlay (optional)     axes[i].hist(data, bins=40, color=\"#a149d1\", edgecolor=\"black\", alpha=0.7)     axes[i].set_title(f\"{col} (mean={data.mean():.1f})\", fontsize=10)     axes[i].grid(alpha=0.3)     if data.max() &gt; 5000:         axes[i].set_xscale(\"log\")     axes[i].set_xlabel(\"\")     axes[i].set_ylabel(\"\")  # hide any unused subplots for j in range(i + 1, len(axes)):     axes[j].axis(\"off\")  fig.suptitle(\"Numerical Features Distribution\", fontsize=16, fontweight=\"bold\", y=1.02) plt.tight_layout() plt.show() In\u00a0[46]: Copied! <pre># Correlation matrix\nplt.figure(figsize=(10, 8))\ncorrelation = train[numerical_features + [target_col]].corr()\nsns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True)\nplt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nMost correlated features with target:\")\nprint(correlation[target_col].sort_values(ascending=False))\n</pre> # Correlation matrix plt.figure(figsize=(10, 8)) correlation = train[numerical_features + [target_col]].corr() sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True) plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold') plt.tight_layout() plt.show()  print(\"\\nMost correlated features with target:\") print(correlation[target_col].sort_values(ascending=False)) <pre>\nMost correlated features with target:\nCalories      1.000000\nDuration      0.959908\nHeart_Rate    0.908748\nBody_Temp     0.828671\nAge           0.145683\nWeight        0.015863\nHeight       -0.004026\nName: Calories, dtype: float64\n</pre> In\u00a0[47]: Copied! <pre># Sex distribution just print as percentages\nprint(\"\\nSex Distribution:\")\nprint(train['Sex'].value_counts(normalize=True) * 100)\n</pre> # Sex distribution just print as percentages print(\"\\nSex Distribution:\") print(train['Sex'].value_counts(normalize=True) * 100) <pre>\nSex Distribution:\nSex\nfemale    50.096133\nmale      49.903867\nName: proportion, dtype: float64\n</pre> In\u00a0[48]: Copied! <pre>plt.figure(figsize=(5,4))\nplt.hist(train[target_col], bins=30, color=\"#a149d1\", edgecolor=\"black\", alpha=0.7)\nplt.title(f\"{target_col} Distribution\", fontsize=14, fontweight=\"bold\")\nplt.xlabel(target_col)\nplt.ylabel(\"Frequency\")\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(5,4)) plt.hist(train[target_col], bins=30, color=\"#a149d1\", edgecolor=\"black\", alpha=0.7) plt.title(f\"{target_col} Distribution\", fontsize=14, fontweight=\"bold\") plt.xlabel(target_col) plt.ylabel(\"Frequency\") plt.grid(alpha=0.3) plt.tight_layout() plt.show() <p>There are no missing values and no duplicates in the dataset. Most features are close to normal (low skewness). No transformations are applied.</p> In\u00a0[49]: Copied! <pre>import numpy as np\n\n# --- Feature Engineering ---\ntrain_fe = train.copy()\n\n# BMI: kg/m^2\ntrain_fe[\"BMI\"] = train_fe[\"Weight\"] / (train_fe[\"Height\"] / 100) ** 2\n\n# Exercise Intensity: heart rate per minute of duration\ntrain_fe[\"Exercise_Intensity\"] = train_fe[\"Heart_Rate\"] / (train_fe[\"Duration\"] + 1e-5)\n\n# Temperature deviation from normal body temp (37\u00b0C)\ntrain_fe[\"Temp_Diff\"] = train_fe[\"Body_Temp\"] - 37.0\n\n# Workload index: combination of duration and heart rate\ntrain_fe[\"Workload_Index\"] = train_fe[\"Duration\"] * train_fe[\"Heart_Rate\"]\n\n# Optional: Age group categorical bin\ntrain_fe[\"Age_Group\"] = pd.cut(\n    train_fe[\"Age\"],\n    bins=[0, 30, 50, np.inf],\n    labels=[\"Young\", \"Mid\", \"Old\"]\n)\n\n# One-hot encode Sex and Age_Group\ntrain_fe = pd.get_dummies(train_fe, columns=[\"Sex\", \"Age_Group\"], drop_first=True)\n\ntrain_fe.head()\n</pre> import numpy as np  # --- Feature Engineering --- train_fe = train.copy()  # BMI: kg/m^2 train_fe[\"BMI\"] = train_fe[\"Weight\"] / (train_fe[\"Height\"] / 100) ** 2  # Exercise Intensity: heart rate per minute of duration train_fe[\"Exercise_Intensity\"] = train_fe[\"Heart_Rate\"] / (train_fe[\"Duration\"] + 1e-5)  # Temperature deviation from normal body temp (37\u00b0C) train_fe[\"Temp_Diff\"] = train_fe[\"Body_Temp\"] - 37.0  # Workload index: combination of duration and heart rate train_fe[\"Workload_Index\"] = train_fe[\"Duration\"] * train_fe[\"Heart_Rate\"]  # Optional: Age group categorical bin train_fe[\"Age_Group\"] = pd.cut(     train_fe[\"Age\"],     bins=[0, 30, 50, np.inf],     labels=[\"Young\", \"Mid\", \"Old\"] )  # One-hot encode Sex and Age_Group train_fe = pd.get_dummies(train_fe, columns=[\"Sex\", \"Age_Group\"], drop_first=True)  train_fe.head()  Out[49]: id Age Height Weight Duration Heart_Rate Body_Temp Calories BMI Exercise_Intensity Temp_Diff Workload_Index Sex_male Age_Group_Mid Age_Group_Old 0 0 36 189.0 82.0 26.0 101.0 41.0 150.0 22.955684 3.884614 4.0 2626.0 True True False 1 1 64 163.0 60.0 8.0 85.0 39.7 34.0 22.582709 10.624987 2.7 680.0 False False True 2 2 51 161.0 64.0 7.0 84.0 39.8 29.0 24.690405 11.999983 2.8 588.0 False False True 3 3 20 192.0 90.0 25.0 105.0 40.7 140.0 24.414062 4.199998 3.7 2625.0 True False False 4 4 38 166.0 61.0 25.0 102.0 40.6 146.0 22.136740 4.079998 3.6 2550.0 False True False In\u00a0[50]: Copied! <pre># Check for invalid values\nprint(train_fe.isna().sum().sort_values(ascending=False).head(10))\nprint(train_fe.describe()[[\"BMI\",\"Exercise_Intensity\",\"Temp_Diff\",\"Workload_Index\"]])\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\nsns.histplot(train_fe[\"BMI\"], bins=40, kde=True, ax=axes[0,0])\nsns.histplot(train_fe[\"Exercise_Intensity\"], bins=40, kde=True, ax=axes[0,1])\nsns.histplot(train_fe[\"Temp_Diff\"], bins=40, kde=True, ax=axes[1,0])\nsns.histplot(train_fe[\"Workload_Index\"], bins=40, kde=True, ax=axes[1,1])\nplt.tight_layout()\nplt.show()\n</pre> # Check for invalid values print(train_fe.isna().sum().sort_values(ascending=False).head(10)) print(train_fe.describe()[[\"BMI\",\"Exercise_Intensity\",\"Temp_Diff\",\"Workload_Index\"]])  fig, axes = plt.subplots(2, 2, figsize=(10, 8)) sns.histplot(train_fe[\"BMI\"], bins=40, kde=True, ax=axes[0,0]) sns.histplot(train_fe[\"Exercise_Intensity\"], bins=40, kde=True, ax=axes[0,1]) sns.histplot(train_fe[\"Temp_Diff\"], bins=40, kde=True, ax=axes[1,0]) sns.histplot(train_fe[\"Workload_Index\"], bins=40, kde=True, ax=axes[1,1]) plt.tight_layout() plt.show()  <pre>id                    0\nAge                   0\nHeight                0\nWeight                0\nDuration              0\nHeart_Rate            0\nBody_Temp             0\nCalories              0\nBMI                   0\nExercise_Intensity    0\ndtype: int64\n                 BMI  Exercise_Intensity      Temp_Diff  Workload_Index\ncount  750000.000000       750000.000000  750000.000000   750000.000000\nmean       24.374817           10.547369       3.036253     1541.562606\nstd         1.511310           12.237609       0.779875      932.453480\nmin        12.375937            2.714285       0.100000       67.000000\n25%        23.255019            4.521737       2.600000      728.000000\n50%        24.391059            6.214281       3.300000     1455.000000\n75%        25.487697           10.749987       3.700000     2323.000000\nmax        46.443986          107.998920       4.500000     3840.000000\n</pre> In\u00a0[51]: Copied! <pre>corr = train_fe.corr(numeric_only=True)[\"Calories\"].sort_values(ascending=False)\ncorr\n</pre> corr = train_fe.corr(numeric_only=True)[\"Calories\"].sort_values(ascending=False) corr Out[51]: <pre>Calories              1.000000\nWorkload_Index        0.977341\nDuration              0.959908\nHeart_Rate            0.908748\nBody_Temp             0.828671\nTemp_Diff             0.828671\nAge                   0.145683\nAge_Group_Old         0.111124\nBMI                   0.049226\nWeight                0.015863\nSex_male              0.012011\nid                    0.001148\nHeight               -0.004026\nAge_Group_Mid        -0.008983\nExercise_Intensity   -0.560975\nName: Calories, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projects/regression/notebook/#multi-layer-perceptron-mlp-classification","title":"Multi-Layer Perceptron (MLP) Classification\u00b6","text":""},{"location":"projects/regression/notebook/#project-overview","title":"Project Overview\u00b6","text":"<p>This notebook implements a Multi-Layer Perceptron (MLP) neural network from scratch for regression on a Kaggle competition dataset.</p> <p>Authors: Jo\u00e3o Pedro Rodrigues, Matheus Castelucci, Rodrigo Medeiros</p>"},{"location":"projects/regression/notebook/#1-dataset-selection","title":"1. Dataset Selection\u00b6","text":""},{"location":"projects/regression/notebook/#dataset-information","title":"Dataset Information\u00b6","text":"<p>Name: Predict Calorie Expenditure</p> <p>Source: Kaggle Playground Series S5E5</p> <p>Original Dataset: Calories Burnt Prediction</p> <p>Size:</p> <ul> <li><p>Training set: 750,000 rows x 9 columns</p> </li> <li><p>Test set: 250,000 rows x 8 columns</p> </li> <li><p>Features: 7 features (6 numerical, 1 categorical)</p> </li> <li><p>Target: Continuous (calories burned)</p> </li> </ul> <p>Why this dataset?</p> <ul> <li><p>Realistic regression problem with a continuous target variable.</p> </li> <li><p>Large enough dataset to train a neural network.</p> </li> <li><p>Good opportunity to practice feature engineering and preprocessing.</p> </li> </ul>"},{"location":"projects/regression/notebook/#2-dataset-explanation","title":"2. Dataset Explanation\u00b6","text":""},{"location":"projects/regression/notebook/#feature-descriptions","title":"Feature Descriptions\u00b6","text":"<p>Numerical Features(6):</p> <ul> <li><code>Age</code>: Age of the individual (years)</li> <li><code>Height</code>: Height of the individual (cm)</li> <li><code>Weight</code>: Weight of the individual (kg)</li> <li><code>Duration</code>: Duration of the activity (minutes)</li> <li><code>Heart_rate</code>: Average heart rate during the activity (bpm)</li> <li><code>Body_temp</code>: Body temperature during the activity (\u00b0C)</li> </ul> <p>Categorical Feature(1):</p> <ul> <li><code>Sex</code>: Biological sex of the individual (Male or Female)</li> </ul> <p>Target Variable:</p> <ul> <li><code>Calories</code>: Total calories burned during the activity (continuous)</li> </ul>"},{"location":"projects/regression/notebook/#numerical-feature-summary","title":"Numerical Feature Summary\u00b6","text":""},{"location":"projects/regression/notebook/#categorical-feature-summary","title":"Categorical Feature Summary\u00b6","text":""},{"location":"projects/regression/notebook/#target-variable-distribution","title":"Target Variable Distribution\u00b6","text":""},{"location":"projects/regression/notebook/#potential-issues-identified","title":"Potential Issues Identified\u00b6","text":"<ul> <li><p>Outliers: Z-score analysis (|z| &gt; 3) flagged &lt; 0.5 % of samples as potential outliers \u2014 primarily in <code>Body_Temp</code> and <code>Heart_Rate</code>. These are rare (&lt; 0.5 %) and will be retained, as they reflect legitimate high-intensity sessions rather than data errors.</p> </li> <li><p>Feature Correlation: The correlation matrix shows strong positive relationships between:</p> <ul> <li><p><code>Height</code> and <code>Weight</code> (0.96) \u2014 expected anthropometric link.</p> </li> <li><p><code>Duration</code>, <code>Heart_Rate</code>, and <code>Body_Temp</code> (0.8\u20130.9) \u2014 plausible physiological connections.</p> </li> <li><p>All three correlate strongly with <code>Calories</code> (0.83\u20130.96), confirming them as the main predictors.</p> </li> </ul> </li> <li><p>Collinearity: The high correlation between Height and Weight could introduce multicollinearity; normalization and careful regularization during training will help mitigate this.</p> </li> </ul>"},{"location":"projects/regression/notebook/#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization\u00b6","text":""},{"location":"projects/regression/notebook/#feature-engineering","title":"Feature Engineering\u00b6","text":"<p>Since some features have strong correlations between them, we'll create new meaningful features to help the MLP learn non-linear relationships:</p> New Feature Formula Motivation BMI (Body Mass Index) <code>BMI = Weight / (Height/100)^2</code> Captures body composition; <code>Weight</code> and <code>Height</code> are highly correlated and redundant alone. BMI can provide a more interpretable ratio. Effort Index <code>Effort_Index = Heart_Rate * Duration</code> Represents total exertion time \u00d7 intensity, directly related to calorie burn. Temp_Deviation <code>Temp_Deviation = Body_Temp - Body_Temp.mean()</code> Indicates thermoregulation response \u2014 people with higher deviation might burn more calories. Age_Squared <code>Age_Squared = Age ** 2</code> Introduces a nonlinear age effect \u2014 metabolism might change nonlinearly with age. Log_Duration <code>Log_Duration = log(Duration + 1)</code> Reduces potential scale imbalance; smooths duration\u2019s long tail."}]}