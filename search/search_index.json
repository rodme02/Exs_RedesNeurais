{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Artificial Neural Networks and Deep Learning - 2025/2","text":"<p>Rodrigo Paoliello de Medeiros</p>"},{"location":"#deliverables","title":"Deliverables","text":"<ul> <li> Data Exercise</li> <li> Perceptron Exercise</li> <li> MLP Exercise</li> <li> Classification Project</li> <li> Metrics Exercise</li> <li> Regression Project</li> <li> Generative Models Project</li> </ul>"},{"location":"exercises/data/notebook/","title":"1. Data","text":"<p>First we will generate a synthetic dataset with a total of 400 samples, divided equally among 4 classes (100 samples each), using a Gaussian distribution based on the given means and standard deviations:</p> In\u00a0[99]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define parameters for each class\nclass_params = {\n    0: {'mean': [2, 3], 'std': [0.8, 2.5]},\n    1: {'mean': [5, 6], 'std': [1.2, 1.9]},\n    2: {'mean': [8, 1], 'std': [0.9, 0.9]},\n    3: {'mean': [15, 4], 'std': [0.5, 2.0]}\n}\n\n# Generate the data\ndata = []\nlabels = []\n\n# Gaussian distributions for each class\nfor label, params in class_params.items():\n    points = np.random.normal(loc=params['mean'], scale=params['std'], size=(100, 2))\n    data.append(points)\n    labels.append(np.full(100, label))\n\n# Combine into arrays\ndata = np.vstack(data)\nlabels = np.hstack(labels)\n\nprint(\"Dataset generated: \", data.shape, labels.shape)\n</pre> import numpy as np import matplotlib.pyplot as plt  # Set random seed for reproducibility np.random.seed(42)  # Define parameters for each class class_params = {     0: {'mean': [2, 3], 'std': [0.8, 2.5]},     1: {'mean': [5, 6], 'std': [1.2, 1.9]},     2: {'mean': [8, 1], 'std': [0.9, 0.9]},     3: {'mean': [15, 4], 'std': [0.5, 2.0]} }  # Generate the data data = [] labels = []  # Gaussian distributions for each class for label, params in class_params.items():     points = np.random.normal(loc=params['mean'], scale=params['std'], size=(100, 2))     data.append(points)     labels.append(np.full(100, label))  # Combine into arrays data = np.vstack(data) labels = np.hstack(labels)  print(\"Dataset generated: \", data.shape, labels.shape) <pre>Dataset generated:  (400, 2) (400,)\n</pre> <p>We will now plot a 2D scatter plot showing all the data points, with a different color for each class:</p> In\u00a0[100]: Copied! <pre>plt.figure(figsize=(10, 7))\ncolors = ['red', 'blue', 'green', 'purple']\n\n# Plot each class with different colors\nfor i in range(4):\n    plt.scatter(\n        data[labels == i, 0],\n        data[labels == i, 1],\n        label=f'Class {i}',\n        color=colors[i],\n        alpha=0.7\n    )\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Synthetic Dataset - 4 Classes')\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> plt.figure(figsize=(10, 7)) colors = ['red', 'blue', 'green', 'purple']  # Plot each class with different colors for i in range(4):     plt.scatter(         data[labels == i, 0],         data[labels == i, 1],         label=f'Class {i}',         color=colors[i],         alpha=0.7     )  plt.xlabel('Feature 1') plt.ylabel('Feature 2') plt.title('Synthetic Dataset - 4 Classes') plt.legend() plt.grid(True) plt.show() <p>The synthetic dataset forms four Gaussian clusters with distinct shapes and spreads. Class 0 is centered near (2, 3) with a wide vertical spread due to its larger variance in the y-direction, while Class 1 lies near (5, 6) with moderate spread in both axes. Class 2 is a compact, almost circular cluster at (8, 1), and Class 3 is tightly concentrated around x = 15 but elongated vertically. Class 3 is clearly isolated because of its much larger x-mean, while Classes 0, 1, and 2 occupy overlapping regions in the central space. Class 0 and Class 1 overlap in the higher y-region, and although Class 2 is generally separate, its boundary edges could touch Class 1 if variance increases.</p> <p>This arrangement shows that purely linear separation is not feasible: Class 3 could be split off by a vertical line, but Classes 0, 1, and 2 require non-linear boundaries. A neural network, such as an MLP with <code>tanh</code> activations, would likely learn curved, flexible decision regions: bending around Class 2, carving out Classes 0 and 1 in the upper region, and isolating Class 3 on the far right. The sketch below illustrates what such non-linear boundaries might look like:</p> <p></p> <p>We'll create a synthetic dataset with 500 samples for Class A and 500 samples for Class B, using a multivariate normal distribution with the parameters provided:</p> In\u00a0[101]: Copied! <pre># Parameters for Class A\nmean_A = np.array([0, 0, 0, 0, 0])\ncov_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\n# Parameters for Class B\nmean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\ncov_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n\n# Generate samples using multivariate normal distribution\nsamples_A = np.random.multivariate_normal(mean_A, cov_A, size=500)\nsamples_B = np.random.multivariate_normal(mean_B, cov_B, size=500)\n\n# Create labels\nlabels_A = np.zeros(500)  # Class A = 0\nlabels_B = np.ones(500)   # Class B = 1\n\n# Combine data\nX = np.vstack([samples_A, samples_B])\ny = np.hstack([labels_A, labels_B])\n\nprint(\"Dataset shape:\", X.shape)\nprint(\"Labels shape:\", y.shape)\n</pre> # Parameters for Class A mean_A = np.array([0, 0, 0, 0, 0]) cov_A = np.array([     [1.0, 0.8, 0.1, 0.0, 0.0],     [0.8, 1.0, 0.3, 0.0, 0.0],     [0.1, 0.3, 1.0, 0.5, 0.0],     [0.0, 0.0, 0.5, 1.0, 0.2],     [0.0, 0.0, 0.0, 0.2, 1.0] ])  # Parameters for Class B mean_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5]) cov_B = np.array([     [1.5, -0.7, 0.2, 0.0, 0.0],     [-0.7, 1.5, 0.4, 0.0, 0.0],     [0.2, 0.4, 1.5, 0.6, 0.0],     [0.0, 0.0, 0.6, 1.5, 0.3],     [0.0, 0.0, 0.0, 0.3, 1.5] ])  # Generate samples using multivariate normal distribution samples_A = np.random.multivariate_normal(mean_A, cov_A, size=500) samples_B = np.random.multivariate_normal(mean_B, cov_B, size=500)  # Create labels labels_A = np.zeros(500)  # Class A = 0 labels_B = np.ones(500)   # Class B = 1  # Combine data X = np.vstack([samples_A, samples_B]) y = np.hstack([labels_A, labels_B])  print(\"Dataset shape:\", X.shape) print(\"Labels shape:\", y.shape) <pre>Dataset shape: (1000, 5)\nLabels shape: (1000,)\n</pre> <p>Since we cannot plot a 5D graph, we will use Principal Component Analysis (PCA) to project the 5D data down to 2 dimensions. Then we'll create a scatter plot of this 2D representation, with Class A represented by red points and Class B being represented as blue points:</p> In\u00a0[102]: Copied! <pre>from sklearn.decomposition import PCA\n\n# Apply PCA to reduce dimensions to 2D\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Scatter plot\nplt.figure(figsize=(10, 7))\ncolors = ['red', 'blue']\nclasses = ['A', 'B']\nfor i, color in enumerate(colors):\n    plt.scatter(\n        X_pca[y == i, 0],\n        X_pca[y == i, 1],\n        label=f'Class {classes[i]}',\n        color=color,\n        alpha=0.7\n    )\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.title('PCA Visualization of 5D Data')\nplt.legend()\nplt.grid()\nplt.show()\n</pre> from sklearn.decomposition import PCA  # Apply PCA to reduce dimensions to 2D pca = PCA(n_components=2) X_pca = pca.fit_transform(X)  # Scatter plot plt.figure(figsize=(10, 7)) colors = ['red', 'blue'] classes = ['A', 'B'] for i, color in enumerate(colors):     plt.scatter(         X_pca[y == i, 0],         X_pca[y == i, 1],         label=f'Class {classes[i]}',         color=color,         alpha=0.7     ) plt.xlabel('First Principal Component') plt.ylabel('Second Principal Component') plt.title('PCA Visualization of 5D Data') plt.legend() plt.grid() plt.show() <p>The PCA projection of the 5D dataset reveals that Classes A and B overlap substantially in two dimensions. Their centers are slightly offset, but the distributions largely intersect, and both classes display similar spread patterns. This means that in the reduced 2D space there are no clean visual boundaries to separate them.</p> <p>This overlap reflects the underlying challenge: the full 5D structure is governed by different covariance patterns in each class, producing relationships that are not well captured by straight lines. A simple linear classifier cannot resolve such intertwined regions, as any single hyperplane would misclassify a significant portion of the data. To achieve better separation, a more expressive model is needed. A multi-layer neural network with non-linear activation functions can transform the input space into higher-order representations, bending decision boundaries around the overlapping regions. Such non-linear models are better suited to capture the complex geometry of the dataset, making accurate classification feasible where linear methods fall short.</p> <p>First, we'll download the Spaceship Titanic dataset from Kaggle, especifically the <code>train.csv</code>, since we're only using this for preparation.</p> <p>The Spaceship Titanic dataset is a sci-fi reimagining of the classic Titanic survival prediction task. It is framed as a binary classification problem, where the target column <code>Transported</code> indicates whether a passenger was transported to another dimension (<code>True</code>) or remained in the original dimension (<code>False</code>) following the spaceship incident.</p> <p>The training file, <code>train.csv</code>, contains records for roughly two-thirds of the ~8,700 passengers. Each passenger is identified by a unique <code>PassengerId</code> that encodes group membership (<code>gggg_pp</code>, where <code>gggg</code> is the group and <code>pp</code> is the index within that group). Groups often represent families or traveling companions.</p> <p>The dataset provides a mix of demographic, behavioral, and voyage-related features:</p> <ul> <li>HomePlanet \u2014 Planet of origin (permanent residence).</li> <li>CryoSleep \u2014 Whether the passenger elected suspended animation for the voyage.</li> <li>Cabin \u2014 Passenger cabin in the format <code>deck/num/side</code>, where <code>side</code> is <code>P</code> (Port) or <code>S</code> (Starboard).</li> <li>Destination \u2014 Planet of debarkation.</li> <li>Age \u2014 Passenger\u2019s age in years.</li> <li>VIP \u2014 Whether the passenger paid for special VIP service.</li> <li>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck \u2014 Expenditures at various luxury amenities onboard.</li> <li>Name \u2014 Passenger\u2019s full name (not directly predictive).</li> <li>Transported \u2014 Target variable: <code>True</code> if transported to another dimension, <code>False</code> otherwise.</li> </ul> <p>Together, these variables form a rich dataset combining categorical, numerical, and textual features. The challenge lies in preprocessing and modeling these attributes effectively to predict the outcome <code>Transported</code>. The task is analogous to Titanic survival prediction but recast in a futuristic setting.</p> In\u00a0[103]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv(\"spaceship-titanic/train.csv\")\n\ndf.head(5)\n</pre> import pandas as pd  df = pd.read_csv(\"spaceship-titanic/train.csv\")  df.head(5) Out[103]: PassengerId HomePlanet CryoSleep Cabin Destination Age VIP RoomService FoodCourt ShoppingMall Spa VRDeck Name Transported 0 0001_01 Europa False B/0/P TRAPPIST-1e 39.0 False 0.0 0.0 0.0 0.0 0.0 Maham Ofracculy False 1 0002_01 Earth False F/0/S TRAPPIST-1e 24.0 False 109.0 9.0 25.0 549.0 44.0 Juanna Vines True 2 0003_01 Europa False A/0/S TRAPPIST-1e 58.0 True 43.0 3576.0 0.0 6715.0 49.0 Altark Susent False 3 0003_02 Europa False A/0/S TRAPPIST-1e 33.0 False 0.0 1283.0 371.0 3329.0 193.0 Solam Susent False 4 0004_01 Earth False F/1/S TRAPPIST-1e 16.0 False 303.0 70.0 151.0 565.0 2.0 Willy Santantines True <p>Let's list all the numerical and categorical features of this dataset:</p> In\u00a0[104]: Copied! <pre># Separate features and target\ntarget_column = 'Transported'\ny = df[target_column].astype(int)\nX = df.drop(columns=[target_column])\n\n# Identify numerical and categorical features\nnumerical_features = X.select_dtypes(include=['number']).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n\nprint(\"\\nNumerical Features:\")\nfor feature in numerical_features:\n    print(f\"- {feature}\")\n\nprint(\"\\nCategorical Features:\")\nfor feature in categorical_features:\n    print(f\"- {feature}\")\n</pre> # Separate features and target target_column = 'Transported' y = df[target_column].astype(int) X = df.drop(columns=[target_column])  # Identify numerical and categorical features numerical_features = X.select_dtypes(include=['number']).columns.tolist() categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()  print(\"\\nNumerical Features:\") for feature in numerical_features:     print(f\"- {feature}\")  print(\"\\nCategorical Features:\") for feature in categorical_features:     print(f\"- {feature}\") <pre>\nNumerical Features:\n- Age\n- RoomService\n- FoodCourt\n- ShoppingMall\n- Spa\n- VRDeck\n\nCategorical Features:\n- PassengerId\n- HomePlanet\n- CryoSleep\n- Cabin\n- Destination\n- VIP\n- Name\n</pre> <p>We'll now investigate the dataset for missing values:</p> In\u00a0[105]: Copied! <pre># Count missing values\nmissing_counts = df.isna().sum()\nn_rows = len(df)\n\n# Create a table for missing values\nmissing_table = (\n    pd.DataFrame({\n        \"missing_count\": missing_counts,\n        \"missing_pct\": (missing_counts / n_rows * 100).round(2)\n    })\n    .query(\"missing_count &gt; 0\")\n    .sort_values(\"missing_count\", ascending=False)\n)\n\nprint(\"\\nMissing Values\")\nprint(missing_table.to_string(float_format=lambda x: f\"{x:,.2f}%\"))\n</pre> # Count missing values missing_counts = df.isna().sum() n_rows = len(df)  # Create a table for missing values missing_table = (     pd.DataFrame({         \"missing_count\": missing_counts,         \"missing_pct\": (missing_counts / n_rows * 100).round(2)     })     .query(\"missing_count &gt; 0\")     .sort_values(\"missing_count\", ascending=False) )  print(\"\\nMissing Values\") print(missing_table.to_string(float_format=lambda x: f\"{x:,.2f}%\")) <pre>\nMissing Values\n              missing_count  missing_pct\nCryoSleep               217        2.50%\nShoppingMall            208        2.39%\nVIP                     203        2.34%\nHomePlanet              201        2.31%\nName                    200        2.30%\nCabin                   199        2.29%\nVRDeck                  188        2.16%\nFoodCourt               183        2.11%\nSpa                     183        2.11%\nDestination             182        2.09%\nRoomService             181        2.08%\nAge                     179        2.06%\n</pre> <p>We will now clean and transform the data so it can be fed into a neural network. The <code>tanh</code> activation function produces outputs in the range <code>[-1, 1]</code>, so the input data should be scaled appropriately for stable training.</p> <p>First we'll implement a strategy to handle the missing values in all the affected columns:</p> <ul> <li><p>Numerical Features (Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck): Use median imputation. Median is robust against outliers (e.g., some passengers spend huge amounts in VRDeck or FoodCourt).</p> </li> <li><p>Categorical Features (HomePlanet, Destination, Cabin): Use most frequent (mode) imputation. This fills missing entries with the most common value, which preserves the categorical distribution. Drop PassengerId and Name as they are identifiers.</p> </li> <li><p>Boolean Features (CryoSleep, VIP): Treat them as categorical and impute with most frequent value. This avoids introducing bias since only ~2% are missing.</p> </li> </ul> In\u00a0[106]: Copied! <pre>from sklearn.impute import SimpleImputer\n\n# Define the columns we will actually use downstream\nnumerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\ncategorical_features = ['HomePlanet', 'Destination', 'Cabin', 'CryoSleep', 'VIP']\n\n# Drop identifier columns from features\nid_cols = ['PassengerId', 'Name']\nX = df.drop(columns=[target_column] + id_cols, errors='ignore').copy()\n\n# Create imputers\nnum_imputer = SimpleImputer(strategy=\"median\")          # robust to outliers in numericals\ncat_imputer = SimpleImputer(strategy=\"most_frequent\")   # preserves mode for categoricals/booleans\n\n# Apply the imputers\nX[numerical_features] = num_imputer.fit_transform(X[numerical_features])\nX[categorical_features] = cat_imputer.fit_transform(X[categorical_features])\n\n# Sanity check: no missing values should remain in these groups\nprint(\"Remaining NAs (numeric):\", int(X[numerical_features].isna().sum().sum()))\nprint(\"Remaining NAs (categorical):\", int(X[categorical_features].isna().sum().sum()))\n</pre> from sklearn.impute import SimpleImputer  # Define the columns we will actually use downstream numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] categorical_features = ['HomePlanet', 'Destination', 'Cabin', 'CryoSleep', 'VIP']  # Drop identifier columns from features id_cols = ['PassengerId', 'Name'] X = df.drop(columns=[target_column] + id_cols, errors='ignore').copy()  # Create imputers num_imputer = SimpleImputer(strategy=\"median\")          # robust to outliers in numericals cat_imputer = SimpleImputer(strategy=\"most_frequent\")   # preserves mode for categoricals/booleans  # Apply the imputers X[numerical_features] = num_imputer.fit_transform(X[numerical_features]) X[categorical_features] = cat_imputer.fit_transform(X[categorical_features])  # Sanity check: no missing values should remain in these groups print(\"Remaining NAs (numeric):\", int(X[numerical_features].isna().sum().sum())) print(\"Remaining NAs (categorical):\", int(X[categorical_features].isna().sum().sum())) <pre>Remaining NAs (numeric): 0\nRemaining NAs (categorical): 0\n</pre> <p>Now, we'll encode categorical features into a numerical format using one-hot encoding with <code>pd.get_dummies()</code>, which creates binary columns for each category:</p> In\u00a0[107]: Copied! <pre># One-hot encode categorical features\nX_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\nprint(\"Encoded shape:\", X_encoded.shape)\n</pre> # One-hot encode categorical features X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True) print(\"Encoded shape:\", X_encoded.shape) <pre>Encoded shape: (8693, 6571)\n</pre> <p>We will now scale the numerical variables. Because the <code>tanh</code> activation function is centered at zero and outputs values in the range [-1, 1], bringing inputs onto a similar scale is essential. Scaling prevents features with large ranges from dominating learning, stabilizes gradient updates, and accelerates convergence.</p> <p>Here we normalize values to [-1, 1], aligning the inputs with the activation\u2019s range. This practice improves training efficiency and helps the network learn more reliable non-linear decision boundaries.</p> In\u00a0[108]: Copied! <pre>from sklearn.preprocessing import MinMaxScaler\n\n# Scale numerical features in the encoded/imputed feature matrix\nscaler = MinMaxScaler(feature_range=(-1, 1))\nX_scaled = X_encoded.copy()\nX_scaled[numerical_features] = scaler.fit_transform(X_scaled[numerical_features])\n\n# Quick preview\nprint(X_scaled[numerical_features].head())\n</pre> from sklearn.preprocessing import MinMaxScaler  # Scale numerical features in the encoded/imputed feature matrix scaler = MinMaxScaler(feature_range=(-1, 1)) X_scaled = X_encoded.copy() X_scaled[numerical_features] = scaler.fit_transform(X_scaled[numerical_features])  # Quick preview print(X_scaled[numerical_features].head()) <pre>        Age  RoomService  FoodCourt  ShoppingMall       Spa    VRDeck\n0 -0.012658    -1.000000  -1.000000     -1.000000 -1.000000 -1.000000\n1 -0.392405    -0.984784  -0.999396     -0.997872 -0.951000 -0.996354\n2  0.468354    -0.993997  -0.760105     -1.000000 -0.400660 -0.995939\n3 -0.164557    -1.000000  -0.913930     -0.968415 -0.702874 -0.984005\n4 -0.594937    -0.957702  -0.995304     -0.987145 -0.949572 -0.999834\n</pre> <p>We'll now create histograms for <code>FoodCourt</code> and <code>Age</code> before and after scaling to show the difference, the values should be between [-1, 1] instead of their original values, following the same distribution:</p> In\u00a0[109]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 1, figsize=(16, 10))\n\n# FoodCourt before\ndf['FoodCourt'].dropna().hist(bins=30, ax=axes[0])\naxes[0].set_title('FoodCourt \u2014 Before Scaling')\n\n# FoodCourt after\nX_scaled['FoodCourt'].dropna().hist(bins=30, ax=axes[1])\naxes[1].set_title('FoodCourt \u2014 After Scaling ([-1, 1])')\n\nplt.show()\n\nfig, axes = plt.subplots(2, 1, figsize=(16, 10))\n\n# Age before\ndf['Age'].dropna().hist(bins=30, ax=axes[0])\naxes[0].set_title('Age \u2014 Before Scaling')\n\n# Age after\nX_scaled['Age'].dropna().hist(bins=30, ax=axes[1])\naxes[1].set_title('Age \u2014 After Scaling ([-1, 1])')\n\nplt.show()\n</pre> import matplotlib.pyplot as plt  fig, axes = plt.subplots(2, 1, figsize=(16, 10))  # FoodCourt before df['FoodCourt'].dropna().hist(bins=30, ax=axes[0]) axes[0].set_title('FoodCourt \u2014 Before Scaling')  # FoodCourt after X_scaled['FoodCourt'].dropna().hist(bins=30, ax=axes[1]) axes[1].set_title('FoodCourt \u2014 After Scaling ([-1, 1])')  plt.show()  fig, axes = plt.subplots(2, 1, figsize=(16, 10))  # Age before df['Age'].dropna().hist(bins=30, ax=axes[0]) axes[0].set_title('Age \u2014 Before Scaling')  # Age after X_scaled['Age'].dropna().hist(bins=30, ax=axes[1]) axes[1].set_title('Age \u2014 After Scaling ([-1, 1])')  plt.show()"},{"location":"exercises/data/notebook/#data","title":"Data\u00b6","text":""},{"location":"exercises/data/notebook/#exercise-1-exploring-class-separability","title":"Exercise 1 - Exploring Class Separability\u00b6","text":""},{"location":"exercises/data/notebook/#generate-the-data","title":"Generate the Data\u00b6","text":""},{"location":"exercises/data/notebook/#plot-the-data","title":"Plot the Data\u00b6","text":""},{"location":"exercises/data/notebook/#exercise-2-non-linearity-in-higher-dimensions","title":"Exercise 2 - Non-Linearity in Higher Dimensions\u00b6","text":""},{"location":"exercises/data/notebook/#generate-the-data","title":"Generate the Data\u00b6","text":""},{"location":"exercises/data/notebook/#visualize-the-data","title":"Visualize the Data\u00b6","text":""},{"location":"exercises/data/notebook/#analyze-the-plot","title":"Analyze the Plot\u00b6","text":""},{"location":"exercises/data/notebook/#exercise-3-preparing-real-world-data-for-a-neural-network","title":"Exercise 3 - Preparing Real-World Data for a Neural Network\u00b6","text":""},{"location":"exercises/data/notebook/#get-the-data","title":"Get the Data\u00b6","text":""},{"location":"exercises/data/notebook/#describe-the-data","title":"Describe the Data\u00b6","text":""},{"location":"exercises/data/notebook/#preprocess-the-data","title":"Preprocess the Data\u00b6","text":""},{"location":"exercises/data/notebook/#visualize-the-results","title":"Visualize the Results\u00b6","text":""},{"location":"exercises/data/notebook/#ai-assistance","title":"AI Assistance\u00b6","text":"<p>I used AI (ChatGPT) to help with:</p> <ul> <li>Reviewing my code and outputs for clarity.</li> <li>Suggesting improvements to formatting and readability.</li> </ul> <p>All code was executed, tested, and validated locally by me. Nothing was done with AI that I didn't understand.</p>"},{"location":"exercises/mlp/notebook/","title":"3. MLP","text":"In\u00a0[256]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom typing import List, Tuple, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Configure matplotlib\nplt.style.use('seaborn-v0_8-darkgrid')\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, confusion_matrix import seaborn as sns from typing import List, Tuple, Optional import warnings warnings.filterwarnings('ignore')  # Set random seed for reproducibility np.random.seed(42)  # Configure matplotlib plt.style.use('seaborn-v0_8-darkgrid') In\u00a0[257]: Copied! <pre># Given values from the exercise\nx = np.array([0.5, -0.2])\ny = 1.0\nW1 = np.array([[0.3, -0.1], [0.2, 0.4]])\nb1 = np.array([0.1, -0.2])\nW2 = np.array([0.5, -0.3])\nb2 = 0.2\neta = 0.3  # Learning rate\n</pre> # Given values from the exercise x = np.array([0.5, -0.2]) y = 1.0 W1 = np.array([[0.3, -0.1], [0.2, 0.4]]) b1 = np.array([0.1, -0.2]) W2 = np.array([0.5, -0.3]) b2 = 0.2 eta = 0.3  # Learning rate In\u00a0[258]: Copied! <pre># Define activation functions from scratch\ndef tanh_manual(x):\n    \"\"\"Hyperbolic tangent activation function\"\"\"\n    return np.tanh(x)\n\ndef tanh_derivative(x):\n    \"\"\"Derivative of tanh: 1 - tanh^2(x)\"\"\"\n    return 1 - np.tanh(x)**2\n\n# Hidden layer pre-activations\nz1 = np.dot(W1, x) + b1\nprint(f\"\\nHidden layer pre-activations:\")\nprint(f\"z^(1) = {z1}\")\n\n# Hidden layer activations\nh1 = tanh_manual(z1)\nprint(f\"\\nHidden layer activations:\")\nprint(f\"h^(1) = {h1}\")\n\n# Output pre-activation\nu2 = np.dot(W2, h1) + b2\nprint(f\"\\nOutput pre-activation:\")\nprint(f\"u^(2) = {u2}\")\n\n# Final output\ny_hat = tanh_manual(u2)\nprint(f\"\\nFinal output:\")\nprint(f\"\u0177 = {y_hat}\")\n</pre> # Define activation functions from scratch def tanh_manual(x):     \"\"\"Hyperbolic tangent activation function\"\"\"     return np.tanh(x)  def tanh_derivative(x):     \"\"\"Derivative of tanh: 1 - tanh^2(x)\"\"\"     return 1 - np.tanh(x)**2  # Hidden layer pre-activations z1 = np.dot(W1, x) + b1 print(f\"\\nHidden layer pre-activations:\") print(f\"z^(1) = {z1}\")  # Hidden layer activations h1 = tanh_manual(z1) print(f\"\\nHidden layer activations:\") print(f\"h^(1) = {h1}\")  # Output pre-activation u2 = np.dot(W2, h1) + b2 print(f\"\\nOutput pre-activation:\") print(f\"u^(2) = {u2}\")  # Final output y_hat = tanh_manual(u2) print(f\"\\nFinal output:\") print(f\"\u0177 = {y_hat}\") <pre>\nHidden layer pre-activations:\nz^(1) = [ 0.27 -0.18]\n\nHidden layer activations:\nh^(1) = [ 0.26362484 -0.17808087]\n\nOutput pre-activation:\nu^(2) = 0.38523667817130075\n\nFinal output:\n\u0177 = 0.36724656264510797\n</pre> In\u00a0[259]: Copied! <pre># Calculate MSE loss\nN = 1  # Single sample\nloss = (1/N) * (y - y_hat)**2\nprint(f\"\\nMean Squared Error (MSE) Loss:\")\nprint(f\"L = {loss}\")\n</pre> # Calculate MSE loss N = 1  # Single sample loss = (1/N) * (y - y_hat)**2 print(f\"\\nMean Squared Error (MSE) Loss:\") print(f\"L = {loss}\") <pre>\nMean Squared Error (MSE) Loss:\nL = 0.4003769124844312\n</pre> In\u00a0[260]: Copied! <pre># Gradient of loss w.r.t output\ndL_dy_hat = -2 * (y - y_hat) / N\nprint(f\"\\nGradient of loss w.r.t. output:\")\nprint(f\"\u2202L/\u2202\u0177 = {dL_dy_hat}\")\n\n# Gradient w.r.t output pre-activation\ndL_du2 = dL_dy_hat * tanh_derivative(u2)\nprint(f\"\\nGradient w.r.t. output pre-activation:\")\nprint(f\"\u2202L/\u2202u^(2) = {dL_du2}\")\n\n# Gradients for output layer\ndL_dW2 = dL_du2 * h1\ndL_db2 = dL_du2\nprint(f\"\\nGradients for output layer:\")\nprint(f\"\u2202L/\u2202W^(2) = {dL_dW2}\")\nprint(f\"\u2202L/\u2202b^(2) = {dL_db2}\")\n\n# Propagate to hidden layer\ndL_dh1 = dL_du2 * W2\nprint(f\"\\nGradient w.r.t. hidden layer activations:\")\nprint(f\"\u2202L/\u2202h^(1) = {dL_dh1}\")\n\n# Gradient w.r.t hidden pre-activations\ndL_dz1 = dL_dh1 * tanh_derivative(z1)\nprint(f\"\\nGradient w.r.t. hidden pre-activations:\")\nprint(f\"\u2202L/\u2202z^(1) = {dL_dz1}\")\n\n# Gradients for hidden layer\ndL_dW1 = np.outer(dL_dz1, x)\ndL_db1 = dL_dz1\nprint(f\"\\nGradients for hidden layer:\")\nprint(f\"\u2202L/\u2202W^(1) =\\n{dL_dW1}\")\nprint(f\"\u2202L/\u2202b^(1) = {dL_db1}\")\n</pre> # Gradient of loss w.r.t output dL_dy_hat = -2 * (y - y_hat) / N print(f\"\\nGradient of loss w.r.t. output:\") print(f\"\u2202L/\u2202\u0177 = {dL_dy_hat}\")  # Gradient w.r.t output pre-activation dL_du2 = dL_dy_hat * tanh_derivative(u2) print(f\"\\nGradient w.r.t. output pre-activation:\") print(f\"\u2202L/\u2202u^(2) = {dL_du2}\")  # Gradients for output layer dL_dW2 = dL_du2 * h1 dL_db2 = dL_du2 print(f\"\\nGradients for output layer:\") print(f\"\u2202L/\u2202W^(2) = {dL_dW2}\") print(f\"\u2202L/\u2202b^(2) = {dL_db2}\")  # Propagate to hidden layer dL_dh1 = dL_du2 * W2 print(f\"\\nGradient w.r.t. hidden layer activations:\") print(f\"\u2202L/\u2202h^(1) = {dL_dh1}\")  # Gradient w.r.t hidden pre-activations dL_dz1 = dL_dh1 * tanh_derivative(z1) print(f\"\\nGradient w.r.t. hidden pre-activations:\") print(f\"\u2202L/\u2202z^(1) = {dL_dz1}\")  # Gradients for hidden layer dL_dW1 = np.outer(dL_dz1, x) dL_db1 = dL_dz1 print(f\"\\nGradients for hidden layer:\") print(f\"\u2202L/\u2202W^(1) =\\n{dL_dW1}\") print(f\"\u2202L/\u2202b^(1) = {dL_db1}\") <pre>\nGradient of loss w.r.t. output:\n\u2202L/\u2202\u0177 = -1.265506874709784\n\nGradient w.r.t. output pre-activation:\n\u2202L/\u2202u^(2) = -1.0948279147135995\n\nGradients for output layer:\n\u2202L/\u2202W^(2) = [-0.28862383  0.19496791]\n\u2202L/\u2202b^(2) = -1.0948279147135995\n\nGradient w.r.t. hidden layer activations:\n\u2202L/\u2202h^(1) = [-0.54741396  0.32844837]\n\nGradient w.r.t. hidden pre-activations:\n\u2202L/\u2202z^(1) = [-0.50936975  0.31803236]\n\nGradients for hidden layer:\n\u2202L/\u2202W^(1) =\n[[-0.25468488  0.10187395]\n [ 0.15901618 -0.06360647]]\n\u2202L/\u2202b^(1) = [-0.50936975  0.31803236]\n</pre> In\u00a0[261]: Copied! <pre># Set learning rate\neta = 0.1\n\n# Update weights and biases\nW2_new = W2 - eta * dL_dW2\nb2_new = b2 - eta * dL_db2\nW1_new = W1 - eta * dL_dW1\nb1_new = b1 - eta * dL_db1\n\nprint(f\"\\nUsing learning rate \u03b7 = {eta}\")\nprint(f\"\\nOutput layer updates:\")\nprint(f\"W^(2)_new = {W2_new}\")\nprint(f\"b^(2)_new = {b2_new:.6f}\")\nprint(f\"\\nHidden layer updates:\")\nprint(f\"W^(1)_new =\\n{W1_new}\")\nprint(f\"b^(1)_new = {b1_new}\")\n</pre> # Set learning rate eta = 0.1  # Update weights and biases W2_new = W2 - eta * dL_dW2 b2_new = b2 - eta * dL_db2 W1_new = W1 - eta * dL_dW1 b1_new = b1 - eta * dL_db1  print(f\"\\nUsing learning rate \u03b7 = {eta}\") print(f\"\\nOutput layer updates:\") print(f\"W^(2)_new = {W2_new}\") print(f\"b^(2)_new = {b2_new:.6f}\") print(f\"\\nHidden layer updates:\") print(f\"W^(1)_new =\\n{W1_new}\") print(f\"b^(1)_new = {b1_new}\") <pre>\nUsing learning rate \u03b7 = 0.1\n\nOutput layer updates:\nW^(2)_new = [ 0.52886238 -0.31949679]\nb^(2)_new = 0.309483\n\nHidden layer updates:\nW^(1)_new =\n[[ 0.32546849 -0.1101874 ]\n [ 0.18409838  0.40636065]]\nb^(1)_new = [ 0.15093698 -0.23180324]\n</pre> <p>The manual calculations show how backpropagation works step by step. The gradients flow backward from the output layer through the hidden layer, and each parameter is updated in the opposite direction of its gradient to minimize the loss.</p> In\u00a0[262]: Copied! <pre># Generate synthetic dataset with asymmetric clusters\ndef generate_binary_data_ex2():\n    \"\"\"Generate binary classification data with 1 cluster for class 0 and 2 clusters for class 1\"\"\"\n    # Generate class 0 with 1 cluster\n    X0, y0 = make_classification(\n        n_samples=1000,\n        n_features=2,\n        n_informative=2,\n        n_redundant=0,\n        n_clusters_per_class=1,\n        n_classes=2,\n        random_state=42,\n        class_sep=2,\n        flip_y=0.1\n    )\n    X0 = X0[y0 == 0][:500]  # Keep only class 0\n    y0 = np.zeros(len(X0))\n    \n    # Generate class 1 with 2 clusters\n    np.random.seed(43)\n    X1_cluster1 = np.random.randn(250, 2) * 0.8 + [3, 2]\n    X1_cluster2 = np.random.randn(250, 2) * 0.8 + [-2, 3]\n    X1 = np.vstack([X1_cluster1, X1_cluster2])\n    y1 = np.ones(len(X1))\n    \n    # Combine and shuffle\n    X = np.vstack([X0, X1])\n    y = np.hstack([y0, y1])\n    shuffle_idx = np.random.permutation(len(X))\n    X = X[shuffle_idx]\n    y = y[shuffle_idx]\n    \n    return X, y\n\n# Generate the dataset\nX_ex2, y_ex2 = generate_binary_data_ex2()\nX_train_ex2, X_test_ex2, y_train_ex2, y_test_ex2 = train_test_split(\n    X_ex2, y_ex2, test_size=0.2, random_state=42\n)\n\nprint(f\"\\nDataset generated:\")\nprint(f\"Training samples: {len(X_train_ex2)}\")\nprint(f\"Testing samples: {len(X_test_ex2)}\")\nprint(f\"Features: {X_ex2.shape[1]}\")\nprint(f\"Classes: {len(np.unique(y_ex2))}\")\n</pre> # Generate synthetic dataset with asymmetric clusters def generate_binary_data_ex2():     \"\"\"Generate binary classification data with 1 cluster for class 0 and 2 clusters for class 1\"\"\"     # Generate class 0 with 1 cluster     X0, y0 = make_classification(         n_samples=1000,         n_features=2,         n_informative=2,         n_redundant=0,         n_clusters_per_class=1,         n_classes=2,         random_state=42,         class_sep=2,         flip_y=0.1     )     X0 = X0[y0 == 0][:500]  # Keep only class 0     y0 = np.zeros(len(X0))          # Generate class 1 with 2 clusters     np.random.seed(43)     X1_cluster1 = np.random.randn(250, 2) * 0.8 + [3, 2]     X1_cluster2 = np.random.randn(250, 2) * 0.8 + [-2, 3]     X1 = np.vstack([X1_cluster1, X1_cluster2])     y1 = np.ones(len(X1))          # Combine and shuffle     X = np.vstack([X0, X1])     y = np.hstack([y0, y1])     shuffle_idx = np.random.permutation(len(X))     X = X[shuffle_idx]     y = y[shuffle_idx]          return X, y  # Generate the dataset X_ex2, y_ex2 = generate_binary_data_ex2() X_train_ex2, X_test_ex2, y_train_ex2, y_test_ex2 = train_test_split(     X_ex2, y_ex2, test_size=0.2, random_state=42 )  print(f\"\\nDataset generated:\") print(f\"Training samples: {len(X_train_ex2)}\") print(f\"Testing samples: {len(X_test_ex2)}\") print(f\"Features: {X_ex2.shape[1]}\") print(f\"Classes: {len(np.unique(y_ex2))}\") <pre>\nDataset generated:\nTraining samples: 800\nTesting samples: 200\nFeatures: 2\nClasses: 2\n</pre> In\u00a0[263]: Copied! <pre># Visualize the generated data\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_train_ex2[y_train_ex2 == 0, 0], X_train_ex2[y_train_ex2 == 0, 1], \n            c='blue', alpha=0.6, label='Class 0 (1 cluster)')\nplt.scatter(X_train_ex2[y_train_ex2 == 1, 0], X_train_ex2[y_train_ex2 == 1, 1], \n            c='red', alpha=0.6, label='Class 1 (2 clusters)')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Training Data Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test_ex2[y_test_ex2 == 0, 0], X_test_ex2[y_test_ex2 == 0, 1], \n            c='blue', alpha=0.6, label='Class 0')\nplt.scatter(X_test_ex2[y_test_ex2 == 1, 0], X_test_ex2[y_test_ex2 == 1, 1], \n            c='red', alpha=0.6, label='Class 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Testing Data Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</pre> # Visualize the generated data plt.figure(figsize=(12, 5))  plt.subplot(1, 2, 1) plt.scatter(X_train_ex2[y_train_ex2 == 0, 0], X_train_ex2[y_train_ex2 == 0, 1],              c='blue', alpha=0.6, label='Class 0 (1 cluster)') plt.scatter(X_train_ex2[y_train_ex2 == 1, 0], X_train_ex2[y_train_ex2 == 1, 1],              c='red', alpha=0.6, label='Class 1 (2 clusters)') plt.xlabel('Feature 1') plt.ylabel('Feature 2') plt.title('Training Data Distribution') plt.legend() plt.grid(True, alpha=0.3)  plt.subplot(1, 2, 2) plt.scatter(X_test_ex2[y_test_ex2 == 0, 0], X_test_ex2[y_test_ex2 == 0, 1],              c='blue', alpha=0.6, label='Class 0') plt.scatter(X_test_ex2[y_test_ex2 == 1, 0], X_test_ex2[y_test_ex2 == 1, 1],              c='red', alpha=0.6, label='Class 1') plt.xlabel('Feature 1') plt.ylabel('Feature 2') plt.title('Testing Data Distribution') plt.legend() plt.grid(True, alpha=0.3)  plt.tight_layout() plt.show() In\u00a0[264]: Copied! <pre>class MLPBinaryClassifier:\n    \"\"\"\n    Multi-Layer Perceptron for Binary Classification\n    Implemented from scratch without using ML libraries\n    \"\"\"\n    \n    def __init__(self, input_size: int, hidden_sizes: List[int], \n                 learning_rate: float = 0.01, activation: str = 'tanh'):\n        \"\"\"Initialize the MLP\"\"\"\n        self.input_size = input_size\n        self.hidden_sizes = hidden_sizes\n        self.learning_rate = learning_rate\n        self.activation = activation\n        \n        # Initialize weights and biases\n        self.weights = []\n        self.biases = []\n        \n        # Layer sizes including input and output\n        layer_sizes = [input_size] + hidden_sizes + [1]  # Binary output\n        \n        # Initialize parameters with Xavier/He initialization\n        for i in range(len(layer_sizes) - 1):\n            if activation == 'relu':\n                # He initialization for ReLU\n                w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2/layer_sizes[i])\n            else:\n                # Xavier initialization for tanh/sigmoid\n                w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(1/layer_sizes[i])\n            b = np.zeros(layer_sizes[i+1])\n            self.weights.append(w)\n            self.biases.append(b)\n        \n        # Storage for layer outputs (for backprop)\n        self.activations = []\n        self.z_values = []\n        \n    def _activate(self, z):\n        \"\"\"Apply activation function\"\"\"\n        if self.activation == 'tanh':\n            return np.tanh(z)\n        elif self.activation == 'relu':\n            return np.maximum(0, z)\n        elif self.activation == 'sigmoid':\n            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n        \n    def _activate_derivative(self, z):\n        \"\"\"Compute derivative of activation function\"\"\"\n        if self.activation == 'tanh':\n            return 1 - np.tanh(z)**2\n        elif self.activation == 'relu':\n            return (z &gt; 0).astype(float)\n        elif self.activation == 'sigmoid':\n            sig = self._activate(z)\n            return sig * (1 - sig)\n    \n    def _sigmoid(self, z):\n        \"\"\"Sigmoid for output layer\"\"\"\n        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n    \n    def _sigmoid_derivative(self, z):\n        \"\"\"Derivative of sigmoid\"\"\"\n        sig = self._sigmoid(z)\n        return sig * (1 - sig)\n    \n    def forward(self, X):\n        \"\"\"Forward pass through the network\"\"\"\n        self.activations = [X]\n        self.z_values = []\n        \n        current_input = X\n        \n        # Hidden layers\n        for i in range(len(self.weights) - 1):\n            z = np.dot(current_input, self.weights[i].T) + self.biases[i]\n            self.z_values.append(z)\n            a = self._activate(z)\n            self.activations.append(a)\n            current_input = a\n        \n        # Output layer (sigmoid for binary classification)\n        z = np.dot(current_input, self.weights[-1].T) + self.biases[-1]\n        self.z_values.append(z)\n        output = self._sigmoid(z)\n        self.activations.append(output)\n        \n        return output.reshape(-1)\n    \n    def binary_cross_entropy(self, y_true, y_pred):\n        \"\"\"Compute binary cross-entropy loss\"\"\"\n        eps = 1e-7  # Small value to avoid log(0)\n        y_pred = np.clip(y_pred, eps, 1 - eps)\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    \n    def backward(self, X, y):\n        \"\"\"Backward pass (backpropagation)\"\"\"\n        m = len(X)\n        \n        # Gradients storage\n        dW = []\n        db = []\n        \n        # Output layer gradient (keep delta as shape (m, 1))\n        y_pred = self.activations[-1]      # already shape (m, 1)\n        delta = (y_pred - y.reshape(-1, 1))   # (m, 1)\n        \n        # Backpropagate through layers\n        for i in range(len(self.weights) - 1, -1, -1):\n            # Weight and bias gradients\n            dW_i = np.dot(delta.T, self.activations[i]) / m\n            db_i = np.mean(delta, axis=0)\n\n            dW.insert(0, dW_i)\n            db.insert(0, db_i)\n\n            if i &gt; 0:\n                # Propagate delta to previous layer\n                delta = np.dot(delta, self.weights[i]) * self._activate_derivative(self.z_values[i-1])\n\n        \n        # Update weights and biases\n        for i in range(len(self.weights)):\n            self.weights[i] -= self.learning_rate * dW[i]\n            self.biases[i] -= self.learning_rate * db[i]\n    \n    def train(self, X_train, y_train, epochs=100, verbose=True):\n        \"\"\"Train the MLP\"\"\"\n        losses = []\n        \n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self.forward(X_train)\n            \n            # Compute loss\n            loss = self.binary_cross_entropy(y_train, y_pred)\n            losses.append(loss)\n            \n            # Backward pass\n            self.backward(X_train, y_train)\n            \n            # Print progress\n            if verbose and (epoch + 1) % 50 == 0:\n                accuracy = np.mean((y_pred &gt; 0.5) == y_train)\n                print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")\n        \n        return losses\n    \n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        probs = self.forward(X)\n        return (probs &gt; 0.5).astype(int)\n    \n    def predict_proba(self, X):\n        \"\"\"Get prediction probabilities\"\"\"\n        return self.forward(X)\n</pre> class MLPBinaryClassifier:     \"\"\"     Multi-Layer Perceptron for Binary Classification     Implemented from scratch without using ML libraries     \"\"\"          def __init__(self, input_size: int, hidden_sizes: List[int],                   learning_rate: float = 0.01, activation: str = 'tanh'):         \"\"\"Initialize the MLP\"\"\"         self.input_size = input_size         self.hidden_sizes = hidden_sizes         self.learning_rate = learning_rate         self.activation = activation                  # Initialize weights and biases         self.weights = []         self.biases = []                  # Layer sizes including input and output         layer_sizes = [input_size] + hidden_sizes + [1]  # Binary output                  # Initialize parameters with Xavier/He initialization         for i in range(len(layer_sizes) - 1):             if activation == 'relu':                 # He initialization for ReLU                 w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2/layer_sizes[i])             else:                 # Xavier initialization for tanh/sigmoid                 w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(1/layer_sizes[i])             b = np.zeros(layer_sizes[i+1])             self.weights.append(w)             self.biases.append(b)                  # Storage for layer outputs (for backprop)         self.activations = []         self.z_values = []              def _activate(self, z):         \"\"\"Apply activation function\"\"\"         if self.activation == 'tanh':             return np.tanh(z)         elif self.activation == 'relu':             return np.maximum(0, z)         elif self.activation == 'sigmoid':             return 1 / (1 + np.exp(-np.clip(z, -500, 500)))              def _activate_derivative(self, z):         \"\"\"Compute derivative of activation function\"\"\"         if self.activation == 'tanh':             return 1 - np.tanh(z)**2         elif self.activation == 'relu':             return (z &gt; 0).astype(float)         elif self.activation == 'sigmoid':             sig = self._activate(z)             return sig * (1 - sig)          def _sigmoid(self, z):         \"\"\"Sigmoid for output layer\"\"\"         return 1 / (1 + np.exp(-np.clip(z, -500, 500)))          def _sigmoid_derivative(self, z):         \"\"\"Derivative of sigmoid\"\"\"         sig = self._sigmoid(z)         return sig * (1 - sig)          def forward(self, X):         \"\"\"Forward pass through the network\"\"\"         self.activations = [X]         self.z_values = []                  current_input = X                  # Hidden layers         for i in range(len(self.weights) - 1):             z = np.dot(current_input, self.weights[i].T) + self.biases[i]             self.z_values.append(z)             a = self._activate(z)             self.activations.append(a)             current_input = a                  # Output layer (sigmoid for binary classification)         z = np.dot(current_input, self.weights[-1].T) + self.biases[-1]         self.z_values.append(z)         output = self._sigmoid(z)         self.activations.append(output)                  return output.reshape(-1)          def binary_cross_entropy(self, y_true, y_pred):         \"\"\"Compute binary cross-entropy loss\"\"\"         eps = 1e-7  # Small value to avoid log(0)         y_pred = np.clip(y_pred, eps, 1 - eps)         return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))          def backward(self, X, y):         \"\"\"Backward pass (backpropagation)\"\"\"         m = len(X)                  # Gradients storage         dW = []         db = []                  # Output layer gradient (keep delta as shape (m, 1))         y_pred = self.activations[-1]      # already shape (m, 1)         delta = (y_pred - y.reshape(-1, 1))   # (m, 1)                  # Backpropagate through layers         for i in range(len(self.weights) - 1, -1, -1):             # Weight and bias gradients             dW_i = np.dot(delta.T, self.activations[i]) / m             db_i = np.mean(delta, axis=0)              dW.insert(0, dW_i)             db.insert(0, db_i)              if i &gt; 0:                 # Propagate delta to previous layer                 delta = np.dot(delta, self.weights[i]) * self._activate_derivative(self.z_values[i-1])                   # Update weights and biases         for i in range(len(self.weights)):             self.weights[i] -= self.learning_rate * dW[i]             self.biases[i] -= self.learning_rate * db[i]          def train(self, X_train, y_train, epochs=100, verbose=True):         \"\"\"Train the MLP\"\"\"         losses = []                  for epoch in range(epochs):             # Forward pass             y_pred = self.forward(X_train)                          # Compute loss             loss = self.binary_cross_entropy(y_train, y_pred)             losses.append(loss)                          # Backward pass             self.backward(X_train, y_train)                          # Print progress             if verbose and (epoch + 1) % 50 == 0:                 accuracy = np.mean((y_pred &gt; 0.5) == y_train)                 print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")                  return losses          def predict(self, X):         \"\"\"Make predictions\"\"\"         probs = self.forward(X)         return (probs &gt; 0.5).astype(int)          def predict_proba(self, X):         \"\"\"Get prediction probabilities\"\"\"         return self.forward(X) In\u00a0[265]: Copied! <pre># Create and train the MLP for Exercise 2\nprint(\"\\nTraining MLP for Binary Classification\")\nprint(\"-\" * 40)\nmlp_ex2 = MLPBinaryClassifier(\n    input_size=2,\n    hidden_sizes=[8, 4],  # 2 hidden layers\n    learning_rate=0.1,\n    activation='tanh'\n)\n\n# Train the model\nlosses_ex2 = mlp_ex2.train(X_train_ex2, y_train_ex2, epochs=300, verbose=True)\n</pre> # Create and train the MLP for Exercise 2 print(\"\\nTraining MLP for Binary Classification\") print(\"-\" * 40) mlp_ex2 = MLPBinaryClassifier(     input_size=2,     hidden_sizes=[8, 4],  # 2 hidden layers     learning_rate=0.1,     activation='tanh' )  # Train the model losses_ex2 = mlp_ex2.train(X_train_ex2, y_train_ex2, epochs=300, verbose=True) <pre>\nTraining MLP for Binary Classification\n----------------------------------------\nEpoch 50/300 - Loss: 0.1524 - Accuracy: 0.9663\nEpoch 100/300 - Loss: 0.1251 - Accuracy: 0.9675\nEpoch 150/300 - Loss: 0.1163 - Accuracy: 0.9675\nEpoch 200/300 - Loss: 0.1112 - Accuracy: 0.9675\nEpoch 250/300 - Loss: 0.1076 - Accuracy: 0.9688\nEpoch 300/300 - Loss: 0.1048 - Accuracy: 0.9675\n</pre> In\u00a0[266]: Copied! <pre># Evaluate the model\nprint(\"\\nModel Evaluation\")\nprint(\"-\" * 40)\n\n# Training accuracy\ntrain_pred_ex2 = mlp_ex2.predict(X_train_ex2)\ntrain_acc_ex2 = accuracy_score(y_train_ex2, train_pred_ex2)\nprint(f\"Training Accuracy: {train_acc_ex2:.4f}\")\n\n# Testing accuracy\ntest_pred_ex2 = mlp_ex2.predict(X_test_ex2)\ntest_acc_ex2 = accuracy_score(y_test_ex2, test_pred_ex2)\nprint(f\"Testing Accuracy: {test_acc_ex2:.4f}\")\n\n# Confusion matrix\ncm_ex2 = confusion_matrix(y_test_ex2, test_pred_ex2)\nprint(f\"\\nConfusion Matrix:\")\nprint(cm_ex2)\n</pre> # Evaluate the model print(\"\\nModel Evaluation\") print(\"-\" * 40)  # Training accuracy train_pred_ex2 = mlp_ex2.predict(X_train_ex2) train_acc_ex2 = accuracy_score(y_train_ex2, train_pred_ex2) print(f\"Training Accuracy: {train_acc_ex2:.4f}\")  # Testing accuracy test_pred_ex2 = mlp_ex2.predict(X_test_ex2) test_acc_ex2 = accuracy_score(y_test_ex2, test_pred_ex2) print(f\"Testing Accuracy: {test_acc_ex2:.4f}\")  # Confusion matrix cm_ex2 = confusion_matrix(y_test_ex2, test_pred_ex2) print(f\"\\nConfusion Matrix:\") print(cm_ex2) <pre>\nModel Evaluation\n----------------------------------------\nTraining Accuracy: 0.9675\nTesting Accuracy: 0.9900\n\nConfusion Matrix:\n[[ 94   1]\n [  1 104]]\n</pre> In\u00a0[267]: Copied! <pre># Visualization of results\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Plot 1: Training Loss\naxes[0, 0].plot(losses_ex2)\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Binary Cross-Entropy Loss')\naxes[0, 0].set_title('Training Loss Over Time')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Confusion Matrix\nsns.heatmap(cm_ex2, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\naxes[0, 1].set_xlabel('Predicted')\naxes[0, 1].set_ylabel('Actual')\naxes[0, 1].set_title('Confusion Matrix - Test Set')\n\n# Plot 3: Decision Boundary\nh = 0.02  # step size in mesh\nx_min, x_max = X_ex2[:, 0].min() - 1, X_ex2[:, 0].max() + 1\ny_min, y_max = X_ex2[:, 1].min() - 1, X_ex2[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = mlp_ex2.predict_proba(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\naxes[1, 0].contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.8)\naxes[1, 0].scatter(X_train_ex2[y_train_ex2 == 0, 0], \n                   X_train_ex2[y_train_ex2 == 0, 1], \n                   c='blue', edgecolor='black', s=50, label='Class 0')\naxes[1, 0].scatter(X_train_ex2[y_train_ex2 == 1, 0], \n                   X_train_ex2[y_train_ex2 == 1, 1], \n                   c='red', edgecolor='black', s=50, label='Class 1')\naxes[1, 0].set_xlabel('Feature 1')\naxes[1, 0].set_ylabel('Feature 2')\naxes[1, 0].set_title('Decision Boundary - Training Set')\naxes[1, 0].legend()\n\n# Plot 4: Test set predictions\naxes[1, 1].contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.8)\ncolors = ['blue' if p == 0 else 'red' for p in test_pred_ex2]\naxes[1, 1].scatter(X_test_ex2[:, 0], X_test_ex2[:, 1], \n                   c=colors, edgecolor='black', s=50, alpha=0.7)\naxes[1, 1].set_xlabel('Feature 1')\naxes[1, 1].set_ylabel('Feature 2')\naxes[1, 1].set_title(f'Test Set Predictions (Accuracy: {test_acc_ex2:.2%})')\n\nplt.tight_layout()\nplt.show()\n</pre> # Visualization of results fig, axes = plt.subplots(2, 2, figsize=(14, 12))  # Plot 1: Training Loss axes[0, 0].plot(losses_ex2) axes[0, 0].set_xlabel('Epoch') axes[0, 0].set_ylabel('Binary Cross-Entropy Loss') axes[0, 0].set_title('Training Loss Over Time') axes[0, 0].grid(True, alpha=0.3)  # Plot 2: Confusion Matrix sns.heatmap(cm_ex2, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1]) axes[0, 1].set_xlabel('Predicted') axes[0, 1].set_ylabel('Actual') axes[0, 1].set_title('Confusion Matrix - Test Set')  # Plot 3: Decision Boundary h = 0.02  # step size in mesh x_min, x_max = X_ex2[:, 0].min() - 1, X_ex2[:, 0].max() + 1 y_min, y_max = X_ex2[:, 1].min() - 1, X_ex2[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h),                      np.arange(y_min, y_max, h))  Z = mlp_ex2.predict_proba(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape)  axes[1, 0].contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.8) axes[1, 0].scatter(X_train_ex2[y_train_ex2 == 0, 0],                     X_train_ex2[y_train_ex2 == 0, 1],                     c='blue', edgecolor='black', s=50, label='Class 0') axes[1, 0].scatter(X_train_ex2[y_train_ex2 == 1, 0],                     X_train_ex2[y_train_ex2 == 1, 1],                     c='red', edgecolor='black', s=50, label='Class 1') axes[1, 0].set_xlabel('Feature 1') axes[1, 0].set_ylabel('Feature 2') axes[1, 0].set_title('Decision Boundary - Training Set') axes[1, 0].legend()  # Plot 4: Test set predictions axes[1, 1].contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.8) colors = ['blue' if p == 0 else 'red' for p in test_pred_ex2] axes[1, 1].scatter(X_test_ex2[:, 0], X_test_ex2[:, 1],                     c=colors, edgecolor='black', s=50, alpha=0.7) axes[1, 1].set_xlabel('Feature 1') axes[1, 1].set_ylabel('Feature 2') axes[1, 1].set_title(f'Test Set Predictions (Accuracy: {test_acc_ex2:.2%})')  plt.tight_layout() plt.show() <p>The MLP successfully learns a non-linear decision boundary to separate the two classes. The model achieves good accuracy despite Class 1 having two distinct clusters, demonstrating the power of hidden layers with non-linear activation functions.</p> In\u00a0[268]: Copied! <pre># Generate multi-class dataset with different clusters per class\ndef generate_multiclass_data_ex3():\n    \"\"\"Generate 3-class dataset with 2, 3, and 4 clusters per class\"\"\"\n    np.random.seed(42)\n    \n    # Class 0 - 2 clusters\n    cluster0_1 = np.random.randn(250, 4) * 0.5 + [2, 2, 1, 1]\n    cluster0_2 = np.random.randn(250, 4) * 0.5 + [-2, -2, -1, -1]\n    X0 = np.vstack([cluster0_1, cluster0_2])\n    y0 = np.zeros(500)\n    \n    # Class 1 - 3 clusters\n    cluster1_1 = np.random.randn(167, 4) * 0.5 + [4, 0, 2, -2]\n    cluster1_2 = np.random.randn(167, 4) * 0.5 + [-4, 0, -2, 2]\n    cluster1_3 = np.random.randn(166, 4) * 0.5 + [0, 4, 0, 0]\n    X1 = np.vstack([cluster1_1, cluster1_2, cluster1_3])\n    y1 = np.ones(500)\n    \n    # Class 2 - 4 clusters\n    cluster2_1 = np.random.randn(125, 4) * 0.5 + [3, 3, 3, 3]\n    cluster2_2 = np.random.randn(125, 4) * 0.5 + [-3, 3, -3, 3]\n    cluster2_3 = np.random.randn(125, 4) * 0.5 + [3, -3, 3, -3]\n    cluster2_4 = np.random.randn(125, 4) * 0.5 + [-3, -3, -3, -3]\n    X2 = np.vstack([cluster2_1, cluster2_2, cluster2_3, cluster2_4])\n    y2 = np.ones(500) * 2\n    \n    # Combine and shuffle\n    X = np.vstack([X0, X1, X2])\n    y = np.hstack([y0, y1, y2])\n    shuffle_idx = np.random.permutation(len(X))\n    X = X[shuffle_idx]\n    y = y[shuffle_idx]\n    \n    return X, y.astype(int)\n\n# Generate the dataset\nX_ex3, y_ex3 = generate_multiclass_data_ex3()\nX_train_ex3, X_test_ex3, y_train_ex3, y_test_ex3 = train_test_split(\n    X_ex3, y_ex3, test_size=0.2, random_state=42\n)\n\nprint(f\"\\nDataset generated:\")\nprint(f\"Training samples: {len(X_train_ex3)}\")\nprint(f\"Testing samples: {len(X_test_ex3)}\")\nprint(f\"Features: {X_ex3.shape[1]}\")\nprint(f\"Classes: {len(np.unique(y_ex3))} - {np.unique(y_ex3)}\")\nprint(f\"Class distribution: {np.bincount(y_ex3)}\")\n</pre> # Generate multi-class dataset with different clusters per class def generate_multiclass_data_ex3():     \"\"\"Generate 3-class dataset with 2, 3, and 4 clusters per class\"\"\"     np.random.seed(42)          # Class 0 - 2 clusters     cluster0_1 = np.random.randn(250, 4) * 0.5 + [2, 2, 1, 1]     cluster0_2 = np.random.randn(250, 4) * 0.5 + [-2, -2, -1, -1]     X0 = np.vstack([cluster0_1, cluster0_2])     y0 = np.zeros(500)          # Class 1 - 3 clusters     cluster1_1 = np.random.randn(167, 4) * 0.5 + [4, 0, 2, -2]     cluster1_2 = np.random.randn(167, 4) * 0.5 + [-4, 0, -2, 2]     cluster1_3 = np.random.randn(166, 4) * 0.5 + [0, 4, 0, 0]     X1 = np.vstack([cluster1_1, cluster1_2, cluster1_3])     y1 = np.ones(500)          # Class 2 - 4 clusters     cluster2_1 = np.random.randn(125, 4) * 0.5 + [3, 3, 3, 3]     cluster2_2 = np.random.randn(125, 4) * 0.5 + [-3, 3, -3, 3]     cluster2_3 = np.random.randn(125, 4) * 0.5 + [3, -3, 3, -3]     cluster2_4 = np.random.randn(125, 4) * 0.5 + [-3, -3, -3, -3]     X2 = np.vstack([cluster2_1, cluster2_2, cluster2_3, cluster2_4])     y2 = np.ones(500) * 2          # Combine and shuffle     X = np.vstack([X0, X1, X2])     y = np.hstack([y0, y1, y2])     shuffle_idx = np.random.permutation(len(X))     X = X[shuffle_idx]     y = y[shuffle_idx]          return X, y.astype(int)  # Generate the dataset X_ex3, y_ex3 = generate_multiclass_data_ex3() X_train_ex3, X_test_ex3, y_train_ex3, y_test_ex3 = train_test_split(     X_ex3, y_ex3, test_size=0.2, random_state=42 )  print(f\"\\nDataset generated:\") print(f\"Training samples: {len(X_train_ex3)}\") print(f\"Testing samples: {len(X_test_ex3)}\") print(f\"Features: {X_ex3.shape[1]}\") print(f\"Classes: {len(np.unique(y_ex3))} - {np.unique(y_ex3)}\") print(f\"Class distribution: {np.bincount(y_ex3)}\") <pre>\nDataset generated:\nTraining samples: 1200\nTesting samples: 300\nFeatures: 4\nClasses: 3 - [0 1 2]\nClass distribution: [500 500 500]\n</pre> In\u00a0[269]: Copied! <pre># Reusable MLP class for multi-class classification\nclass MLPMultiClassifier:\n    \"\"\"\n    Multi-Layer Perceptron for Multi-Class Classification\n    This is a reusable version that can handle both binary and multi-class problems\n    \"\"\"\n    \n    def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int,\n                 learning_rate: float = 0.01, activation: str = 'tanh'):\n        \"\"\"Initialize the MLP\"\"\"\n        self.input_size = input_size\n        self.hidden_sizes = hidden_sizes\n        self.output_size = output_size\n        self.learning_rate = learning_rate\n        self.activation = activation\n        \n        # Initialize weights and biases\n        self.weights = []\n        self.biases = []\n        \n        # Layer sizes\n        layer_sizes = [input_size] + hidden_sizes + [output_size]\n        \n        # Initialize parameters\n        for i in range(len(layer_sizes) - 1):\n            if activation == 'relu':\n                w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2/layer_sizes[i])\n            else:\n                w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(1/layer_sizes[i])\n            b = np.zeros(layer_sizes[i+1])\n            self.weights.append(w)\n            self.biases.append(b)\n        \n        self.activations = []\n        self.z_values = []\n        \n    def _activate(self, z):\n        \"\"\"Apply activation function\"\"\"\n        if self.activation == 'tanh':\n            return np.tanh(z)\n        elif self.activation == 'relu':\n            return np.maximum(0, z)\n        elif self.activation == 'sigmoid':\n            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n        \n    def _activate_derivative(self, z):\n        \"\"\"Compute derivative of activation function\"\"\"\n        if self.activation == 'tanh':\n            return 1 - np.tanh(z)**2\n        elif self.activation == 'relu':\n            return (z &gt; 0).astype(float)\n        elif self.activation == 'sigmoid':\n            sig = self._activate(z)\n            return sig * (1 - sig)\n    \n    def _softmax(self, z):\n        \"\"\"Softmax for output layer (multi-class)\"\"\"\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n    \n    def forward(self, X):\n        \"\"\"Forward pass through the network\"\"\"\n        self.activations = [X]\n        self.z_values = []\n        \n        current_input = X\n        \n        # Hidden layers\n        for i in range(len(self.weights) - 1):\n            z = np.dot(current_input, self.weights[i].T) + self.biases[i]\n            self.z_values.append(z)\n            a = self._activate(z)\n            self.activations.append(a)\n            current_input = a\n        \n        # Output layer\n        z = np.dot(current_input, self.weights[-1].T) + self.biases[-1]\n        self.z_values.append(z)\n        \n        if self.output_size == 1:\n            # Binary classification\n            output = 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n        else:\n            # Multi-class classification\n            output = self._softmax(z)\n        \n        self.activations.append(output)\n        return output\n    \n    def categorical_cross_entropy(self, y_true_oh, y_pred):\n        \"\"\"Compute categorical cross-entropy loss\"\"\"\n        eps = 1e-7\n        y_pred = np.clip(y_pred, eps, 1 - eps)\n        return -np.mean(np.sum(y_true_oh * np.log(y_pred), axis=1))\n    \n    def binary_cross_entropy(self, y_true, y_pred):\n        \"\"\"Binary cross-entropy (for compatibility)\"\"\"\n        eps = 1e-7\n        y_pred = np.clip(y_pred, eps, 1 - eps)\n        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    \n    def backward(self, X, y_true_oh):\n        \"\"\"Backward pass (backpropagation) for multi-class\"\"\"\n        m = len(X)\n        \n        # Gradients storage\n        dW = []\n        db = []\n        \n        # Output layer gradient (softmax with cross-entropy)\n        y_pred = self.activations[-1]\n        delta = (y_pred - y_true_oh) / m\n        \n        # Backpropagate through layers\n        for i in range(len(self.weights) - 1, -1, -1):\n            # Gradient for weights and biases\n            dW_i = np.dot(delta.T, self.activations[i])\n            db_i = np.sum(delta, axis=0)\n            \n            dW.insert(0, dW_i)\n            db.insert(0, db_i)\n            \n            if i &gt; 0:\n                # Propagate error to previous layer\n                delta = np.dot(delta, self.weights[i])\n                # Apply derivative of activation function\n                delta = delta * self._activate_derivative(self.z_values[i-1])\n        \n        # Update weights and biases\n        for i in range(len(self.weights)):\n            self.weights[i] -= self.learning_rate * dW[i]\n            self.biases[i] -= self.learning_rate * db[i]\n    \n    def train(self, X_train, y_train, epochs=100, verbose=True):\n        \"\"\"Train the MLP\"\"\"\n        losses = []\n        \n        # Convert labels to one-hot encoding\n        if self.output_size &gt; 1:\n            y_train_oh = np.zeros((len(y_train), self.output_size))\n            y_train_oh[np.arange(len(y_train)), y_train] = 1\n        else:\n            y_train_oh = y_train.reshape(-1, 1)\n        \n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self.forward(X_train)\n            \n            # Compute loss\n            if self.output_size == 1:\n                loss = self.binary_cross_entropy(y_train, y_pred.flatten())\n            else:\n                loss = self.categorical_cross_entropy(y_train_oh, y_pred)\n            losses.append(loss)\n            \n            # Backward pass\n            self.backward(X_train, y_train_oh)\n            \n            # Print progress\n            if verbose and (epoch + 1) % 50 == 0:\n                if self.output_size == 1:\n                    accuracy = np.mean((y_pred.flatten() &gt; 0.5) == y_train)\n                else:\n                    accuracy = np.mean(np.argmax(y_pred, axis=1) == y_train)\n                print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")\n        \n        return losses\n    \n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        probs = self.forward(X)\n        if self.output_size == 1:\n            return (probs.flatten() &gt; 0.5).astype(int)\n        else:\n            return np.argmax(probs, axis=1)\n    \n    def predict_proba(self, X):\n        \"\"\"Get prediction probabilities\"\"\"\n        return self.forward(X)\n</pre> # Reusable MLP class for multi-class classification class MLPMultiClassifier:     \"\"\"     Multi-Layer Perceptron for Multi-Class Classification     This is a reusable version that can handle both binary and multi-class problems     \"\"\"          def __init__(self, input_size: int, hidden_sizes: List[int], output_size: int,                  learning_rate: float = 0.01, activation: str = 'tanh'):         \"\"\"Initialize the MLP\"\"\"         self.input_size = input_size         self.hidden_sizes = hidden_sizes         self.output_size = output_size         self.learning_rate = learning_rate         self.activation = activation                  # Initialize weights and biases         self.weights = []         self.biases = []                  # Layer sizes         layer_sizes = [input_size] + hidden_sizes + [output_size]                  # Initialize parameters         for i in range(len(layer_sizes) - 1):             if activation == 'relu':                 w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2/layer_sizes[i])             else:                 w = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(1/layer_sizes[i])             b = np.zeros(layer_sizes[i+1])             self.weights.append(w)             self.biases.append(b)                  self.activations = []         self.z_values = []              def _activate(self, z):         \"\"\"Apply activation function\"\"\"         if self.activation == 'tanh':             return np.tanh(z)         elif self.activation == 'relu':             return np.maximum(0, z)         elif self.activation == 'sigmoid':             return 1 / (1 + np.exp(-np.clip(z, -500, 500)))              def _activate_derivative(self, z):         \"\"\"Compute derivative of activation function\"\"\"         if self.activation == 'tanh':             return 1 - np.tanh(z)**2         elif self.activation == 'relu':             return (z &gt; 0).astype(float)         elif self.activation == 'sigmoid':             sig = self._activate(z)             return sig * (1 - sig)          def _softmax(self, z):         \"\"\"Softmax for output layer (multi-class)\"\"\"         exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability         return exp_z / np.sum(exp_z, axis=1, keepdims=True)          def forward(self, X):         \"\"\"Forward pass through the network\"\"\"         self.activations = [X]         self.z_values = []                  current_input = X                  # Hidden layers         for i in range(len(self.weights) - 1):             z = np.dot(current_input, self.weights[i].T) + self.biases[i]             self.z_values.append(z)             a = self._activate(z)             self.activations.append(a)             current_input = a                  # Output layer         z = np.dot(current_input, self.weights[-1].T) + self.biases[-1]         self.z_values.append(z)                  if self.output_size == 1:             # Binary classification             output = 1 / (1 + np.exp(-np.clip(z, -500, 500)))         else:             # Multi-class classification             output = self._softmax(z)                  self.activations.append(output)         return output          def categorical_cross_entropy(self, y_true_oh, y_pred):         \"\"\"Compute categorical cross-entropy loss\"\"\"         eps = 1e-7         y_pred = np.clip(y_pred, eps, 1 - eps)         return -np.mean(np.sum(y_true_oh * np.log(y_pred), axis=1))          def binary_cross_entropy(self, y_true, y_pred):         \"\"\"Binary cross-entropy (for compatibility)\"\"\"         eps = 1e-7         y_pred = np.clip(y_pred, eps, 1 - eps)         return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))          def backward(self, X, y_true_oh):         \"\"\"Backward pass (backpropagation) for multi-class\"\"\"         m = len(X)                  # Gradients storage         dW = []         db = []                  # Output layer gradient (softmax with cross-entropy)         y_pred = self.activations[-1]         delta = (y_pred - y_true_oh) / m                  # Backpropagate through layers         for i in range(len(self.weights) - 1, -1, -1):             # Gradient for weights and biases             dW_i = np.dot(delta.T, self.activations[i])             db_i = np.sum(delta, axis=0)                          dW.insert(0, dW_i)             db.insert(0, db_i)                          if i &gt; 0:                 # Propagate error to previous layer                 delta = np.dot(delta, self.weights[i])                 # Apply derivative of activation function                 delta = delta * self._activate_derivative(self.z_values[i-1])                  # Update weights and biases         for i in range(len(self.weights)):             self.weights[i] -= self.learning_rate * dW[i]             self.biases[i] -= self.learning_rate * db[i]          def train(self, X_train, y_train, epochs=100, verbose=True):         \"\"\"Train the MLP\"\"\"         losses = []                  # Convert labels to one-hot encoding         if self.output_size &gt; 1:             y_train_oh = np.zeros((len(y_train), self.output_size))             y_train_oh[np.arange(len(y_train)), y_train] = 1         else:             y_train_oh = y_train.reshape(-1, 1)                  for epoch in range(epochs):             # Forward pass             y_pred = self.forward(X_train)                          # Compute loss             if self.output_size == 1:                 loss = self.binary_cross_entropy(y_train, y_pred.flatten())             else:                 loss = self.categorical_cross_entropy(y_train_oh, y_pred)             losses.append(loss)                          # Backward pass             self.backward(X_train, y_train_oh)                          # Print progress             if verbose and (epoch + 1) % 50 == 0:                 if self.output_size == 1:                     accuracy = np.mean((y_pred.flatten() &gt; 0.5) == y_train)                 else:                     accuracy = np.mean(np.argmax(y_pred, axis=1) == y_train)                 print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")                  return losses          def predict(self, X):         \"\"\"Make predictions\"\"\"         probs = self.forward(X)         if self.output_size == 1:             return (probs.flatten() &gt; 0.5).astype(int)         else:             return np.argmax(probs, axis=1)          def predict_proba(self, X):         \"\"\"Get prediction probabilities\"\"\"         return self.forward(X) In\u00a0[270]: Copied! <pre># Create and train the MLP for Exercise 3\nprint(\"\\nTraining MLP for Multi-Class Classification\")\nprint(\"-\" * 40)\nmlp_ex3 = MLPMultiClassifier(\n    input_size=4,\n    hidden_sizes=[16],  # Single hidden layer\n    output_size=3,\n    learning_rate=0.1,\n    activation='tanh'\n)\n\n# Train the model\nlosses_ex3 = mlp_ex3.train(X_train_ex3, y_train_ex3, epochs=500, verbose=True)\n</pre> # Create and train the MLP for Exercise 3 print(\"\\nTraining MLP for Multi-Class Classification\") print(\"-\" * 40) mlp_ex3 = MLPMultiClassifier(     input_size=4,     hidden_sizes=[16],  # Single hidden layer     output_size=3,     learning_rate=0.1,     activation='tanh' )  # Train the model losses_ex3 = mlp_ex3.train(X_train_ex3, y_train_ex3, epochs=500, verbose=True) <pre>\nTraining MLP for Multi-Class Classification\n----------------------------------------\nEpoch 50/500 - Loss: 0.9518 - Accuracy: 0.5550\nEpoch 100/500 - Loss: 0.8063 - Accuracy: 0.7450\nEpoch 150/500 - Loss: 0.6013 - Accuracy: 0.8900\nEpoch 200/500 - Loss: 0.4148 - Accuracy: 0.9733\nEpoch 250/500 - Loss: 0.2915 - Accuracy: 0.9850\nEpoch 300/500 - Loss: 0.2173 - Accuracy: 0.9883\nEpoch 350/500 - Loss: 0.1710 - Accuracy: 0.9908\nEpoch 400/500 - Loss: 0.1401 - Accuracy: 0.9917\nEpoch 450/500 - Loss: 0.1185 - Accuracy: 0.9925\nEpoch 500/500 - Loss: 0.1027 - Accuracy: 0.9925\n</pre> In\u00a0[271]: Copied! <pre># Training accuracy\ntrain_pred_ex3 = mlp_ex3.predict(X_train_ex3)\ntrain_acc_ex3 = accuracy_score(y_train_ex3, train_pred_ex3)\nprint(f\"Training Accuracy: {train_acc_ex3:.4f}\")\n\n# Testing accuracy\ntest_pred_ex3 = mlp_ex3.predict(X_test_ex3)\ntest_acc_ex3 = accuracy_score(y_test_ex3, test_pred_ex3)\nprint(f\"Testing Accuracy: {test_acc_ex3:.4f}\")\n\n# Confusion matrix\ncm_ex3 = confusion_matrix(y_test_ex3, test_pred_ex3)\nprint(f\"\\nConfusion Matrix:\")\nprint(cm_ex3)\n\n# Per-class accuracy\nfor i in range(3):\n    class_mask = y_test_ex3 == i\n    if np.any(class_mask):\n        class_acc = accuracy_score(y_test_ex3[class_mask], test_pred_ex3[class_mask])\n        print(f\"Class {i} Accuracy: {class_acc:.4f}\")\n</pre> # Training accuracy train_pred_ex3 = mlp_ex3.predict(X_train_ex3) train_acc_ex3 = accuracy_score(y_train_ex3, train_pred_ex3) print(f\"Training Accuracy: {train_acc_ex3:.4f}\")  # Testing accuracy test_pred_ex3 = mlp_ex3.predict(X_test_ex3) test_acc_ex3 = accuracy_score(y_test_ex3, test_pred_ex3) print(f\"Testing Accuracy: {test_acc_ex3:.4f}\")  # Confusion matrix cm_ex3 = confusion_matrix(y_test_ex3, test_pred_ex3) print(f\"\\nConfusion Matrix:\") print(cm_ex3)  # Per-class accuracy for i in range(3):     class_mask = y_test_ex3 == i     if np.any(class_mask):         class_acc = accuracy_score(y_test_ex3[class_mask], test_pred_ex3[class_mask])         print(f\"Class {i} Accuracy: {class_acc:.4f}\") <pre>Training Accuracy: 0.9925\nTesting Accuracy: 0.9867\n\nConfusion Matrix:\n[[109   0   1]\n [  0  94   0]\n [  3   0  93]]\nClass 0 Accuracy: 0.9909\nClass 1 Accuracy: 1.0000\nClass 2 Accuracy: 0.9688\n</pre> In\u00a0[272]: Copied! <pre># Visualization for Exercise 3\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Plot 1: Training Loss\naxes[0, 0].plot(losses_ex3)\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Categorical Cross-Entropy Loss')\naxes[0, 0].set_title('Training Loss Over Time')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Confusion Matrix\nsns.heatmap(cm_ex3, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\naxes[0, 1].set_xlabel('Predicted')\naxes[0, 1].set_ylabel('Actual')\naxes[0, 1].set_title('Confusion Matrix - Test Set')\n\n# Plot 3: Feature space visualization (first 2 features)\nscatter = axes[1, 0].scatter(X_train_ex3[:, 0], X_train_ex3[:, 1], \n                            c=y_train_ex3, cmap='viridis', alpha=0.6)\naxes[1, 0].set_xlabel('Feature 1')\naxes[1, 0].set_ylabel('Feature 2')\naxes[1, 0].set_title('Training Data (First 2 Features)')\nplt.colorbar(scatter, ax=axes[1, 0])\n\n# Plot 4: Predicted vs Actual for test set\naxes[1, 1].scatter(range(len(y_test_ex3)), y_test_ex3, \n                   alpha=0.5, label='Actual', s=30)\naxes[1, 1].scatter(range(len(test_pred_ex3)), test_pred_ex3, \n                   alpha=0.5, label='Predicted', s=30)\naxes[1, 1].set_xlabel('Sample Index')\naxes[1, 1].set_ylabel('Class')\naxes[1, 1].set_title(f'Test Set: Actual vs Predicted (Accuracy: {test_acc_ex3:.2%})')\naxes[1, 1].legend()\naxes[1, 1].set_yticks([0, 1, 2])\n\nplt.tight_layout()\nplt.show()\n</pre> # Visualization for Exercise 3 fig, axes = plt.subplots(2, 2, figsize=(14, 12))  # Plot 1: Training Loss axes[0, 0].plot(losses_ex3) axes[0, 0].set_xlabel('Epoch') axes[0, 0].set_ylabel('Categorical Cross-Entropy Loss') axes[0, 0].set_title('Training Loss Over Time') axes[0, 0].grid(True, alpha=0.3)  # Plot 2: Confusion Matrix sns.heatmap(cm_ex3, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1]) axes[0, 1].set_xlabel('Predicted') axes[0, 1].set_ylabel('Actual') axes[0, 1].set_title('Confusion Matrix - Test Set')  # Plot 3: Feature space visualization (first 2 features) scatter = axes[1, 0].scatter(X_train_ex3[:, 0], X_train_ex3[:, 1],                              c=y_train_ex3, cmap='viridis', alpha=0.6) axes[1, 0].set_xlabel('Feature 1') axes[1, 0].set_ylabel('Feature 2') axes[1, 0].set_title('Training Data (First 2 Features)') plt.colorbar(scatter, ax=axes[1, 0])  # Plot 4: Predicted vs Actual for test set axes[1, 1].scatter(range(len(y_test_ex3)), y_test_ex3,                     alpha=0.5, label='Actual', s=30) axes[1, 1].scatter(range(len(test_pred_ex3)), test_pred_ex3,                     alpha=0.5, label='Predicted', s=30) axes[1, 1].set_xlabel('Sample Index') axes[1, 1].set_ylabel('Class') axes[1, 1].set_title(f'Test Set: Actual vs Predicted (Accuracy: {test_acc_ex3:.2%})') axes[1, 1].legend() axes[1, 1].set_yticks([0, 1, 2])  plt.tight_layout() plt.show() <p>The reusable MLP successfully handles multi-class classification using softmax output and categorical cross-entropy loss. The model learns to distinguish between the three classes despite their varying cluster structures.</p> In\u00a0[273]: Copied! <pre># Use the same data as Exercise 3\nX_ex4, y_ex4 = X_ex3.copy(), y_ex3.copy()\nX_train_ex4, X_test_ex4 = X_train_ex3.copy(), X_test_ex3.copy()\ny_train_ex4, y_test_ex4 = y_train_ex3.copy(), y_test_ex3.copy()\n\nprint(f\"\\nUsing the same dataset as Exercise 3:\")\nprint(f\"Training samples: {len(X_train_ex4)}\")\nprint(f\"Testing samples: {len(X_test_ex4)}\")\nprint(f\"Features: {X_ex4.shape[1]}\")\nprint(f\"Classes: {len(np.unique(y_ex4))}\")\n\n# Create and train a DEEPER MLP for Exercise 4\nprint(\"\\nTraining DEEPER MLP for Multi-Class Classification\")\nprint(\"Architecture: Input(4) -&gt; Hidden(32) -&gt; Hidden(16) -&gt; Hidden(8) -&gt; Output(3)\")\nprint(\"-\" * 40)\n\nmlp_ex4 = MLPMultiClassifier(\n    input_size=4,\n    hidden_sizes=[32, 16, 8],  # 3 hidden layers (deeper architecture)\n    output_size=3,\n    learning_rate=0.1,\n    activation='tanh'\n)\n\n# Train the deeper model\nlosses_ex4 = mlp_ex4.train(X_train_ex4, y_train_ex4, epochs=500, verbose=True)\n</pre> # Use the same data as Exercise 3 X_ex4, y_ex4 = X_ex3.copy(), y_ex3.copy() X_train_ex4, X_test_ex4 = X_train_ex3.copy(), X_test_ex3.copy() y_train_ex4, y_test_ex4 = y_train_ex3.copy(), y_test_ex3.copy()  print(f\"\\nUsing the same dataset as Exercise 3:\") print(f\"Training samples: {len(X_train_ex4)}\") print(f\"Testing samples: {len(X_test_ex4)}\") print(f\"Features: {X_ex4.shape[1]}\") print(f\"Classes: {len(np.unique(y_ex4))}\")  # Create and train a DEEPER MLP for Exercise 4 print(\"\\nTraining DEEPER MLP for Multi-Class Classification\") print(\"Architecture: Input(4) -&gt; Hidden(32) -&gt; Hidden(16) -&gt; Hidden(8) -&gt; Output(3)\") print(\"-\" * 40)  mlp_ex4 = MLPMultiClassifier(     input_size=4,     hidden_sizes=[32, 16, 8],  # 3 hidden layers (deeper architecture)     output_size=3,     learning_rate=0.1,     activation='tanh' )  # Train the deeper model losses_ex4 = mlp_ex4.train(X_train_ex4, y_train_ex4, epochs=500, verbose=True) <pre>\nUsing the same dataset as Exercise 3:\nTraining samples: 1200\nTesting samples: 300\nFeatures: 4\nClasses: 3\n\nTraining DEEPER MLP for Multi-Class Classification\nArchitecture: Input(4) -&gt; Hidden(32) -&gt; Hidden(16) -&gt; Hidden(8) -&gt; Output(3)\n----------------------------------------\nEpoch 50/500 - Loss: 0.9281 - Accuracy: 0.6300\nEpoch 100/500 - Loss: 0.5612 - Accuracy: 0.8792\nEpoch 150/500 - Loss: 0.3109 - Accuracy: 0.9808\nEpoch 200/500 - Loss: 0.1816 - Accuracy: 0.9875\nEpoch 250/500 - Loss: 0.1201 - Accuracy: 0.9867\nEpoch 300/500 - Loss: 0.0879 - Accuracy: 0.9917\nEpoch 350/500 - Loss: 0.0687 - Accuracy: 0.9917\nEpoch 400/500 - Loss: 0.0561 - Accuracy: 0.9925\nEpoch 450/500 - Loss: 0.0472 - Accuracy: 0.9942\nEpoch 500/500 - Loss: 0.0406 - Accuracy: 0.9950\n</pre> In\u00a0[274]: Copied! <pre># Training accuracy\ntrain_pred_ex4 = mlp_ex4.predict(X_train_ex4)\ntrain_acc_ex4 = accuracy_score(y_train_ex4, train_pred_ex4)\nprint(f\"Training Accuracy: {train_acc_ex4:.4f}\")\n\n# Testing accuracy\ntest_pred_ex4 = mlp_ex4.predict(X_test_ex4)\ntest_acc_ex4 = accuracy_score(y_test_ex4, test_pred_ex4)\nprint(f\"Testing Accuracy: {test_acc_ex4:.4f}\")\n\n# Confusion matrix\ncm_ex4 = confusion_matrix(y_test_ex4, test_pred_ex4)\nprint(f\"\\nConfusion Matrix:\")\nprint(cm_ex4)\n\n# Per-class accuracy\nfor i in range(3):\n    class_mask = y_test_ex4 == i\n    if np.any(class_mask):\n        class_acc = accuracy_score(y_test_ex4[class_mask], test_pred_ex4[class_mask])\n        print(f\"Class {i} Accuracy: {class_acc:.4f}\")\n\nprint(\"\\nArchitecture Comparison:\")\nprint(f\"Exercise 3 (1 hidden layer): Test Accuracy = {test_acc_ex3:.4f}\")\nprint(f\"Exercise 4 (3 hidden layers): Test Accuracy = {test_acc_ex4:.4f}\")\nprint(f\"Improvement: {(test_acc_ex4 - test_acc_ex3)*100:.2f}%\")\n</pre> # Training accuracy train_pred_ex4 = mlp_ex4.predict(X_train_ex4) train_acc_ex4 = accuracy_score(y_train_ex4, train_pred_ex4) print(f\"Training Accuracy: {train_acc_ex4:.4f}\")  # Testing accuracy test_pred_ex4 = mlp_ex4.predict(X_test_ex4) test_acc_ex4 = accuracy_score(y_test_ex4, test_pred_ex4) print(f\"Testing Accuracy: {test_acc_ex4:.4f}\")  # Confusion matrix cm_ex4 = confusion_matrix(y_test_ex4, test_pred_ex4) print(f\"\\nConfusion Matrix:\") print(cm_ex4)  # Per-class accuracy for i in range(3):     class_mask = y_test_ex4 == i     if np.any(class_mask):         class_acc = accuracy_score(y_test_ex4[class_mask], test_pred_ex4[class_mask])         print(f\"Class {i} Accuracy: {class_acc:.4f}\")  print(\"\\nArchitecture Comparison:\") print(f\"Exercise 3 (1 hidden layer): Test Accuracy = {test_acc_ex3:.4f}\") print(f\"Exercise 4 (3 hidden layers): Test Accuracy = {test_acc_ex4:.4f}\") print(f\"Improvement: {(test_acc_ex4 - test_acc_ex3)*100:.2f}%\") <pre>Training Accuracy: 0.9950\nTesting Accuracy: 0.9967\n\nConfusion Matrix:\n[[110   0   0]\n [  0  94   0]\n [  1   0  95]]\nClass 0 Accuracy: 1.0000\nClass 1 Accuracy: 1.0000\nClass 2 Accuracy: 0.9896\n\nArchitecture Comparison:\nExercise 3 (1 hidden layer): Test Accuracy = 0.9867\nExercise 4 (3 hidden layers): Test Accuracy = 0.9967\nImprovement: 1.00%\n</pre> In\u00a0[275]: Copied! <pre># Cleaner visualization for Exercise 4\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Plot 1: Training Loss Comparison\naxes[0].plot(losses_ex3, label='Exercise 3 (Shallow)', alpha=0.7)\naxes[0].plot(losses_ex4, label='Exercise 4 (Deep)', alpha=0.7)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss Comparison')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Plot 2: Confusion Matrix for Exercise 4\nsns.heatmap(cm_ex4, annot=True, fmt='d', cmap='Blues', ax=axes[1])\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('Actual')\naxes[1].set_title('Confusion Matrix - Exercise 4')\n\n# Plot 3: Accuracy Comparison (Ex3 vs Ex4)\nmodels = ['Ex3\\n(1 Hidden)', 'Ex4\\n(3 Hidden)']\ntrain_accs = [train_acc_ex3, train_acc_ex4]\ntest_accs = [test_acc_ex3, test_acc_ex4]\n\nx = np.arange(len(models))\nwidth = 0.35\n\nbars1 = axes[2].bar(x - width/2, train_accs, width, label='Train', color='skyblue')\nbars2 = axes[2].bar(x + width/2, test_accs, width, label='Test', color='lightcoral')\n\naxes[2].set_ylabel('Accuracy')\naxes[2].set_title('Model Accuracy Comparison')\naxes[2].set_xticks(x)\naxes[2].set_xticklabels(models)\naxes[2].legend()\naxes[2].set_ylim([0, 1])\n\n# Add accuracy values on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        axes[2].annotate(f'{height:.3f}',\n                        xy=(bar.get_x() + bar.get_width() / 2, height/2),\n                        xytext=(0, 3),\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n</pre> # Cleaner visualization for Exercise 4 fig, axes = plt.subplots(1, 3, figsize=(18, 5))  # Plot 1: Training Loss Comparison axes[0].plot(losses_ex3, label='Exercise 3 (Shallow)', alpha=0.7) axes[0].plot(losses_ex4, label='Exercise 4 (Deep)', alpha=0.7) axes[0].set_xlabel('Epoch') axes[0].set_ylabel('Loss') axes[0].set_title('Training Loss Comparison') axes[0].legend() axes[0].grid(True, alpha=0.3)  # Plot 2: Confusion Matrix for Exercise 4 sns.heatmap(cm_ex4, annot=True, fmt='d', cmap='Blues', ax=axes[1]) axes[1].set_xlabel('Predicted') axes[1].set_ylabel('Actual') axes[1].set_title('Confusion Matrix - Exercise 4')  # Plot 3: Accuracy Comparison (Ex3 vs Ex4) models = ['Ex3\\n(1 Hidden)', 'Ex4\\n(3 Hidden)'] train_accs = [train_acc_ex3, train_acc_ex4] test_accs = [test_acc_ex3, test_acc_ex4]  x = np.arange(len(models)) width = 0.35  bars1 = axes[2].bar(x - width/2, train_accs, width, label='Train', color='skyblue') bars2 = axes[2].bar(x + width/2, test_accs, width, label='Test', color='lightcoral')  axes[2].set_ylabel('Accuracy') axes[2].set_title('Model Accuracy Comparison') axes[2].set_xticks(x) axes[2].set_xticklabels(models) axes[2].legend() axes[2].set_ylim([0, 1])  # Add accuracy values on bars for bars in [bars1, bars2]:     for bar in bars:         height = bar.get_height()         axes[2].annotate(f'{height:.3f}',                         xy=(bar.get_x() + bar.get_width() / 2, height/2),                         xytext=(0, 3),                         textcoords=\"offset points\",                         ha='center', va='bottom')  plt.tight_layout() plt.show()  <p>The deeper MLP improved accuracy slightly compared to the shallow one, confirming that additional layers can enhance the model\u2019s ability to capture complex decision boundaries. However, the gain is small, showing that deeper is not always necessary when the data is already well-separated.</p>"},{"location":"exercises/mlp/notebook/#multi-layer-perceptrons-mlps","title":"Multi-Layer Perceptrons (MLPs)\u00b6","text":"<p>This notebook implements Multi-Layer Perceptrons from scratch, covering manual calculations, binary classification, multi-class classification, and deeper architectures. All implementations use only NumPy for matrix operations, with activation functions, loss calculations, and gradients implemented manually.</p>"},{"location":"exercises/mlp/notebook/#exercise-1-manual-calculation-of-mlp-steps","title":"Exercise 1: Manual Calculation of MLP Steps\u00b6","text":"<p>We'll manually calculate the forward pass, loss computation, backpropagation, and parameter updates for a simple MLP with 2 input features, 1 hidden layer containing 2 neurons, and 1 output neuron.</p>"},{"location":"exercises/mlp/notebook/#exercise-2-binary-classification-with-synthetic-data-and-scratch-mlp","title":"Exercise 2: Binary Classification with Synthetic Data and Scratch MLP\u00b6","text":"<p>We'll generate a synthetic dataset with 1 cluster for class 0 and 2 clusters for class 1, then implement an MLP from scratch for binary classification.</p>"},{"location":"exercises/mlp/notebook/#exercise-3-multi-class-classification-with-synthetic-data-and-reusable-mlp","title":"Exercise 3: Multi-Class Classification with Synthetic Data and Reusable MLP\u00b6","text":"<p>We'll generate a 3-class dataset with varying numbers of clusters per class (2, 3, and 4 clusters) and implement a reusable MLP that can handle both binary and multi-class classification.</p>"},{"location":"exercises/mlp/notebook/#exercise-4-multi-class-classification-with-deeper-mlp","title":"Exercise 4: Multi-Class Classification with Deeper MLP\u00b6","text":"<p>Using the same dataset as Exercise 3 but with a deeper architecture (at least 2 hidden layers) to demonstrate the effect of network depth on performance.</p>"},{"location":"exercises/mlp/notebook/#ai-assistance","title":"AI Assistance\u00b6","text":"<p>I used AI (Claude and ChatGPT) to help with:</p> <ul> <li>Reviewing my code and outputs for clarity.</li> <li>Suggesting improvements to formatting and readability.</li> </ul> <p>All code was executed, tested, and validated locally by me. Nothing was done with AI that I didn't understand.</p>"},{"location":"exercises/perceptron/notebook/","title":"2. Perceptron","text":"<p>For this exercise, we will implement a simple Perceptron model from scratch using only NumPy for basic linear algebra operations. Below is the perceptron class, along with functions to visualize the decision boundary and accuracy over epochs.</p> In\u00a0[24]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Random seed for reproducibility\nnp.random.seed(24)\n\n# Perceptron Implementation\nclass Perceptron:\n    def __init__(self, input_dim=2, learning_rate=0.01, max_epochs=100):\n        self.w = np.zeros(input_dim)   # weights\n        self.b = 0.0                   # bias\n        self.lr = learning_rate\n        self.max_epochs = max_epochs\n        self.accuracy_history = []\n\n    # Prediction function\n    def predict(self, X):\n        linear_output = np.dot(X, self.w) + self.b\n        return np.where(linear_output &gt;= 0, 1, -1)\n\n    # Training function\n    def fit(self, X, y):\n        for epoch in range(self.max_epochs):\n            errors = 0\n            for xi, target in zip(X, y):\n                pred = self.predict(xi)\n                if pred != target: # Misclassification\n                    update = self.lr * target\n                    self.w += update * xi\n                    self.b += update\n                    errors += 1\n            acc = self.evaluate(X, y)\n            self.accuracy_history.append(acc)\n            if errors == 0:\n                break\n    \n    # Evaluation function\n    def evaluate(self, X, y):\n        predictions = self.predict(X)\n        return np.mean(predictions == y)\n\n# Utility functions\ndef plot_data(X, y, title=\"Data Distribution\"):\n    plt.figure(figsize=(6,6))\n    plt.scatter(X[y==1,0], X[y==1,1], color=\"red\", label=\"Class 0\", alpha=0.6)\n    plt.scatter(X[y==-1,0], X[y==-1,1], color=\"blue\", label=\"Class 1\", alpha=0.6)\n    plt.legend()\n    plt.title(title)\n    plt.xlabel(\"X1\")\n    plt.ylabel(\"X2\")\n    plt.grid(True)\n    plt.show()\n\n# Decision boundary plotting\ndef plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n    plt.figure(figsize=(6,6))\n    plt.scatter(X[y==1,0], X[y==1,1], color=\"red\", label=\"Class 0\", alpha=0.6)\n    plt.scatter(X[y==-1,0], X[y==-1,1], color=\"blue\", label=\"Class 1\", alpha=0.6)\n\n    # Decision boundary\n    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n    xx = np.linspace(x_min, x_max, 200)\n    if model.w[1] != 0: \n        yy = -(model.w[0] * xx + model.b) / model.w[1]\n        plt.plot(xx, yy, \"k--\", label=\"Decision boundary\")\n\n    # Highlight misclassified\n    preds = model.predict(X)\n    misclassified = X[preds != y]\n    if len(misclassified) &gt; 0:\n        plt.scatter(misclassified[:,0], misclassified[:,1], \n                    facecolors='none', edgecolors='yellow', s=80, label=\"Misclassified\")\n\n    plt.legend()\n    plt.title(title)\n    plt.xlabel(\"X1\")\n    plt.ylabel(\"X2\")\n    plt.grid(True)\n    plt.show()\n\n# Accuracy over epochs plotting\ndef plot_accuracy(history, title=\"Accuracy over Epochs\"):\n    plt.figure()\n    plt.plot(range(1, len(history)+1), history, marker=\"o\")\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.grid(True)\n    plt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  # Random seed for reproducibility np.random.seed(24)  # Perceptron Implementation class Perceptron:     def __init__(self, input_dim=2, learning_rate=0.01, max_epochs=100):         self.w = np.zeros(input_dim)   # weights         self.b = 0.0                   # bias         self.lr = learning_rate         self.max_epochs = max_epochs         self.accuracy_history = []      # Prediction function     def predict(self, X):         linear_output = np.dot(X, self.w) + self.b         return np.where(linear_output &gt;= 0, 1, -1)      # Training function     def fit(self, X, y):         for epoch in range(self.max_epochs):             errors = 0             for xi, target in zip(X, y):                 pred = self.predict(xi)                 if pred != target: # Misclassification                     update = self.lr * target                     self.w += update * xi                     self.b += update                     errors += 1             acc = self.evaluate(X, y)             self.accuracy_history.append(acc)             if errors == 0:                 break          # Evaluation function     def evaluate(self, X, y):         predictions = self.predict(X)         return np.mean(predictions == y)  # Utility functions def plot_data(X, y, title=\"Data Distribution\"):     plt.figure(figsize=(6,6))     plt.scatter(X[y==1,0], X[y==1,1], color=\"red\", label=\"Class 0\", alpha=0.6)     plt.scatter(X[y==-1,0], X[y==-1,1], color=\"blue\", label=\"Class 1\", alpha=0.6)     plt.legend()     plt.title(title)     plt.xlabel(\"X1\")     plt.ylabel(\"X2\")     plt.grid(True)     plt.show()  # Decision boundary plotting def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):     plt.figure(figsize=(6,6))     plt.scatter(X[y==1,0], X[y==1,1], color=\"red\", label=\"Class 0\", alpha=0.6)     plt.scatter(X[y==-1,0], X[y==-1,1], color=\"blue\", label=\"Class 1\", alpha=0.6)      # Decision boundary     x_min, x_max = X[:,0].min()-1, X[:,0].max()+1     y_min, y_max = X[:,1].min()-1, X[:,1].max()+1     xx = np.linspace(x_min, x_max, 200)     if model.w[1] != 0:          yy = -(model.w[0] * xx + model.b) / model.w[1]         plt.plot(xx, yy, \"k--\", label=\"Decision boundary\")      # Highlight misclassified     preds = model.predict(X)     misclassified = X[preds != y]     if len(misclassified) &gt; 0:         plt.scatter(misclassified[:,0], misclassified[:,1],                      facecolors='none', edgecolors='yellow', s=80, label=\"Misclassified\")      plt.legend()     plt.title(title)     plt.xlabel(\"X1\")     plt.ylabel(\"X2\")     plt.grid(True)     plt.show()  # Accuracy over epochs plotting def plot_accuracy(history, title=\"Accuracy over Epochs\"):     plt.figure()     plt.plot(range(1, len(history)+1), history, marker=\"o\")     plt.title(title)     plt.xlabel(\"Epoch\")     plt.ylabel(\"Accuracy\")     plt.grid(True)     plt.show() <p>First we're going to generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions with the parameters that were given:</p> In\u00a0[25]: Copied! <pre># Parameters for linearly separable data\nmean0, cov0 = [1.5, 1.5], [[0.5, 0], [0, 0.5]]\nmean1, cov1 = [5, 5], [[0.5, 0], [0, 0.5]]\n\n# Generate linearly separable data\nclass0 = np.random.multivariate_normal(mean0, cov0, 1000)\nclass1 = np.random.multivariate_normal(mean1, cov1, 1000)\n\n# Combine into dataset\nX1 = np.vstack((class0, class1))\ny1 = np.hstack((-1*np.ones(1000), np.ones(1000)))\n\nplot_data(X1, y1, \"Linearly Separable Data\")\n</pre> # Parameters for linearly separable data mean0, cov0 = [1.5, 1.5], [[0.5, 0], [0, 0.5]] mean1, cov1 = [5, 5], [[0.5, 0], [0, 0.5]]  # Generate linearly separable data class0 = np.random.multivariate_normal(mean0, cov0, 1000) class1 = np.random.multivariate_normal(mean1, cov1, 1000)  # Combine into dataset X1 = np.vstack((class0, class1)) y1 = np.hstack((-1*np.ones(1000), np.ones(1000)))  plot_data(X1, y1, \"Linearly Separable Data\") <p>Now, we're going to implement a single-layer perceptron from scratch to classify the generated data into the two classes, using NumPy only for basic linear algebra operations.</p> In\u00a0[26]: Copied! <pre># Train Perceptron\nperc1 = Perceptron(input_dim=2, learning_rate=0.01, max_epochs=100)\nperc1.fit(X1, y1)\n\nprint(\"Final weights:\", perc1.w)\nprint(\"Final bias:\", perc1.b)\nprint(\"Final accuracy:\", perc1.evaluate(X1, y1))\n\nplot_decision_boundary(perc1, X1, y1, \"Decision Boundary\")\nplot_accuracy(perc1.accuracy_history, \"Accuracy over Epochs\")\n</pre> # Train Perceptron perc1 = Perceptron(input_dim=2, learning_rate=0.01, max_epochs=100) perc1.fit(X1, y1)  print(\"Final weights:\", perc1.w) print(\"Final bias:\", perc1.b) print(\"Final accuracy:\", perc1.evaluate(X1, y1))  plot_decision_boundary(perc1, X1, y1, \"Decision Boundary\") plot_accuracy(perc1.accuracy_history, \"Accuracy over Epochs\") <pre>Final weights: [0.02109579 0.03614885]\nFinal bias: -0.20000000000000004\nFinal accuracy: 1.0\n</pre> <p>The data's separability leads to quick convergence because the two clusters are centered far apart with little overlap, which means a straight line can perfectly separate them. In such a situation, the perceptron learning rule rapidly adjusts the weights after only a few misclassifications, since each update moves the decision boundary closer to an exact separator. Once all points fall on the correct side, no more updates are needed, and the algorithm converges in very few epochs. This clean separation also explains why the model achieves 100% accuracy rather than oscillating or plateauing below it.</p> <p>Now, we'll generate two new classes of 2D data points (1000 samples per class) using multivariate normal distributions with the parameters that were given, which will create partial overlap between the classes:</p> In\u00a0[27]: Copied! <pre>np.random.seed(24)\n# Parameters for overlapping data\nmean0, cov0 = [3, 3], [[1.5, 0], [0, 1.5]]\nmean1, cov1 = [4, 4], [[1.5, 0], [0, 1.5]]\n\n# Generate overlapping data\nclass0 = np.random.multivariate_normal(mean0, cov0, 1000)\nclass1 = np.random.multivariate_normal(mean1, cov1, 1000)\n\n# Combine into dataset\nX2 = np.vstack((class0, class1))\ny2 = np.hstack((-1*np.ones(1000), np.ones(1000)))\n\nplot_data(X2, y2, \"Exercise 2: Overlapping Data\")\n</pre> np.random.seed(24) # Parameters for overlapping data mean0, cov0 = [3, 3], [[1.5, 0], [0, 1.5]] mean1, cov1 = [4, 4], [[1.5, 0], [0, 1.5]]  # Generate overlapping data class0 = np.random.multivariate_normal(mean0, cov0, 1000) class1 = np.random.multivariate_normal(mean1, cov1, 1000)  # Combine into dataset X2 = np.vstack((class0, class1)) y2 = np.hstack((-1*np.ones(1000), np.ones(1000)))  plot_data(X2, y2, \"Exercise 2: Overlapping Data\") In\u00a0[28]: Copied! <pre>n_runs = 5\nresults = []\n\nfor i in range(n_runs):\n    # Reinitialize and train a new perceptron each run\n    perc = Perceptron(input_dim=2, learning_rate=0.01, max_epochs=100)\n    perc.fit(X2, y2)\n    \n    final_acc = perc.evaluate(X2, y2)\n    \n    results.append((perc, final_acc))\n\n# Select best run by highest final accuracy\nbest_run = max(results, key=lambda x: x[1])\nbest_perc, best_final_acc = best_run\n\nprint(\"Best Run:\")\nprint(\"Final weights:\", best_perc.w)\nprint(\"Final bias:\", best_perc.b)\nprint(f\"Final accuracy: {best_final_acc:.2f}\")\n\n# Plot decision boundary for best run\nplot_decision_boundary(best_perc, X2, y2, \"Decision Boundary (Best Run)\")\n\n# Plot accuracy history for best run\nplot_accuracy(best_perc.accuracy_history, \"Accuracy over Epochs (Best Run)\")\n</pre> n_runs = 5 results = []  for i in range(n_runs):     # Reinitialize and train a new perceptron each run     perc = Perceptron(input_dim=2, learning_rate=0.01, max_epochs=100)     perc.fit(X2, y2)          final_acc = perc.evaluate(X2, y2)          results.append((perc, final_acc))  # Select best run by highest final accuracy best_run = max(results, key=lambda x: x[1]) best_perc, best_final_acc = best_run  print(\"Best Run:\") print(\"Final weights:\", best_perc.w) print(\"Final bias:\", best_perc.b) print(f\"Final accuracy: {best_final_acc:.2f}\")  # Plot decision boundary for best run plot_decision_boundary(best_perc, X2, y2, \"Decision Boundary (Best Run)\")  # Plot accuracy history for best run plot_accuracy(best_perc.accuracy_history, \"Accuracy over Epochs (Best Run)\") <pre>Best Run:\nFinal weights: [0.06944049 0.09727756]\nFinal bias: -0.20000000000000004\nFinal accuracy: 0.51\n</pre> <p>Because the classes overlap, the perceptron cannot find a perfect linear separator. Unlike in Exercise 1, where convergence was guaranteed, here the updates never stabilize: correcting errors on one side inevitably creates misclassifications on the other. As a result, training does not converge, the accuracy hovers just above chance (around 50%), and many points remain incorrectly classified. This illustrates the perceptron\u2019s fundamental limitation: it only guarantees convergence when the data is linearly separable.</p>"},{"location":"exercises/perceptron/notebook/#perceptron","title":"Perceptron\u00b6","text":""},{"location":"exercises/perceptron/notebook/#exercise-1","title":"Exercise 1\u00b6","text":""},{"location":"exercises/perceptron/notebook/#data-generation-task","title":"Data Generation Task\u00b6","text":""},{"location":"exercises/perceptron/notebook/#perceptron-implementation-task","title":"Perceptron Implementation Task\u00b6","text":""},{"location":"exercises/perceptron/notebook/#exercise-2","title":"Exercise 2\u00b6","text":""},{"location":"exercises/perceptron/notebook/#data-generation-task","title":"Data Generation Task\u00b6","text":""},{"location":"exercises/perceptron/notebook/#perceptron-implementation-task","title":"Perceptron Implementation Task\u00b6","text":""},{"location":"exercises/perceptron/notebook/#ai-assistance","title":"AI Assistance\u00b6","text":"<p>I used AI (ChatGPT) to help with:</p> <ul> <li>Reviewing my code and outputs for clarity.</li> <li>Suggesting improvements to formatting and readability.</li> </ul> <p>All code was executed, tested, and validated locally by me. Nothing was done with AI that I didn't understand.</p>"}]}